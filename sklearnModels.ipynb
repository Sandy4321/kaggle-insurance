{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "import quadratic_weighted_kappa\n",
    "import feature_generator\n",
    "from scipy import optimize\n",
    "from NN import NN\n",
    "from XgBoost import XGBoostModel\n",
    "from CutPoints import CutPointOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-90c4d2a80d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimportantFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureImpDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatureImpDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Importance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimportantFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportantFeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimportantFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportantFeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimportantFeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimportantFeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not str"
     ]
    }
   ],
   "source": [
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')  \n",
    "\n",
    "train, test, labels = feature_generator.GetFeatures(dfTrain, dfTest, 10000, True)\n",
    "\n",
    "featureImpDf = pd.read_csv('FeatureImportance.csv')\n",
    "importantFeatures = featureImpDf[featureImpDf['Importance'] > 11]\n",
    "importantFeatures = [column for column in train.columns if column in importantFeatures['Feature'].values]\n",
    "importantFeatures = importantFeatures['Feature'].values\n",
    "train = train[importantFeatures]\n",
    "test = test[importantFeatures]\n",
    "\n",
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = XGBoostModel(900, 7, 0.025, 0.50, 25)\n",
    "model.fit(train, labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance = model.bst.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "for item in importance:\n",
    "    print item\n",
    "\n",
    "imp = pd.DataFrame()\n",
    "imp['Feature'] = [item[0] for item in importance]\n",
    "imp['Importance'] = [item[1] for item in importance]\n",
    "imp.to_csv(path_or_buf='FeatureImportance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CombinedModel:\n",
    "    \n",
    "    def __init__(self, modelsFromFile, modelsToCalculate):\n",
    "        self.modelsToCalculate = modelsToCalculate \n",
    "        self.modelsFromFile = modelsFromFile\n",
    "        \n",
    "    def fit(self, X, Y, fileName, num):     \n",
    "        \n",
    "        stackingData = np.ndarray((X.shape[0], len(self.modelsFromFile) + len(self.modelsToCalculate)))    \n",
    "        df = pd.read_csv(fileName) if os.path.isfile(fileName) else pd.DataFrame()\n",
    "        \n",
    "        for i in range(len(self.modelsToCalculate)):\n",
    "            model = self.modelsToCalculate[i]\n",
    "            model.fit(X, Y, num)\n",
    "            predictions = model.predict(X)\n",
    "            stackingData[:,i] = predictions\n",
    "            \n",
    "        if len(self.modelsFromFile) > 0:\n",
    "            colsToChange = range(len(self.modelsToCalculate), len(self.modelsToCalculate) + len(self.modelsFromFile))\n",
    "            stackingData[:,colsToChange] = df[self.modelsFromFile].values\n",
    "            \n",
    "        self.stackingModel.fit(stackingData, Y)\n",
    "        predictions = np.mean(stackingData, axis=1)\n",
    "        \n",
    "        initialCutPoints = np.array([1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5])\n",
    "        cpo = CutPointOptimizer(predictions, Y)\n",
    "        self.cutPoints = optimize.fmin(cpo.qwk, initialCutPoints)\n",
    "        \n",
    "        predictions = np.searchsorted(self.cutPoints, predictions) + 1   \n",
    "\n",
    "        trainQwk = quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, Y)\n",
    "        print \"Train QWK: %s\\n\" % trainQwk\n",
    "                           \n",
    "    def predict(self, X, fileName, num):\n",
    "        stackingData = np.ndarray((X.shape[0], len(self.modelsFromFile) + len(self.modelsToCalculate)))\n",
    "        df = pd.read_csv(fileName) if os.path.isfile(fileName) else pd.DataFrame()\n",
    "        \n",
    "        for i in range(len(self.modelsToCalculate)):\n",
    "            model = self.modelsToCalculate[i]\n",
    "            predictions = model.predict(X)\n",
    "            stackingData[:,i] = predictions\n",
    "            \n",
    "        if len(self.modelsFromFile) > 0:\n",
    "            colsToChange = range(len(self.modelsToCalculate), len(self.modelsToCalculate) + len(self.modelsFromFile))\n",
    "            stackingData[:,colsToChange] = df[self.modelsFromFile].values\n",
    "            \n",
    "        predictions = np.mean(stackingData, axis=1)\n",
    "        return np.searchsorted(self.cutPoints, predictions) + 1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-22d2c54cd56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobjectives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "from keras import objectives\n",
    "\n",
    "X_train = train\n",
    "y_train = labels\n",
    "\n",
    "folds = 8\n",
    "kf = KFold(len(X_train), folds)\n",
    "num = 1\n",
    "combinedModels = list()\n",
    "qwks = list()\n",
    "\n",
    "nnModel = NN(inputShape = train.shape[1], cutPointArray=cpa, layers = [250, 75], dropout = [0.5, 0.5], activation='relu', patience=5, loss=custom_loss, optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 50)\n",
    "xgBoostModel = XGBoostModel(700, 7, 0.025, 0.50, 25)\n",
    "for train_index, test_index in kf:\n",
    "    \n",
    "    combinedModel = CombinedModel([], [nnModel, xgBoostModel])\n",
    "    \n",
    "    trainFile = 'combinedTrainPredictions%s.csv' % str(num)\n",
    "    validateFile = 'combinedValidatePredictions%s.csv' % str(num)\n",
    "\n",
    "    xTrain = X_train.iloc[train_index].values\n",
    "    yTrain = y_train.iloc[train_index]      \n",
    "    xValidate = X_train.iloc[test_index].values\n",
    "    yValidate = y_train.iloc[test_index]\n",
    "\n",
    "    combinedModel.fit(xTrain, yTrain, trainFile, num-1)\n",
    "    predictions = combinedModel.predict(xValidate, validateFile, num-1)\n",
    "    qwk = quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, yValidate)\n",
    "    qwks.append(qwk)\n",
    "    print qwk\n",
    "    \n",
    "    combinedModels.append(combinedModel)\n",
    "    num += 1\n",
    "    \n",
    "meanQwk = quadratic_weighted_kappa.mean_quadratic_weighted_kappa(qwks)\n",
    "print \"Overall Test Qwk: %s\" % meanQwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'folds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b54b60729fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestPredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombinedModels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtestPredictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'combinedTestPredictions%s.csv'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'folds' is not defined"
     ]
    }
   ],
   "source": [
    "testPredictions = np.zeros(len(test))\n",
    "for i in range(1, folds + 1):\n",
    "    model = combinedModels[i-1]\n",
    "    testPredictions += model.predict(test, 'combinedTestPredictions%s.csv' % str(i), i)\n",
    "\n",
    "testPredictions /= folds\n",
    "predDf = pd.DataFrame()\n",
    "predDf['Id'] = dfTest['Id']\n",
    "predDf['Response'] = np.round(testPredictions).astype(int)\n",
    "print predDf['Response'].values\n",
    "predDf.to_csv(path_or_buf='XgBoost.csv', columns=['Id', 'Response'], index=False, header=['Id', 'Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(len(train), folds)\n",
    "num = 1\n",
    "qwks = list()\n",
    "\n",
    "name = 'Boost'\n",
    "# model = NN(inputShape = train.shape[1], cutPointArray=cpa, layers = [250, 75, 15], dropout = [0.5, 0.5, 0.5], activation='relu', patience=5, loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 50)\n",
    "\n",
    "model = XGBoostModel(800, 7, 0.025, 0.50, 25)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    trainFile = 'combinedTrainPredictions%s.csv' % str(num)\n",
    "    validateFile = 'combinedValidatePredictions%s.csv' % str(num)\n",
    "    testFile = 'combinedTestPredictions%s.csv' % str(num)\n",
    "    \n",
    "    trainDF = pd.read_csv(trainFile) if os.path.isfile(trainFile) else pd.DataFrame()  \n",
    "    validateDF = pd.read_csv(validateFile) if os.path.isfile(validateFile) else pd.DataFrame()  \n",
    "    testDF = pd.read_csv(testFile) if os.path.isfile(testFile) else pd.DataFrame()  \n",
    "\n",
    "    xTrain = train.iloc[train_index].values\n",
    "    yTrain = labels.iloc[train_index]      \n",
    "    xValidate = train.iloc[test_index].values\n",
    "    yValidate = labels.iloc[test_index]\n",
    "    \n",
    "    model.fit(xTrain, yTrain, num-1)\n",
    "    testPredictions = model.predict(test.values)\n",
    "    trainPredictions = model.predict(xTrain)\n",
    "    validatePredictions = model.predict(xValidate)\n",
    "    \n",
    "    trainDF[name] = trainPredictions\n",
    "    validateDF[name] = validatePredictions\n",
    "    testDF[name] = testPredictions\n",
    "        \n",
    "    trainDF.to_csv(path_or_buf=trainFile, index=False)\n",
    "    validateDF.to_csv(path_or_buf=validateFile, index=False)\n",
    "    testDF.to_csv(path_or_buf=testFile, index=False)\n",
    "            \n",
    "    num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
