{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import quadratic_weighted_kappa\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import feature_generator\n",
    "import xgboost as xgb\n",
    "from scipy import optimize\n",
    "import os.path\n",
    "from NN import NN\n",
    "from XgBoost import XGBoostModel\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical_History_2\n",
      "Medical_History_10\n",
      "Scaling...\n"
     ]
    }
   ],
   "source": [
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')    \n",
    "train, test, labels = feature_generator.GetFeatures(dfTrain, dfTest, 100, True)\n",
    "# train, test, labels = feature_generator.make_dataset(True, \"mean\", True, dfTrain, dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # oldInd = train[train['Ins_Age'] > train['Ins_Age'].median()].index\n",
    "# ind = train[train['Ins_Age'] <= train['Ins_Age'].median()].index\n",
    "# train = train.loc[ind]\n",
    "# # test = test.iloc[ind]\n",
    "# labels = labels.loc[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WritePredictionsToFile(model, modelName):\n",
    "    \n",
    "    kf = KFold(len(train), 3)\n",
    "    num = 1\n",
    "    for train_index, test_index in kf:\n",
    "        foldFile = 'fold%s.csv' % str(num)\n",
    "        if os.path.isfile(foldFile):\n",
    "            predictionsDF = pd.read_csv(foldFile)  \n",
    "        else:\n",
    "            predictionsDF = pd.DataFrame()\n",
    "          \n",
    "        xTrain = train.iloc[train_index].values\n",
    "        yTrain = labels.iloc[train_index]      \n",
    "        model.fit(xTrain, yTrain)\n",
    "        trainPredictions = model.predict(xTrain)\n",
    "        \n",
    "        xValidate = train.iloc[test_index].values\n",
    "        yValidate = labels.iloc[test_index]\n",
    "        predictions = model.predict(xValidate)\n",
    "        predictionsDF[modelName] = predictions\n",
    "        \n",
    "        print quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, yValidate)\n",
    "        print quadratic_weighted_kappa.quadratic_weighted_kappa(trainPredictions, yTrain)\n",
    "        \n",
    "#         youngTrainPredictions = [trainPredictions[i] for i in range(len(xTrain)) if i in youngInd]\n",
    "#         oldTrainPredictions = [trainPredictions[i] for i in range(len(trainPredictions)) if i in oldInd]\n",
    "#         youngYTrain = yTrain.iloc[youngInd] \n",
    "                        \n",
    "        predictionsDF.to_csv(path_or_buf=foldFile, index=False)\n",
    "   \n",
    "        predictionsFile = 'testPredictions%s.csv' % str(num)\n",
    "        if os.path.isfile(predictionsFile):\n",
    "            testDF = pd.read_csv(predictionsFile)   \n",
    "        else:\n",
    "            testDF = pd.DataFrame()\n",
    "            \n",
    "        xTest = test.values\n",
    "        testPredictions = model.predict(xTest)\n",
    "        testDF[modelName] = testPredictions\n",
    "        testDF.to_csv(path_or_buf=predictionsFile, index=False)\n",
    "    \n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.520426724009\n",
      "0.996979898227\n",
      "0.516454884274\n",
      "0.996442061475\n",
      "0.526019188949\n",
      "0.997765713136\n"
     ]
    }
   ],
   "source": [
    "# WritePredictionsToFile(LogisticRegression(), 'LogisticRegression')\n",
    "# WritePredictionsToFile(XGBoostModel(700, 10, 0.025, 0.65, \"reg:linear\"), 'XGBoostRegLin')\n",
    "\n",
    "# WritePredictionsToFile(XGBoostModel(0.3, 1, 0, 700), 'XGBoostLinear')\n",
    "#  __init__(self, max_depth, eta, colsample_bytree, objective):\n",
    "# WritePredictionsToFile(BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=20), 'BaggingDescisionTrees_n_estimators=20')\n",
    "WritePredictionsToFile(BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=20, max_features=0.9, max_samples=1.0), 'BaggingDescisionTreeClassifiers_n_estimators=20')\n",
    "# WritePredictionsToFile(BaggingRegressor(base_estimator=LinearRegression(), n_estimators=10), 'BaggingLinearRegression_n_estimators=10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dfTrain = pd.read_csv('train.csv')\n",
    "# dfTest = pd.read_csv('test.csv')    \n",
    "# train, test, labels = feature_generator.GetFeatures(dfTrain, dfTest, 100)\n",
    "# train, test, labels = feature_generator.make_dataset(True, \"mean\", True, dfTrain, dfTest)\n",
    "# WritePredictionsToFile(NN(inputShape = train.shape[1], layers = [100, 50], dropout = [0.5, 0.5], activation='sigmoid', loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 8), 'Keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "scorer = make_scorer(quadratic_weighted_kappa.quadratic_weighted_kappa)\n",
    "# print len(features)\n",
    "# print len(dummyVariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qwk_scorer(estimator, X, Y):\n",
    "    predictions = np.clip(estimator.predict(X), 1, 8)\n",
    "    return quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bestRandomForest = GetBestModel(lambda: RandomForestRegressor(n_estimators=10, max_depth=10), features) \n",
    "# bestRandomForest = GetBestModel(lambda: AdaBoostRegressor(LinearRegression()), features) \n",
    "# bestCombinedModel = GetBestModel(GenerateNewCombinedModel, features, 3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.20, random_state=0)\n",
    "bcf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=20, max_features=0.9, max_samples=1.0)\n",
    "bcf.fit(X_train, y_train)\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_estimators': [15, 20, 25], 'max_samples': [0.9, 1.0], 'max_features': [0.9, 1.0]}]\n",
    "# clf = GridSearchCV(SVR(kernel='rbf', max_iter=1000, epsilon=0.49, tol=0.01, verbose=True), tuned_parameters, cv=3, scoring=qwk_scorer)\n",
    "# clf = GridSearchCV(BaggingClassifier(base_estimator=DecisionTreeClassifier()), tuned_parameters, cv=3, scoring=qwk_scorer)\n",
    "# clf.fit(X_train, y_train)\n",
    "# SVR(kernel='rbf', max_iter=1, , tol=0.01, verbose=True)  \n",
    "#C=5, g=0.1\n",
    "# TestQWK: 0.24807920235173586\n",
    "#C=2, g=0.05, qwk = 0.21\n",
    "#C=5, g=0.05, qwk = 0.25465662455597715\n",
    "# do C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=0.9,\n",
      "         max_samples=1.0, n_estimators=20, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525540312378\n"
     ]
    }
   ],
   "source": [
    "bcp = bcf.predict(X_test)\n",
    "print quadratic_weighted_kappa.quadratic_weighted_kappa(bcp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print clf.best_params_\n",
    "clfPredictions = np.clip(clf.predict(X_test), 1, 8)\n",
    "\n",
    "# dataPoints = list()\n",
    "\n",
    "# folds = (2, 5, 10, 20)\n",
    "# for K in folds:\n",
    "#     _, testQwk, trainQwk = GetBestModel(GenerateNewCombinedModel, features, K)\n",
    "#     dataPoints.append((K, testQwk, trainQwk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.min(clfPredictions)\n",
    "print np.max(clfPredictions)\n",
    "print clfPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quadratic_weighted_kappa.quadratic_weighted_kappa(clfPredictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.font_manager import FontProperties\n",
    "# fontProps = FontProperties()\n",
    "# fontProps.set_size('small')\n",
    "\n",
    "\n",
    "# plt.figure(1)\n",
    "# train_data = [point[2] for point in dataPoints]\n",
    "# test_data = [point[1] for point in dataPoints]\n",
    "# plt.plot(folds, train_data, label=\"Training Data\")\n",
    "# plt.plot(folds, test_data, label=\"Test Data\")\n",
    "# plt.title(\"Learning Curve for Logistic Regression\")\n",
    "# plt.xlabel(\"Training Set Size\")\n",
    "# plt.ylabel(\"Classification Accuracy\")\n",
    "# plt.legend(prop = fontProps)\n",
    "# x1,x2,y1,y2 = plt.axis()\n",
    "# plt.axis((x1,x2,y1,1.1))\n",
    "# plt.show()\n",
    "\n",
    "# print dataPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredictions = bestCombinedModel[0].predict(dfTrain[features].values)\n",
    "trainPredictions = np.rint(trainPredictions).astype(int)\n",
    "print quadratic_weighted_kappa.quadratic_weighted_kappa(trainPredictions, dfTrain.Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestPredictions = np.rint(bestCombinedModel[0].predict(dfTest[features].values)).astype(int)\n",
    "bestPredictions = np.rint(bestPredictions).astype(int)\n",
    "dfTest['Predictions'] = bestPredictions\n",
    "print np.max(bestPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTest.to_csv(path_or_buf='Combo.csv', columns=['Id', 'Predictions'], index=False, header=['Id', 'Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['Ins_Age', 'BMI']\n",
    "print len(dfTrain)\n",
    "print len(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keywords = ['Medical_Keyword_' + str(i) for i in range(1, 49)]\n",
    "# print dfTrain[keywords].sum(axis=1)\n",
    "\n",
    "# for keyword in keywords:\n",
    "#     print pearsonr(dfTrain[keyword], dfTrain.Response)\n",
    "#     break\n",
    "\n",
    "# uniqueValues = dfTest['InsuredInfo_7'].unique()\n",
    "# for i in range(len(uniqueValues)):\n",
    "#     arr = dfTrain['InsuredInfo_7'].apply(lambda x: x == uniqueValues[i])\n",
    "    \n",
    "\n",
    "for column in ['Product_Info_4', 'Ins_Age', 'Ht', 'Wt', 'BMI', 'Employment_Info_1', 'Employment_Info_4', 'Employment_Info_6', 'Insurance_History_5', 'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5']:\n",
    "   \n",
    "    if dfTrain[column].isnull().sum():\n",
    "        print column\n",
    "        print pearsonr(dfTrain[dfTrain[column].notnull()][column], dfTrain[dfTrain[column].notnull()].Response)\n",
    "        print dfTrain[column].median()\n",
    "        print len(dfTrain[dfTrain.Response == 8])\n",
    "        print len(dfTrain)\n",
    "#         print dfTrain[column]\n",
    "#         plt.plot(dfTrain[dfTrain[column].notnull()][column], dfTrain[dfTrain[column].notnull()].Response)\n",
    "        break\n",
    "# plt.show()\n",
    "# print dfTest['InsuredInfo_4'].unique()\n",
    "# print dfTrain['InsuredInfo_6'].apply(lambda x: x == 1)\n",
    "print dfTest['Medical_History_32'].isnull().sum()\n",
    "            \n",
    "print dfTrain['Medical_History_32'].median()\n",
    "print dfTrain[dfTrain['Family_Hist_2'].notnull()]['Family_Hist_2'].max()\n",
    "# print dfTrain['Medical_History_32'].null().sum()\n",
    "# print dfTest[dfTest['Medical_History_32'].notnull()]['Medical_History_32'].median()\n",
    "# print len(pd.concat([dfTest[dfTest['Medical_History_32'].notnull()]['Medical_History_32'], dfTrain[dfTrain['Medical_History_32'].notnull()]['Medical_History_32']]))\n",
    "# print pearsonr(dfTrain[dfTrain['Medical_History_32'].notnull()]['Medical_History_32'], dfTrain[dfTrain['Medical_History_32'].notnull()]['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborModel = GetBestModel(lambda: NearestNeighbors(), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearestNeighbors = neighborModel.kneighbors(dfTrain[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = np.zeros(len(dfTrain))\n",
    "for i in range(len(dfTrain)):\n",
    "#     predictions[i] = dfTrain.iloc[nearestNeighbors[1][i]].Response.mean()\n",
    "    responses = dfTrain.iloc[nearestNeighbors[1][i]].Response\n",
    "    weights = nearestNeighbors[0][i][4] - nearestNeighbors[0][i]\n",
    "    predictions[i] = np.sum(responses * weights) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = np.rint(predictions)\n",
    "print quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, dfTrain.Response)\n",
    "print np.min(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "lowerBound = np.mean(dfTrain['BMI']) - np.std(dfTrain['BMI'])\n",
    "upperBound = np.mean(dfTrain['BMI']) + np.std(dfTrain['BMI'])\n",
    "mean = np.mean(dfTrain['BMI'])\n",
    "stats.normaltest(dfTrain['BMI'])\n",
    "\n",
    "bmiScore = dfTrain['BMI'].apply(lambda x: max(0, lowerBound - x) if x < mean else max(0, x - upperBound))\n",
    "\n",
    "model = LinearRegression(normalize=True)\n",
    "features = ['Ins_Age', 'Wt', 'BMI']\n",
    "X = np.ndarray((59381, 9))\n",
    "X[:,0] = dfTrain['Ins_Age'].values\n",
    "X[:,1] = dfTrain['BMI'].values\n",
    "X[:,2] = dfTrain['Wt'].values\n",
    "X[:,3] = dfTrain['Ins_Age'] * dfTrain['Wt']\n",
    "X[:,4] = dfTrain['Ins_Age'] * dfTrain['BMI']\n",
    "X[:,5] = dfTrain['Ins_Age'] * dfTrain['Ins_Age']\n",
    "X[:,6] = dfTrain['Wt'] * dfTrain['Wt']\n",
    "X[:,7] = dfTrain['Wt'] * dfTrain['BMI']\n",
    "X[:,8] = dfTrain['Wt'] * dfTrain['Ins_Age']\n",
    "\n",
    "model.fit(X, dfTrain['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CutPointOptimizer:\n",
    "    \n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "\n",
    "    def qwk(self, cutPoints):\n",
    "        transformedPredictions = np.searchsorted(cutPoints, self.predicted) + 1            \n",
    "        return -1 * quadratic_weighted_kappa.quadratic_weighted_kappa(transformedPredictions, self.actual)\n",
    "\n",
    "initialCutPoints = np.array([1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Product_Info_2\n",
    "# Product_Info_3\n",
    "# Employment_Info_2\n",
    "# InsuredInfo_3\n",
    "# Medical_History_2\n",
    "# Medical_History_10\n",
    "\n",
    "var = 'Product_Info_3'\n",
    "for val in dfTrain[var].unique():\n",
    "    hits = dfTrain[dfTrain[var] == val]\n",
    "    print '%s, %s, %s' % (val, len(hits), np.mean(hits.Response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
