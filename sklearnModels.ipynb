{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import quadratic_weighted_kappa\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import feature_generator\n",
    "import xgboost as xgb\n",
    "from scipy import optimize\n",
    "import os.path\n",
    "from NN import NN\n",
    "from XgBoost import XGBoostModel\n",
    "from sklearn.linear_model import Ridge\n",
    "from CutPoints import CutPointOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dummies...\n",
      "Filling in missing values...\n",
      "Scaling...\n"
     ]
    }
   ],
   "source": [
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')  \n",
    "\n",
    "# train, test, labels = feature_generator.GetFeatures(dfTrain, dfTest, 10000, True)\n",
    "# featureImpDf = pd.read_csv('FeatureImportance.csv')\n",
    "# importantFeatures = featureImpDf[featureImpDf['Importance'] > 11]\n",
    "# importantFeatures = [column for column in train.columns if column in importantFeatures['Feature'].values]\n",
    "# train = train[importantFeatures]\n",
    "# test = test[importantFeatures]\n",
    "\n",
    "train, test, labels = feature_generator.make_dataset(True, \"mean\", True, dfTrain, dfTest)\n",
    "\n",
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model = XGBoostModel(700, 7, 0.025, 0.65, \"reg:linear\", 25)\n",
    "# model.fit(train, labels)\n",
    "print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance = model.bst.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "for item in importance:\n",
    "    print item\n",
    "# print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WritePredictionsToFile(model, modelName):\n",
    "    \n",
    "    kf = KFold(len(train), 3)\n",
    "    num = 1\n",
    "    for train_index, test_index in kf:\n",
    "        foldFile = 'fold%s.csv' % str(num)\n",
    "        if os.path.isfile(foldFile):\n",
    "            predictionsDF = pd.read_csv(foldFile)  \n",
    "        else:\n",
    "            predictionsDF = pd.DataFrame()\n",
    "          \n",
    "        xTrain = train.iloc[train_index].values\n",
    "        yTrain = labels.iloc[train_index]      \n",
    "        model.fit(xTrain, yTrain)\n",
    "        trainPredictions = model.predict(xTrain)\n",
    "        \n",
    "        xValidate = train.iloc[test_index].values\n",
    "        yValidate = labels.iloc[test_index]\n",
    "        predictions = model.predict(xValidate)\n",
    "        predictionsDF[modelName] = predictions\n",
    "        \n",
    "        print quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, yValidate)\n",
    "        print quadratic_weighted_kappa.quadratic_weighted_kappa(trainPredictions, yTrain)\n",
    "        \n",
    "#         youngTrainPredictions = [trainPredictions[i] for i in range(len(xTrain)) if i in youngInd]\n",
    "#         oldTrainPredictions = [trainPredictions[i] for i in range(len(trainPredictions)) if i in oldInd]\n",
    "#         youngYTrain = yTrain.iloc[youngInd] \n",
    "                        \n",
    "        predictionsDF.to_csv(path_or_buf=foldFile, index=False)\n",
    "   \n",
    "        predictionsFile = 'testPredictions%s.csv' % str(num)\n",
    "        if os.path.isfile(predictionsFile):\n",
    "            testDF = pd.read_csv(predictionsFile)   \n",
    "        else:\n",
    "            testDF = pd.DataFrame()\n",
    "            \n",
    "        xTest = test.values\n",
    "        testPredictions = model.predict(xTest)\n",
    "        testDF[modelName] = testPredictions\n",
    "        testDF.to_csv(path_or_buf=predictionsFile, index=False)\n",
    "    \n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WritePredictionsToFile(LogisticRegression(), 'LogisticRegression')\n",
    "# WritePredictionsToFile(XGBoostModel(700, 10, 0.025, 0.65, \"reg:linear\"), 'XGBoostRegLin')\n",
    "\n",
    "# WritePredictionsToFile(XGBoostModel(0.3, 1, 0, 700), 'XGBoostLinear')\n",
    "# WritePredictionsToFile(RandomForestRegressor(n_estimators=10, max_depth=10), 'RandomForest')\n",
    "# WritePredictionsToFile(BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=20), 'BaggingDescisionTrees_n_estimators=20')\n",
    "# WritePredictionsToFile(BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=20, max_features=0.9, max_samples=1.0), 'BaggingDescisionTreeClassifiers_n_estimators=20')\n",
    "# WritePredictionsToFile(BaggingRegressor(base_estimator=LinearRegression(), n_estimators=10), 'BaggingLinearRegression_n_estimators=10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')    \n",
    "# train, test, labels = feature_generator.GetFeatures(dfTrain, dfTest, 100)\n",
    "train, test, labels = feature_generator.make_dataset(True, \"mean\", True, dfTrain, dfTest)\n",
    "WritePredictionsToFile(NN(inputShape = train.shape[1], layers = [128, 64], dropout = [0.5, 0.5], activation='sigmoid', loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 8), 'Keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "scorer = make_scorer(quadratic_weighted_kappa.quadratic_weighted_kappa)\n",
    "# print len(features)\n",
    "# print len(dummyVariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qwk_scorer(estimator, X, Y):\n",
    "    predictions = np.clip(estimator.predict(X), 1, 8)\n",
    "    return quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.20, random_state=0)\n",
    "bcf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=20, max_features=0.9, max_samples=1.0)\n",
    "bcf.fit(X_train, y_train)\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_estimators': [15, 20, 25], 'max_samples': [0.9, 1.0], 'max_features': [0.9, 1.0]}]\n",
    "# clf = GridSearchCV(SVR(kernel='rbf', max_iter=1000, epsilon=0.49, tol=0.01, verbose=True), tuned_parameters, cv=3, scoring=qwk_scorer)\n",
    "# clf = GridSearchCV(BaggingClassifier(base_estimator=DecisionTreeClassifier()), tuned_parameters, cv=3, scoring=qwk_scorer)\n",
    "# clf.fit(X_train, y_train)\n",
    "# SVR(kernel='rbf', max_iter=1, , tol=0.01, verbose=True)  \n",
    "#C=5, g=0.1\n",
    "# TestQWK: 0.24807920235173586\n",
    "#C=2, g=0.05, qwk = 0.21\n",
    "#C=5, g=0.05, qwk = 0.25465662455597715\n",
    "# do C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bcp = bcf.predict(X_test)\n",
    "print quadratic_weighted_kappa.quadratic_weighted_kappa(bcp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print clf.best_params_\n",
    "clfPredictions = np.clip(clf.predict(X_test), 1, 8)\n",
    "\n",
    "# dataPoints = list()\n",
    "\n",
    "# folds = (2, 5, 10, 20)\n",
    "# for K in folds:\n",
    "#     _, testQwk, trainQwk = GetBestModel(GenerateNewCombinedModel, features, K)\n",
    "#     dataPoints.append((K, testQwk, trainQwk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.min(clfPredictions)\n",
    "print np.max(clfPredictions)\n",
    "print clfPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CombinedModel:\n",
    "    \n",
    "    def __init__(self, modelsFromFile, modelsToCalculate):\n",
    "        self.modelsToCalculate = modelsToCalculate \n",
    "        self.modelsFromFile = modelsFromFile\n",
    "        self.stackingModel = LinearRegression()\n",
    "        self.boost = [0.652626727963, 0.652281118598, 0.651128156224, 0.651189716136, 0.662062433355]\n",
    "        self.keras = [0.621456525096, 0.640965818589, 0.64586691993, 0.66517675706, 0.679622912852]\n",
    "        \n",
    "    def fit(self, X, Y, fileName, num):     \n",
    "        stackingTrainData = np.ndarray((X.shape[0], len(self.modelsFromFile) + len(self.modelsToCalculate)))    \n",
    "        df = pd.read_csv(fileName) if os.path.isfile(fileName) else pd.DataFrame()\n",
    "        \n",
    "        for i in range(len(self.modelsToCalculate)):\n",
    "            model = self.modelsToCalculate[i]\n",
    "            model.fit(X, Y, num)\n",
    "            predictions = model.predict(X)\n",
    "            stackingTrainData[:,i] = predictions\n",
    "            df[model] = predictions\n",
    "            \n",
    "        if len(self.modelsFromFile) > 0:\n",
    "            colsToChange = range(len(self.modelsToCalculate), len(self.modelsToCalculate) + len(self.modelsFromFile))\n",
    "            stackingTrainData[:,colsToChange] = df[self.modelsFromFile].values\n",
    "            \n",
    "#         self.stackingModel.fit(stackingTrainData, Y)\n",
    "#         predictions = self.stackingModel.predict(stackingTrainData)\n",
    "#         predictions = np.add(stackingTrainData[:,0], stackingTrainData[:,1]) / 2\n",
    "        predictions = stackingTrainData[:,0]\n",
    "#         predictions = np.add(self.boost[num] * stackingTrainData[:,0], self.keras[num]*stackingTrainData[:,1]) / (self.boost[num] + self.keras[num])\n",
    "        \n",
    "        initialCutPoints = np.array([1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5])\n",
    "        cpo = CutPointOptimizer(predictions, Y)\n",
    "        self.cutPoints = optimize.fmin(cpo.qwk, initialCutPoints)\n",
    "\n",
    "        predictions = np.searchsorted(self.cutPoints, predictions) + 1   \n",
    "\n",
    "        trainQwk = quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, Y)\n",
    "        print \"Train QWK: %s\\n\" % trainQwk\n",
    "                           \n",
    "    def predict(self, X, fileName, num):\n",
    "        stackingData = np.ndarray((X.shape[0], len(self.modelsFromFile) + len(self.modelsToCalculate)))\n",
    "        df = pd.read_csv(fileName) if os.path.isfile(fileName) else pd.DataFrame()\n",
    "        \n",
    "        for i in range(len(self.modelsToCalculate)):\n",
    "            model = self.modelsToCalculate[i]\n",
    "            predictions = model.predict(X)\n",
    "            stackingData[:,i] = predictions\n",
    "            \n",
    "        if len(self.modelsFromFile) > 0:\n",
    "            colsToChange = range(len(self.modelsToCalculate), len(self.modelsToCalculate) + len(self.modelsFromFile))\n",
    "            stackingData[:,colsToChange] = df[self.modelsFromFile].values\n",
    "            \n",
    "#         predictions = self.stackingModel.predict(stackingData)\n",
    "#         predictions = np.add(stackingData[:,0], stackingData[:,1]) / 2\n",
    "#         predictions = np.add(self.boost[num] * stackingData[:,0], self.keras[num]*stackingData[:,1]) / (self.boost[num] + self.keras[num])\n",
    "        predictions = stackingData[:,0]\n",
    "        print \"CUT POINTS\"\n",
    "        print self.cutPoints\n",
    "        return np.searchsorted(self.cutPoints, predictions) + 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: 1078\n",
      "Adding Layer 0: 250\n",
      "Adding 0.5 dropout\n",
      "Adding Layer 1: 75\n",
      "Adding 0.5 dropout\n",
      "Adding Layer 2: 25\n",
      "Adding 0.25 dropout\n",
      "Epoch 1/50\n",
      "38003/38003 [==============================] - 9s - loss: 1.7297     \n",
      "Epoch 0 Kappa: 0.586490 | Best Kappa: 0.000000 \n",
      "\n",
      "Epoch 2/50\n",
      "38003/38003 [==============================] - 9s - loss: 1.5215     \n",
      "Epoch 1 Kappa: 0.605599 | Best Kappa: 0.586490 \n",
      "\n",
      "Epoch 3/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.4625    \n",
      "Epoch 2 Kappa: 0.583192 | Best Kappa: 0.605599 \n",
      "\n",
      "Epoch 4/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.4155    \n",
      "Epoch 3 Kappa: 0.616938 | Best Kappa: 0.605599 \n",
      "\n",
      "Epoch 5/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.3833    \n",
      "Epoch 4 Kappa: 0.616216 | Best Kappa: 0.616938 \n",
      "\n",
      "Epoch 6/50\n",
      "25408/38003 [===================>..........] - ETA: 3s - loss: 1.3447"
     ]
    }
   ],
   "source": [
    "folds = 5\n",
    "kf = KFold(len(train), folds)\n",
    "num = 1\n",
    "combinedModels = list()\n",
    "qwks = list()\n",
    "\n",
    "cpa = np.ndarray((5, 7))\n",
    "# cpa[0,:] = [1.94094589,  2.77827061, 4.47922564,  5.59375133,  5.98938611,  6.86875996, 7.45790894]\n",
    "cpa[0,:] = [2.27420049,  3.32644001,  3.22110338,  5.37172015,  6.01216561,  6.68318259, 7.4242825]\n",
    "cpa[1,:] = [1.93043217,  3.06875824,  4.8242938,   5.40717815,  6.17241788,  6.97729218, 7.46620894]\n",
    "cpa[2,:] = [1.23315182,  3.39688586,  4.6380404,   5.37977014,  6.16224374,  6.73081194, 7.48322552]\n",
    "cpa[3,:] = [2.33061803,  3.5055729,   4.52173829,  5.24184554,  6.1711789,   6.96746293, 7.7211743]\n",
    "cpa[4,:] = [2.26716625,  3.59018596,  4.3780072,   5.21537676,  5.95488354,  6.72023874, 7.5133742]\n",
    "\n",
    "nnModel = NN(inputShape = train.shape[1], cutPointArray=cpa, layers = [250, 75, 25], dropout = [0.5, 0.5, 0.25], activation='relu', patience=5, loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 50)\n",
    " \n",
    "for train_index, test_index in kf:\n",
    "    \n",
    "    combinedModel = CombinedModel([], [nnModel])\n",
    "#     combinedModel = CombinedModel(['Keras'], [])\n",
    "    \n",
    "    trainFile = 'combinedTrainPredictions%s.csv' % str(num)\n",
    "    validateFile = 'combinedValidatePredictions%s.csv' % str(num)\n",
    "\n",
    "    xTrain = train.iloc[train_index].values\n",
    "    yTrain = labels.iloc[train_index]      \n",
    "    xValidate = train.iloc[test_index].values\n",
    "    yValidate = labels.iloc[test_index]\n",
    "\n",
    "    combinedModel.fit(xTrain, yTrain, trainFile, num-1)\n",
    "    predictions = combinedModel.predict(xValidate, validateFile, num-1)\n",
    "    qwk = quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, yValidate)\n",
    "    qwks.append(qwk)\n",
    "    print qwk\n",
    "    \n",
    "    combinedModels.append(combinedModel)\n",
    "    num += 1\n",
    "    \n",
    "meanQwk = quadratic_weighted_kappa.mean_quadratic_weighted_kappa(qwks)\n",
    "print \"Overall Test Qwk: %s\" % meanQwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70278949  3.57952058  4.55016914  5.19638604  5.89882557  6.58744627\n",
      "  7.08271467]\n",
      "[ 0.70278949  3.57952058  4.55016914  5.19638604  5.89882557  6.58744627\n",
      "  7.08271467]\n",
      "[ 0.70278949  3.57952058  4.55016914  5.19638604  5.89882557  6.58744627\n",
      "  7.08271467]\n",
      "[ 0.70278949  3.57952058  4.55016914  5.19638604  5.89882557  6.58744627\n",
      "  7.08271467]\n",
      "[ 0.70278949  3.57952058  4.55016914  5.19638604  5.89882557  6.58744627\n",
      "  7.08271467]\n"
     ]
    }
   ],
   "source": [
    "testPredictions = np.zeros(len(test))\n",
    "# testModels = ['XGBoost']\n",
    "for i in range(1, folds + 1):\n",
    "    model = combinedModels[i-1]\n",
    "    print model.cutPoints\n",
    "#     testPredictions += model.predict(test, 'combinedTestPredictions%s.csv' % str(i), i)\n",
    "\n",
    "    \n",
    "# testPredictions /= folds\n",
    "# predDf = pd.DataFrame()\n",
    "# predDf['Id'] = dfTest['Id']\n",
    "# predDf['Response'] = np.round(testPredictions).astype(int)\n",
    "# print predDf['Response'].values\n",
    "# predDf.to_csv(path_or_buf='NnWithXgBoost.csv', columns=['Id', 'Response'], index=False, header=['Id', 'Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: 1078\n",
      "Adding Layer 0: 200\n",
      "Adding 0.5 dropout\n",
      "Adding Layer 1: 75\n",
      "Adding 0.5 dropout\n",
      "Adding Layer 2: 25\n",
      "Adding 0.5 dropout\n",
      "Epoch 1/50\n",
      "38003/38003 [==============================] - 9s - loss: 1.9125     \n",
      "Epoch 0 Kappa: 0.564208 | Best Kappa: 0.000000 \n",
      "\n",
      "Epoch 2/50\n",
      "38003/38003 [==============================] - 9s - loss: 1.6355     \n",
      "Epoch 1 Kappa: 0.592169 | Best Kappa: 0.564208 \n",
      "\n",
      "Epoch 3/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.5241    \n",
      "Epoch 2 Kappa: 0.567515 | Best Kappa: 0.592169 \n",
      "\n",
      "Epoch 4/50\n",
      "38003/38003 [==============================] - 11s - loss: 1.4376    \n",
      "Epoch 3 Kappa: 0.595805 | Best Kappa: 0.592169 \n",
      "\n",
      "Epoch 5/50\n",
      "38003/38003 [==============================] - 11s - loss: 1.3830    \n",
      "Epoch 4 Kappa: 0.581372 | Best Kappa: 0.595805 \n",
      "\n",
      "Epoch 6/50\n",
      "38003/38003 [==============================] - 11s - loss: 1.3434    \n",
      "Epoch 5 Kappa: 0.568966 | Best Kappa: 0.595805 \n",
      "\n",
      "Epoch 7/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.3163    \n",
      "Epoch 6 Kappa: 0.604376 | Best Kappa: 0.595805 \n",
      "\n",
      "Epoch 8/50\n",
      "38003/38003 [==============================] - 11s - loss: 1.2914    \n",
      "Epoch 7 Kappa: 0.604354 | Best Kappa: 0.604376 \n",
      "\n",
      "Epoch 9/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.2821    \n",
      "Epoch 8 Kappa: 0.603619 | Best Kappa: 0.604376 \n",
      "\n",
      "Epoch 10/50\n",
      "38003/38003 [==============================] - 12s - loss: 1.2698    \n",
      "Epoch 9 Kappa: 0.613940 | Best Kappa: 0.604376 \n",
      "\n",
      "Epoch 11/50\n",
      "38003/38003 [==============================] - 9s - loss: 1.2587     \n",
      "Epoch 10 Kappa: 0.611311 | Best Kappa: 0.613940 \n",
      "\n",
      "Epoch 12/50\n",
      "38003/38003 [==============================] - 9s - loss: 1.2525     \n",
      "Epoch 11 Kappa: 0.606006 | Best Kappa: 0.613940 \n",
      "\n",
      "Epoch 13/50\n",
      "38003/38003 [==============================] - 9s - loss: 1.2370     \n",
      "Epoch 12 Kappa: 0.618276 | Best Kappa: 0.613940 \n",
      "\n",
      "Epoch 14/50\n",
      "38003/38003 [==============================] - 9s - loss: 1.2247     \n",
      "Epoch 13 Kappa: 0.623633 | Best Kappa: 0.618276 \n",
      "\n",
      "Epoch 15/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.2235    \n",
      "Epoch 14 Kappa: 0.620517 | Best Kappa: 0.623633 \n",
      "\n",
      "Epoch 16/50\n",
      "38003/38003 [==============================] - 11s - loss: 1.2107    \n",
      "Epoch 15 Kappa: 0.621374 | Best Kappa: 0.623633 \n",
      "\n",
      "Epoch 17/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.2100    \n",
      "Epoch 16 Kappa: 0.623503 | Best Kappa: 0.623633 \n",
      "\n",
      "Epoch 18/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.2011    \n",
      "Epoch 17 Kappa: 0.617991 | Best Kappa: 0.623633 \n",
      "\n",
      "Epoch 19/50\n",
      "38003/38003 [==============================] - 9s - loss: 1.1951     \n",
      "Epoch 18 Kappa: 0.621849 | Best Kappa: 0.623633 \n",
      "\n",
      "Epoch 20/50\n",
      "38003/38003 [==============================] - 10s - loss: 1.1867    \n",
      "Epoch 19 Kappa: 0.620065 | Best Kappa: 0.623633 \n",
      "\n",
      "Best number of rounds: 14 \n",
      "Kappa: 0.623633 \n",
      "\n",
      "19765/19765 [==============================] - 1s     \n",
      "47504/47504 [==============================] - 2s     \n",
      "11877/11877 [==============================] - 0s     \n",
      "Epoch 1/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2891     \n",
      "Epoch 0 Kappa: 0.654128 | Best Kappa: 0.000000 \n",
      "\n",
      "Epoch 2/50\n",
      "38004/38004 [==============================] - 8s - loss: 1.2662     \n",
      "Epoch 1 Kappa: 0.650678 | Best Kappa: 0.654128 \n",
      "\n",
      "Epoch 3/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2525     \n",
      "Epoch 2 Kappa: 0.648566 | Best Kappa: 0.654128 \n",
      "\n",
      "Epoch 4/50\n",
      "38004/38004 [==============================] - 10s - loss: 1.2395    \n",
      "Epoch 3 Kappa: 0.657252 | Best Kappa: 0.654128 \n",
      "\n",
      "Epoch 5/50\n",
      "38004/38004 [==============================] - 10s - loss: 1.2341    \n",
      "Epoch 4 Kappa: 0.640239 | Best Kappa: 0.657252 \n",
      "\n",
      "Epoch 6/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2199     \n",
      "Epoch 5 Kappa: 0.645385 | Best Kappa: 0.657252 \n",
      "\n",
      "Epoch 7/50\n",
      "38004/38004 [==============================] - 11s - loss: 1.2238    \n",
      "Epoch 6 Kappa: 0.648220 | Best Kappa: 0.657252 \n",
      "\n",
      "Epoch 8/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2064     \n",
      "Epoch 7 Kappa: 0.646607 | Best Kappa: 0.657252 \n",
      "\n",
      "Epoch 9/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2029     \n",
      "Epoch 8 Kappa: 0.654915 | Best Kappa: 0.657252 \n",
      "\n",
      "Epoch 10/50\n",
      "38004/38004 [==============================] - 11s - loss: 1.1969    \n",
      "Epoch 9 Kappa: 0.646689 | Best Kappa: 0.657252 \n",
      "\n",
      "Best number of rounds: 4 \n",
      "Kappa: 0.657252 \n",
      "\n",
      "19765/19765 [==============================] - 1s     \n",
      "47505/47505 [==============================] - 2s     \n",
      "11876/11876 [==============================] - 0s     \n",
      "Epoch 1/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2542     \n",
      "Epoch 0 Kappa: 0.662374 | Best Kappa: 0.000000 \n",
      "\n",
      "Epoch 2/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2386     \n",
      "Epoch 1 Kappa: 0.661652 | Best Kappa: 0.662374 \n",
      "\n",
      "Epoch 3/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2172     \n",
      "Epoch 2 Kappa: 0.657095 | Best Kappa: 0.662374 \n",
      "\n",
      "Epoch 4/50\n",
      "38004/38004 [==============================] - 10s - loss: 1.2124    \n",
      "Epoch 3 Kappa: 0.648825 | Best Kappa: 0.662374 \n",
      "\n",
      "Epoch 5/50\n",
      "38004/38004 [==============================] - 8s - loss: 1.2015     \n",
      "Epoch 4 Kappa: 0.651929 | Best Kappa: 0.662374 \n",
      "\n",
      "Epoch 6/50\n",
      "38004/38004 [==============================] - 8s - loss: 1.1921     \n",
      "Epoch 5 Kappa: 0.653536 | Best Kappa: 0.662374 \n",
      "\n",
      "Epoch 7/50\n",
      "38004/38004 [==============================] - 8s - loss: 1.1883     \n",
      "Epoch 6 Kappa: 0.649526 | Best Kappa: 0.662374 \n",
      "\n",
      "Best number of rounds: 1 \n",
      "Kappa: 0.662374 \n",
      "\n",
      "19765/19765 [==============================] - 1s     \n",
      "47505/47505 [==============================] - 2s     \n",
      "11876/11876 [==============================] - 0s     \n",
      "Epoch 1/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2567     \n",
      "Epoch 0 Kappa: 0.675750 | Best Kappa: 0.000000 \n",
      "\n",
      "Epoch 2/50\n",
      "38004/38004 [==============================] - 8s - loss: 1.2327     \n",
      "Epoch 1 Kappa: 0.680654 | Best Kappa: 0.675750 \n",
      "\n",
      "Epoch 3/50\n",
      "38004/38004 [==============================] - 8s - loss: 1.2303     \n",
      "Epoch 2 Kappa: 0.667709 | Best Kappa: 0.680654 \n",
      "\n",
      "Epoch 4/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2261     \n",
      "Epoch 3 Kappa: 0.673331 | Best Kappa: 0.680654 \n",
      "\n",
      "Epoch 5/50\n",
      "38004/38004 [==============================] - 9s - loss: 1.2123     \n",
      "Epoch 4 Kappa: 0.673840 | Best Kappa: 0.680654 \n",
      "\n",
      "Epoch 6/50\n",
      "38004/38004 [==============================] - 8s - loss: 1.2027     \n",
      "Epoch 5 Kappa: 0.667448 | Best Kappa: 0.680654 \n",
      "\n",
      "Epoch 7/50\n",
      "38004/38004 [==============================] - 32s - loss: 1.1967    \n",
      "Epoch 6 Kappa: 0.656385 | Best Kappa: 0.680654 \n",
      "\n",
      "Epoch 8/50\n",
      "38004/38004 [==============================] - 26s - loss: 1.1898    \n",
      "Epoch 7 Kappa: 0.654323 | Best Kappa: 0.680654 \n",
      "\n",
      "Best number of rounds: 2 \n",
      "Kappa: 0.680654 \n",
      "\n",
      "19765/19765 [==============================] - 3s     \n",
      "47505/47505 [==============================] - 6s     \n",
      "11876/11876 [==============================] - 1s     \n",
      "Epoch 1/50\n",
      "38004/38004 [==============================] - 32s - loss: 1.2527    \n",
      "Epoch 0 Kappa: 0.695004 | Best Kappa: 0.000000 \n",
      "\n",
      "Epoch 2/50\n",
      "38004/38004 [==============================] - 35s - loss: 1.2361    \n",
      "Epoch 1 Kappa: 0.691023 | Best Kappa: 0.695004 \n",
      "\n",
      "Epoch 3/50\n",
      "38004/38004 [==============================] - 35s - loss: 1.2270    \n",
      "Epoch 2 Kappa: 0.684107 | Best Kappa: 0.695004 \n",
      "\n",
      "Epoch 4/50\n",
      "38004/38004 [==============================] - 32s - loss: 1.2183    \n",
      "Epoch 3 Kappa: 0.685427 | Best Kappa: 0.695004 \n",
      "\n",
      "Epoch 5/50\n",
      "38004/38004 [==============================] - 26s - loss: 1.2071    \n",
      "Epoch 4 Kappa: 0.680432 | Best Kappa: 0.695004 \n",
      "\n",
      "Epoch 6/50\n",
      "38004/38004 [==============================] - 27s - loss: 1.2004    \n",
      "Epoch 5 Kappa: 0.659393 | Best Kappa: 0.695004 \n",
      "\n",
      "Epoch 7/50\n",
      "38004/38004 [==============================] - 34s - loss: 1.1926    \n",
      "Epoch 6 Kappa: 0.680058 | Best Kappa: 0.695004 \n",
      "\n",
      "Best number of rounds: 1 \n",
      "Kappa: 0.695004 \n",
      "\n",
      "19765/19765 [==============================] - 4s     \n",
      "47505/47505 [==============================] - 10s    \n",
      "11876/11876 [==============================] - 1s     \n"
     ]
    }
   ],
   "source": [
    "kf = KFold(len(train), 5)\n",
    "num = 1\n",
    "qwks = list()\n",
    "\n",
    "name = 'Keras'\n",
    "model = nnModel = NN(inputShape = train.shape[1], cutPointArray=cpa, layers = [200, 75, 25], dropout = [0.5, 0.5, 0.5], activation='relu', patience=5, loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 50)\n",
    "\n",
    "# model = XGBoostModel(700, 7, 0.025, 0.50, \"reg:linear\", 25)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    trainFile = 'combinedTrainPredictions%s.csv' % str(num)\n",
    "    validateFile = 'combinedValidatePredictions%s.csv' % str(num)\n",
    "    testFile = 'combinedTestPredictions%s.csv' % str(num)\n",
    "    \n",
    "    trainDF = pd.read_csv(trainFile) if os.path.isfile(trainFile) else pd.DataFrame()  \n",
    "    validateDF = pd.read_csv(validateFile) if os.path.isfile(validateFile) else pd.DataFrame()  \n",
    "    testDF = pd.read_csv(testFile) if os.path.isfile(testFile) else pd.DataFrame()  \n",
    "\n",
    "    xTrain = train.iloc[train_index].values\n",
    "    yTrain = labels.iloc[train_index]      \n",
    "    xValidate = train.iloc[test_index].values\n",
    "    yValidate = labels.iloc[test_index]\n",
    "    \n",
    "    model.fit(xTrain, yTrain, num-1)\n",
    "    testPredictions = model.predict(test.values)\n",
    "    trainPredictions = model.predict(xTrain)\n",
    "    validatePredictions = model.predict(xValidate)\n",
    "    \n",
    "    trainDF[name] = trainPredictions\n",
    "    validateDF[name] = validatePredictions\n",
    "    testDF[name] = testPredictions\n",
    "        \n",
    "    trainDF.to_csv(path_or_buf=trainFile, index=False)\n",
    "    validateDF.to_csv(path_or_buf=validateFile, index=False)\n",
    "    testDF.to_csv(path_or_buf=testFile, index=False)\n",
    "            \n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keywords = ['Medical_Keyword_' + str(i) for i in range(1, 49)]\n",
    "# print dfTrain[keywords].sum(axis=1)\n",
    "\n",
    "print dfTrain[]['Response'].mean()\n",
    "print dfTrain['Response'].mean()\n",
    "#     Medical_History_10\n",
    "# # uniqueValues = dfTest['InsuredInfo_7'].unique()\n",
    "# # for i in range(len(uniqueValues)):\n",
    "# #     arr = dfTrain['InsuredInfo_7'].apply(lambda x: x == uniqueValues[i])\n",
    "    \n",
    "\n",
    "# for column in ['Product_Info_4', 'Ins_Age', 'Ht', 'Wt', 'BMI', 'Employment_Info_1', 'Employment_Info_4', 'Employment_Info_6', 'Insurance_History_5', 'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5']:\n",
    "   \n",
    "#     if dfTrain[column].isnull().sum():\n",
    "#         print column\n",
    "#         print pearsonr(dfTrain[dfTrain[column].notnull()][column], dfTrain[dfTrain[column].notnull()].Response)\n",
    "#         print dfTrain[column].median()\n",
    "#         print len(dfTrain[dfTrain.Response == 8])\n",
    "#         print len(dfTrain)\n",
    "# #         print dfTrain[column]\n",
    "# #         plt.plot(dfTrain[dfTrain[column].notnull()][column], dfTrain[dfTrain[column].notnull()].Response)\n",
    "#         break\n",
    "# # plt.show()\n",
    "# # print dfTest['InsuredInfo_4'].unique()\n",
    "# # print dfTrain['InsuredInfo_6'].apply(lambda x: x == 1)\n",
    "# print dfTest['Medical_History_32'].isnull().sum()\n",
    "            \n",
    "# print dfTrain['Medical_History_32'].median()\n",
    "# print dfTrain[dfTrain['Family_Hist_2'].notnull()]['Family_Hist_2'].max()\n",
    "# # print dfTrain['Medical_History_32'].null().sum()\n",
    "# # print dfTest[dfTest['Medical_History_32'].notnull()]['Medical_History_32'].median()\n",
    "# # print len(pd.concat([dfTest[dfTest['Medical_History_32'].notnull()]['Medical_History_32'], dfTrain[dfTrain['Medical_History_32'].notnull()]['Medical_History_32']]))\n",
    "# # print pearsonr(dfTrain[dfTrain['Medical_History_32'].notnull()]['Medical_History_32'], dfTrain[dfTrain['Medical_History_32'].notnull()]['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborModel = GetBestModel(lambda: NearestNeighbors(), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearestNeighbors = neighborModel.kneighbors(dfTrain[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = np.zeros(len(dfTrain))\n",
    "for i in range(len(dfTrain)):\n",
    "#     predictions[i] = dfTrain.iloc[nearestNeighbors[1][i]].Response.mean()\n",
    "    responses = dfTrain.iloc[nearestNeighbors[1][i]].Response\n",
    "    weights = nearestNeighbors[0][i][4] - nearestNeighbors[0][i]\n",
    "    predictions[i] = np.sum(responses * weights) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = np.rint(predictions)\n",
    "print quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, dfTrain.Response)\n",
    "print np.min(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "lowerBound = np.mean(dfTrain['BMI']) - np.std(dfTrain['BMI'])\n",
    "upperBound = np.mean(dfTrain['BMI']) + np.std(dfTrain['BMI'])\n",
    "mean = np.mean(dfTrain['BMI'])\n",
    "stats.normaltest(dfTrain['BMI'])\n",
    "\n",
    "bmiScore = dfTrain['BMI'].apply(lambda x: max(0, lowerBound - x) if x < mean else max(0, x - upperBound))\n",
    "\n",
    "model = LinearRegression(normalize=True)\n",
    "features = ['Ins_Age', 'Wt', 'BMI']\n",
    "X = np.ndarray((59381, 9))\n",
    "X[:,0] = dfTrain['Ins_Age'].values\n",
    "X[:,1] = dfTrain['BMI'].values\n",
    "X[:,2] = dfTrain['Wt'].values\n",
    "X[:,3] = dfTrain['Ins_Age'] * dfTrain['Wt']\n",
    "X[:,4] = dfTrain['Ins_Age'] * dfTrain['BMI']\n",
    "X[:,5] = dfTrain['Ins_Age'] * dfTrain['Ins_Age']\n",
    "X[:,6] = dfTrain['Wt'] * dfTrain['Wt']\n",
    "X[:,7] = dfTrain['Wt'] * dfTrain['BMI']\n",
    "X[:,8] = dfTrain['Wt'] * dfTrain['Ins_Age']\n",
    "\n",
    "model.fit(X, dfTrain['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CutPointOptimizer:\n",
    "    \n",
    "    def __init__(self, predicted, actual):\n",
    "        self.predicted = predicted\n",
    "        self.actual = actual\n",
    "\n",
    "    def qwk(self, cutPoints):\n",
    "        transformedPredictions = np.searchsorted(cutPoints, self.predicted) + 1            \n",
    "        return -1 * quadratic_weighted_kappa.quadratic_weighted_kappa(transformedPredictions, self.actual)\n",
    "\n",
    "initialCutPoints = np.array([1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Product_Info_2\n",
    "# Product_Info_3\n",
    "# Employment_Info_2\n",
    "# InsuredInfo_3\n",
    "# Medical_History_2\n",
    "# Medical_History_10\n",
    "\n",
    "var = 'Product_Info_3'\n",
    "for val in dfTrain[var].unique():\n",
    "    hits = dfTrain[dfTrain[var] == val]\n",
    "    print '%s, %s, %s' % (val, len(hits), np.mean(hits.Response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import quadratic_weighted_kappa\n",
    "y1, y2 = T.vectors('y1', 'y2')\n",
    "# s = 1 / (1 + T.exp(-x))\n",
    "s = quadratic_weighted_kappa.quadratic_weighted_kappa(y1, y2)\n",
    "# logistic = theano.function([y1, y2], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2 layer 200/75 cut points\n",
    "cpa = np.ndarray((5, 7))\n",
    "cpa[0,:] = [1.94094589,  2.77827061, 4.47922564,  5.59375133,  5.98938611,  6.86875996, 7.45790894]\n",
    "cpa[1,:] = [1.93043217,  3.06875824,  4.8242938,   5.40717815,  6.17241788,  6.97729218, 7.46620894]\n",
    "cpa[2,:] = [1.23315182,  3.39688586,  4.6380404,   5.37977014,  6.16224374,  6.73081194, 7.48322552]\n",
    "cpa[3,:] = [2.33061803,  3.5055729,   4.52173829,  5.24184554,  6.1711789,   6.96746293, 7.7211743]\n",
    "cpa[4,:] = [2.26716625,  3.59018596,  4.3780072,   5.21537676,  5.95488354,  6.72023874, 7.5133742]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
