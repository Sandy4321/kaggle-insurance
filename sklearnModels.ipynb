{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import quadratic_weighted_kappa\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import feature_generator\n",
    "import xgboost as xgb\n",
    "from scipy import optimize\n",
    "import os.path\n",
    "from NN import NN\n",
    "from XgBoost import XGBoostModel\n",
    "from sklearn.linear_model import Ridge\n",
    "from CutPoints import CutPointOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-90c4d2a80d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimportantFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureImpDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatureImpDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Importance'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimportantFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportantFeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimportantFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportantFeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Feature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimportantFeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimportantFeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not str"
     ]
    }
   ],
   "source": [
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')  \n",
    "\n",
    "train, test, labels = feature_generator.GetFeatures(dfTrain, dfTest, 10000, True)\n",
    "\n",
    "featureImpDf = pd.read_csv('FeatureImportance.csv')\n",
    "importantFeatures = featureImpDf[featureImpDf['Importance'] > 11]\n",
    "importantFeatures = [column for column in train.columns if column in importantFeatures['Feature'].values]\n",
    "importantFeatures = importantFeatures['Feature'].values\n",
    "train = train[importantFeatures]\n",
    "test = test[importantFeatures]\n",
    "\n",
    "# train, test, labels = feature_generator.make_dataset(True, \"mean\", True, dfTrain, dfTest)\n",
    "\n",
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = XGBoostModel(900, 7, 0.025, 0.50, 25)\n",
    "model.fit(train, labels, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance = model.bst.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance = sorted(importance.items(), key=operator.itemgetter(1))\n",
    "for item in importance:\n",
    "    print item\n",
    "# print train.shape\n",
    "\n",
    "imp = pd.DataFrame()\n",
    "imp['Feature'] = [item[0] for item in importance]\n",
    "imp['Importance'] = [item[1] for item in importance]\n",
    "imp.to_csv(path_or_buf='FeatureImportance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WritePredictionsToFile(model, modelName):\n",
    "    \n",
    "    kf = KFold(len(train), 3)\n",
    "    num = 1\n",
    "    for train_index, test_index in kf:\n",
    "        foldFile = 'fold%s.csv' % str(num)\n",
    "        if os.path.isfile(foldFile):\n",
    "            predictionsDF = pd.read_csv(foldFile)  \n",
    "        else:\n",
    "            predictionsDF = pd.DataFrame()\n",
    "          \n",
    "        xTrain = train.iloc[train_index].values\n",
    "        yTrain = labels.iloc[train_index]      \n",
    "        model.fit(xTrain, yTrain)\n",
    "        trainPredictions = model.predict(xTrain)\n",
    "        \n",
    "        xValidate = train.iloc[test_index].values\n",
    "        yValidate = labels.iloc[test_index]\n",
    "        predictions = model.predict(xValidate)\n",
    "        predictionsDF[modelName] = predictions\n",
    "        \n",
    "        print quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, yValidate)\n",
    "        print quadratic_weighted_kappa.quadratic_weighted_kappa(trainPredictions, yTrain)\n",
    "        \n",
    "#         youngTrainPredictions = [trainPredictions[i] for i in range(len(xTrain)) if i in youngInd]\n",
    "#         oldTrainPredictions = [trainPredictions[i] for i in range(len(trainPredictions)) if i in oldInd]\n",
    "#         youngYTrain = yTrain.iloc[youngInd] \n",
    "                        \n",
    "        predictionsDF.to_csv(path_or_buf=foldFile, index=False)\n",
    "   \n",
    "        predictionsFile = 'testPredictions%s.csv' % str(num)\n",
    "        if os.path.isfile(predictionsFile):\n",
    "            testDF = pd.read_csv(predictionsFile)   \n",
    "        else:\n",
    "            testDF = pd.DataFrame()\n",
    "            \n",
    "        xTest = test.values\n",
    "        testPredictions = model.predict(xTest)\n",
    "        testDF[modelName] = testPredictions\n",
    "        testDF.to_csv(path_or_buf=predictionsFile, index=False)\n",
    "    \n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WritePredictionsToFile(LogisticRegression(), 'LogisticRegression')\n",
    "# WritePredictionsToFile(XGBoostModel(700, 10, 0.025, 0.65, \"reg:linear\"), 'XGBoostRegLin')\n",
    "\n",
    "# WritePredictionsToFile(XGBoostModel(0.3, 1, 0, 700), 'XGBoostLinear')\n",
    "# WritePredictionsToFile(RandomForestRegressor(n_estimators=10, max_depth=10), 'RandomForest')\n",
    "# WritePredictionsToFile(BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=20), 'BaggingDescisionTrees_n_estimators=20')\n",
    "# WritePredictionsToFile(BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=20, max_features=0.9, max_samples=1.0), 'BaggingDescisionTreeClassifiers_n_estimators=20')\n",
    "# WritePredictionsToFile(BaggingRegressor(base_estimator=LinearRegression(), n_estimators=10), 'BaggingLinearRegression_n_estimators=10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')    \n",
    "# train, test, labels = feature_generator.GetFeatures(dfTrain, dfTest, 100)\n",
    "train, test, labels = feature_generator.make_dataset(True, \"mean\", True, dfTrain, dfTest)\n",
    "WritePredictionsToFile(NN(inputShape = train.shape[1], layers = [128, 64], dropout = [0.5, 0.5], activation='sigmoid', loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 8), 'Keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "scorer = make_scorer(quadratic_weighted_kappa.quadratic_weighted_kappa)\n",
    "# print len(features)\n",
    "# print len(dummyVariables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qwk_scorer(estimator, X, Y):\n",
    "    predictions = np.clip(estimator.predict(X), 1, 8)\n",
    "    return quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.20, random_state=0)\n",
    "bcf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=20, max_features=0.9, max_samples=1.0)\n",
    "bcf.fit(X_train, y_train)\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_estimators': [15, 20, 25], 'max_samples': [0.9, 1.0], 'max_features': [0.9, 1.0]}]\n",
    "# clf = GridSearchCV(SVR(kernel='rbf', max_iter=1000, epsilon=0.49, tol=0.01, verbose=True), tuned_parameters, cv=3, scoring=qwk_scorer)\n",
    "# clf = GridSearchCV(BaggingClassifier(base_estimator=DecisionTreeClassifier()), tuned_parameters, cv=3, scoring=qwk_scorer)\n",
    "# clf.fit(X_train, y_train)\n",
    "# SVR(kernel='rbf', max_iter=1, , tol=0.01, verbose=True)  \n",
    "#C=5, g=0.1\n",
    "# TestQWK: 0.24807920235173586\n",
    "#C=2, g=0.05, qwk = 0.21\n",
    "#C=5, g=0.05, qwk = 0.25465662455597715\n",
    "# do C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bcp = bcf.predict(X_test)\n",
    "print quadratic_weighted_kappa.quadratic_weighted_kappa(bcp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print clf.best_params_\n",
    "clfPredictions = np.clip(clf.predict(X_test), 1, 8)\n",
    "\n",
    "# dataPoints = list()\n",
    "\n",
    "# folds = (2, 5, 10, 20)\n",
    "# for K in folds:\n",
    "#     _, testQwk, trainQwk = GetBestModel(GenerateNewCombinedModel, features, K)\n",
    "#     dataPoints.append((K, testQwk, trainQwk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.min(clfPredictions)\n",
    "print np.max(clfPredictions)\n",
    "print clfPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from keras import backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "#     medianCutPoints = theano.shared(np.array([2.00711375,  3.37756859,  4.31663661,  5.32317815,  6.09457793,  6.81579768, 7.52165309]))\n",
    "    x = K.abs(y_true - y_pred)\n",
    "    return K.mean(K.maximum(x-K.exp(-6*x), 0.0), axis=-1)\n",
    "#     return K.mean((x)*(1-K.exp(-(x))), axis=-1)\n",
    "#     return K.mean(K.maximum(x-0.25, 0.0), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-kappa:-0.000000\n",
      "[1]\ttrain-kappa:-0.000000\n",
      "[2]\ttrain-kappa:-0.000000\n",
      "[3]\ttrain-kappa:-0.000000\n",
      "[4]\ttrain-kappa:-0.000000\n",
      "[5]\ttrain-kappa:-0.000000\n",
      "[6]\ttrain-kappa:-0.000000\n",
      "[7]\ttrain-kappa:-0.000000\n",
      "[8]\ttrain-kappa:-0.000000\n",
      "[9]\ttrain-kappa:-0.027943\n",
      "[10]\ttrain-kappa:-0.043175\n",
      "[11]\ttrain-kappa:-0.053980\n",
      "[12]\ttrain-kappa:-0.056978\n",
      "[13]\ttrain-kappa:-0.055907\n",
      "[14]\ttrain-kappa:-0.053054\n",
      "[15]\ttrain-kappa:-0.049413\n",
      "[16]\ttrain-kappa:-0.045879\n",
      "[17]\ttrain-kappa:-0.041538\n",
      "[18]\ttrain-kappa:-0.038216\n",
      "[19]\ttrain-kappa:-0.034890\n",
      "[20]\ttrain-kappa:-0.032050\n",
      "[21]\ttrain-kappa:-0.045942\n",
      "[22]\ttrain-kappa:-0.064824\n",
      "[23]\ttrain-kappa:-0.074395\n",
      "[24]\ttrain-kappa:-0.081703\n",
      "[25]\ttrain-kappa:-0.088135\n",
      "[26]\ttrain-kappa:-0.092916\n",
      "[27]\ttrain-kappa:-0.096353\n",
      "[28]\ttrain-kappa:-0.098487\n",
      "[29]\ttrain-kappa:-0.099430\n",
      "[30]\ttrain-kappa:-0.100089\n",
      "[31]\ttrain-kappa:-0.107694\n",
      "[32]\ttrain-kappa:-0.129702\n",
      "[33]\ttrain-kappa:-0.148321\n",
      "[34]\ttrain-kappa:-0.158889\n",
      "[35]\ttrain-kappa:-0.166418\n",
      "[36]\ttrain-kappa:-0.172615\n",
      "[37]\ttrain-kappa:-0.178919\n",
      "[38]\ttrain-kappa:-0.186206\n",
      "[39]\ttrain-kappa:-0.192052\n",
      "[40]\ttrain-kappa:-0.195834\n",
      "[41]\ttrain-kappa:-0.222909\n",
      "[42]\ttrain-kappa:-0.240996\n",
      "[43]\ttrain-kappa:-0.260554\n",
      "[44]\ttrain-kappa:-0.273834\n",
      "[45]\ttrain-kappa:-0.283135\n",
      "[46]\ttrain-kappa:-0.291570\n",
      "[47]\ttrain-kappa:-0.298455\n",
      "[48]\ttrain-kappa:-0.305794\n",
      "[49]\ttrain-kappa:-0.313488\n",
      "[50]\ttrain-kappa:-0.320387\n",
      "[51]\ttrain-kappa:-0.335957\n",
      "[52]\ttrain-kappa:-0.357641\n",
      "[53]\ttrain-kappa:-0.371753\n",
      "[54]\ttrain-kappa:-0.386938\n",
      "[55]\ttrain-kappa:-0.402392\n",
      "[56]\ttrain-kappa:-0.414944\n",
      "[57]\ttrain-kappa:-0.426134\n",
      "[58]\ttrain-kappa:-0.432811\n",
      "[59]\ttrain-kappa:-0.440395\n",
      "[60]\ttrain-kappa:-0.447660\n",
      "[61]\ttrain-kappa:-0.454214\n",
      "[62]\ttrain-kappa:-0.459833\n",
      "[63]\ttrain-kappa:-0.465494\n",
      "[64]\ttrain-kappa:-0.471115\n",
      "[65]\ttrain-kappa:-0.475678\n",
      "[66]\ttrain-kappa:-0.480884\n",
      "[67]\ttrain-kappa:-0.500485\n",
      "[68]\ttrain-kappa:-0.509465\n",
      "[69]\ttrain-kappa:-0.518960\n",
      "[70]\ttrain-kappa:-0.525521\n",
      "[71]\ttrain-kappa:-0.533933\n",
      "[72]\ttrain-kappa:-0.541982\n",
      "[73]\ttrain-kappa:-0.548850\n",
      "[74]\ttrain-kappa:-0.554680\n",
      "[75]\ttrain-kappa:-0.560301\n",
      "[76]\ttrain-kappa:-0.564627\n",
      "[77]\ttrain-kappa:-0.569125\n",
      "[78]\ttrain-kappa:-0.573726\n",
      "[79]\ttrain-kappa:-0.577922\n",
      "[80]\ttrain-kappa:-0.581492\n",
      "[81]\ttrain-kappa:-0.584653\n",
      "[82]\ttrain-kappa:-0.587455\n",
      "[83]\ttrain-kappa:-0.590657\n",
      "[84]\ttrain-kappa:-0.593412\n",
      "[85]\ttrain-kappa:-0.596034\n",
      "[86]\ttrain-kappa:-0.601241\n",
      "[87]\ttrain-kappa:-0.606791\n",
      "[88]\ttrain-kappa:-0.611561\n",
      "[89]\ttrain-kappa:-0.613881\n",
      "[90]\ttrain-kappa:-0.616023\n",
      "[91]\ttrain-kappa:-0.619038\n",
      "[92]\ttrain-kappa:-0.621532\n",
      "[93]\ttrain-kappa:-0.623403\n",
      "[94]\ttrain-kappa:-0.625221\n",
      "[95]\ttrain-kappa:-0.627657\n",
      "[96]\ttrain-kappa:-0.629895\n",
      "[97]\ttrain-kappa:-0.632551\n",
      "[98]\ttrain-kappa:-0.634354\n",
      "[99]\ttrain-kappa:-0.635935\n",
      "[100]\ttrain-kappa:-0.637998\n",
      "[101]\ttrain-kappa:-0.639644\n",
      "[102]\ttrain-kappa:-0.641765\n",
      "[103]\ttrain-kappa:-0.643071\n",
      "[104]\ttrain-kappa:-0.644273\n",
      "[105]\ttrain-kappa:-0.645748\n",
      "[106]\ttrain-kappa:-0.646777\n",
      "[107]\ttrain-kappa:-0.648119\n",
      "[108]\ttrain-kappa:-0.648923\n",
      "[109]\ttrain-kappa:-0.649768\n",
      "[110]\ttrain-kappa:-0.650701\n",
      "[111]\ttrain-kappa:-0.651579\n",
      "[112]\ttrain-kappa:-0.652372\n",
      "[113]\ttrain-kappa:-0.652997\n",
      "[114]\ttrain-kappa:-0.653920\n",
      "[115]\ttrain-kappa:-0.654404\n",
      "[116]\ttrain-kappa:-0.654903\n",
      "[117]\ttrain-kappa:-0.655409\n",
      "[118]\ttrain-kappa:-0.656025\n",
      "[119]\ttrain-kappa:-0.656575\n",
      "[120]\ttrain-kappa:-0.657528\n",
      "[121]\ttrain-kappa:-0.658015\n",
      "[122]\ttrain-kappa:-0.658288\n",
      "[123]\ttrain-kappa:-0.659131\n",
      "[124]\ttrain-kappa:-0.659328\n",
      "[125]\ttrain-kappa:-0.659512\n",
      "[126]\ttrain-kappa:-0.659800\n",
      "[127]\ttrain-kappa:-0.660430\n",
      "[128]\ttrain-kappa:-0.660799\n",
      "[129]\ttrain-kappa:-0.660913\n",
      "[130]\ttrain-kappa:-0.661253\n",
      "[131]\ttrain-kappa:-0.661596\n",
      "[132]\ttrain-kappa:-0.661943\n",
      "[133]\ttrain-kappa:-0.662161\n",
      "[134]\ttrain-kappa:-0.662561\n",
      "[135]\ttrain-kappa:-0.662692\n",
      "[136]\ttrain-kappa:-0.662873\n",
      "[137]\ttrain-kappa:-0.663569\n",
      "[138]\ttrain-kappa:-0.664098\n",
      "[139]\ttrain-kappa:-0.664382\n",
      "[140]\ttrain-kappa:-0.664480\n",
      "[141]\ttrain-kappa:-0.664965\n",
      "[142]\ttrain-kappa:-0.665524\n",
      "[143]\ttrain-kappa:-0.665784\n",
      "[144]\ttrain-kappa:-0.666381\n",
      "[145]\ttrain-kappa:-0.666565\n",
      "[146]\ttrain-kappa:-0.667073\n",
      "[147]\ttrain-kappa:-0.667601\n",
      "[148]\ttrain-kappa:-0.667844\n",
      "[149]\ttrain-kappa:-0.668071\n",
      "[150]\ttrain-kappa:-0.668385\n",
      "[151]\ttrain-kappa:-0.668687\n",
      "[152]\ttrain-kappa:-0.668999\n",
      "[153]\ttrain-kappa:-0.669037\n",
      "[154]\ttrain-kappa:-0.669447\n",
      "[155]\ttrain-kappa:-0.669730\n",
      "[156]\ttrain-kappa:-0.669973\n",
      "[157]\ttrain-kappa:-0.670257\n",
      "[158]\ttrain-kappa:-0.670715\n",
      "[159]\ttrain-kappa:-0.670984\n",
      "[160]\ttrain-kappa:-0.671416\n",
      "[161]\ttrain-kappa:-0.671674\n",
      "[162]\ttrain-kappa:-0.671709\n",
      "[163]\ttrain-kappa:-0.672112\n",
      "[164]\ttrain-kappa:-0.672157\n",
      "[165]\ttrain-kappa:-0.672367\n",
      "[166]\ttrain-kappa:-0.672496\n",
      "[167]\ttrain-kappa:-0.672690\n",
      "[168]\ttrain-kappa:-0.672805\n",
      "[169]\ttrain-kappa:-0.673035\n",
      "[170]\ttrain-kappa:-0.673342\n",
      "[171]\ttrain-kappa:-0.674070\n",
      "[172]\ttrain-kappa:-0.674169\n",
      "[173]\ttrain-kappa:-0.674743\n",
      "[174]\ttrain-kappa:-0.674898\n",
      "[175]\ttrain-kappa:-0.675180\n",
      "[176]\ttrain-kappa:-0.675479\n",
      "[177]\ttrain-kappa:-0.675780\n",
      "[178]\ttrain-kappa:-0.676046\n",
      "[179]\ttrain-kappa:-0.676269\n",
      "[180]\ttrain-kappa:-0.676474\n",
      "[181]\ttrain-kappa:-0.676868\n",
      "[182]\ttrain-kappa:-0.677201\n",
      "[183]\ttrain-kappa:-0.677436\n",
      "[184]\ttrain-kappa:-0.677532\n",
      "[185]\ttrain-kappa:-0.677702\n",
      "[186]\ttrain-kappa:-0.678065\n",
      "[187]\ttrain-kappa:-0.678339\n",
      "[188]\ttrain-kappa:-0.678782\n",
      "[189]\ttrain-kappa:-0.679086\n",
      "[190]\ttrain-kappa:-0.679442\n",
      "[191]\ttrain-kappa:-0.679801\n",
      "[192]\ttrain-kappa:-0.679829\n",
      "[193]\ttrain-kappa:-0.680170\n",
      "[194]\ttrain-kappa:-0.680645\n",
      "[195]\ttrain-kappa:-0.680784\n",
      "[196]\ttrain-kappa:-0.680927\n",
      "[197]\ttrain-kappa:-0.681075\n",
      "[198]\ttrain-kappa:-0.681341\n",
      "[199]\ttrain-kappa:-0.681605\n",
      "[200]\ttrain-kappa:-0.681797\n",
      "[201]\ttrain-kappa:-0.681971\n",
      "[202]\ttrain-kappa:-0.681962\n",
      "[203]\ttrain-kappa:-0.682177\n",
      "[204]\ttrain-kappa:-0.682295\n",
      "[205]\ttrain-kappa:-0.682743\n",
      "[206]\ttrain-kappa:-0.682803\n",
      "[207]\ttrain-kappa:-0.682873\n",
      "[208]\ttrain-kappa:-0.683209\n",
      "[209]\ttrain-kappa:-0.683247\n",
      "[210]\ttrain-kappa:-0.683512\n",
      "[211]\ttrain-kappa:-0.683518\n",
      "[212]\ttrain-kappa:-0.683753\n",
      "[213]\ttrain-kappa:-0.683932\n",
      "[214]\ttrain-kappa:-0.684169\n",
      "[215]\ttrain-kappa:-0.684430\n",
      "[216]\ttrain-kappa:-0.684561\n",
      "[217]\ttrain-kappa:-0.684778\n",
      "[218]\ttrain-kappa:-0.684914\n",
      "[219]\ttrain-kappa:-0.685168\n",
      "[220]\ttrain-kappa:-0.685529\n",
      "[221]\ttrain-kappa:-0.685658\n",
      "[222]\ttrain-kappa:-0.685971\n",
      "[223]\ttrain-kappa:-0.686115\n",
      "[224]\ttrain-kappa:-0.686297\n",
      "[225]\ttrain-kappa:-0.686718\n",
      "[226]\ttrain-kappa:-0.686778\n",
      "[227]\ttrain-kappa:-0.686906\n",
      "[228]\ttrain-kappa:-0.687220\n",
      "[229]\ttrain-kappa:-0.687297\n",
      "[230]\ttrain-kappa:-0.687707\n",
      "[231]\ttrain-kappa:-0.687833\n",
      "[232]\ttrain-kappa:-0.687897\n",
      "[233]\ttrain-kappa:-0.688058\n",
      "[234]\ttrain-kappa:-0.688393\n",
      "[235]\ttrain-kappa:-0.688599\n",
      "[236]\ttrain-kappa:-0.688791\n",
      "[237]\ttrain-kappa:-0.688850\n",
      "[238]\ttrain-kappa:-0.689018\n",
      "[239]\ttrain-kappa:-0.689339\n",
      "[240]\ttrain-kappa:-0.689655\n",
      "[241]\ttrain-kappa:-0.689887\n",
      "[242]\ttrain-kappa:-0.690262\n",
      "[243]\ttrain-kappa:-0.690359\n",
      "[244]\ttrain-kappa:-0.690497\n",
      "[245]\ttrain-kappa:-0.690681\n",
      "[246]\ttrain-kappa:-0.690992\n",
      "[247]\ttrain-kappa:-0.691151\n",
      "[248]\ttrain-kappa:-0.691353\n",
      "[249]\ttrain-kappa:-0.691427\n",
      "[250]\ttrain-kappa:-0.691756\n",
      "[251]\ttrain-kappa:-0.691975\n",
      "[252]\ttrain-kappa:-0.692037\n",
      "[253]\ttrain-kappa:-0.692203\n",
      "[254]\ttrain-kappa:-0.692291\n",
      "[255]\ttrain-kappa:-0.692425\n",
      "[256]\ttrain-kappa:-0.692703\n",
      "[257]\ttrain-kappa:-0.692734\n",
      "[258]\ttrain-kappa:-0.692805\n",
      "[259]\ttrain-kappa:-0.693093\n",
      "[260]\ttrain-kappa:-0.693257\n",
      "[261]\ttrain-kappa:-0.693579\n",
      "[262]\ttrain-kappa:-0.693454\n",
      "[263]\ttrain-kappa:-0.693588\n",
      "[264]\ttrain-kappa:-0.693678\n",
      "[265]\ttrain-kappa:-0.693833\n",
      "[266]\ttrain-kappa:-0.693882\n",
      "[267]\ttrain-kappa:-0.694069\n",
      "[268]\ttrain-kappa:-0.694597\n",
      "[269]\ttrain-kappa:-0.694958\n",
      "[270]\ttrain-kappa:-0.695110\n",
      "[271]\ttrain-kappa:-0.695147\n",
      "[272]\ttrain-kappa:-0.695448\n",
      "[273]\ttrain-kappa:-0.695725\n",
      "[274]\ttrain-kappa:-0.695910\n",
      "[275]\ttrain-kappa:-0.695984\n",
      "[276]\ttrain-kappa:-0.696035\n",
      "[277]\ttrain-kappa:-0.696318\n",
      "[278]\ttrain-kappa:-0.696600\n",
      "[279]\ttrain-kappa:-0.696586\n",
      "[280]\ttrain-kappa:-0.696744\n",
      "[281]\ttrain-kappa:-0.696973\n",
      "[282]\ttrain-kappa:-0.697069\n",
      "[283]\ttrain-kappa:-0.697359\n",
      "[284]\ttrain-kappa:-0.697404\n",
      "[285]\ttrain-kappa:-0.697573\n",
      "[286]\ttrain-kappa:-0.697809\n",
      "[287]\ttrain-kappa:-0.697983\n",
      "[288]\ttrain-kappa:-0.698210\n",
      "[289]\ttrain-kappa:-0.698315\n",
      "[290]\ttrain-kappa:-0.698455\n",
      "[291]\ttrain-kappa:-0.698776\n",
      "[292]\ttrain-kappa:-0.698864\n",
      "[293]\ttrain-kappa:-0.699020\n",
      "[294]\ttrain-kappa:-0.699186\n",
      "[295]\ttrain-kappa:-0.699395\n",
      "[296]\ttrain-kappa:-0.699582\n",
      "[297]\ttrain-kappa:-0.699689\n",
      "[298]\ttrain-kappa:-0.699895\n",
      "[299]\ttrain-kappa:-0.699975\n",
      "[300]\ttrain-kappa:-0.700470\n",
      "[301]\ttrain-kappa:-0.700570\n",
      "[302]\ttrain-kappa:-0.700711\n",
      "[303]\ttrain-kappa:-0.700934\n",
      "[304]\ttrain-kappa:-0.701253\n",
      "[305]\ttrain-kappa:-0.701428\n",
      "[306]\ttrain-kappa:-0.701731\n",
      "[307]\ttrain-kappa:-0.701678\n",
      "[308]\ttrain-kappa:-0.701846\n",
      "[309]\ttrain-kappa:-0.701960\n",
      "[310]\ttrain-kappa:-0.702065\n",
      "[311]\ttrain-kappa:-0.702207\n",
      "[312]\ttrain-kappa:-0.702529\n",
      "[313]\ttrain-kappa:-0.702561\n",
      "[314]\ttrain-kappa:-0.702885\n",
      "[315]\ttrain-kappa:-0.702981\n",
      "[316]\ttrain-kappa:-0.703030\n",
      "[317]\ttrain-kappa:-0.702975\n",
      "[318]\ttrain-kappa:-0.703145\n",
      "[319]\ttrain-kappa:-0.703269\n",
      "[320]\ttrain-kappa:-0.703496\n",
      "[321]\ttrain-kappa:-0.703626\n",
      "[322]\ttrain-kappa:-0.703887\n",
      "[323]\ttrain-kappa:-0.703999\n",
      "[324]\ttrain-kappa:-0.704198\n",
      "[325]\ttrain-kappa:-0.704300\n",
      "[326]\ttrain-kappa:-0.704562\n",
      "[327]\ttrain-kappa:-0.704662\n",
      "[328]\ttrain-kappa:-0.704755\n",
      "[329]\ttrain-kappa:-0.704905\n",
      "[330]\ttrain-kappa:-0.704991\n",
      "[331]\ttrain-kappa:-0.705044\n",
      "[332]\ttrain-kappa:-0.705179\n",
      "[333]\ttrain-kappa:-0.705382\n",
      "[334]\ttrain-kappa:-0.705488\n",
      "[335]\ttrain-kappa:-0.705663\n",
      "[336]\ttrain-kappa:-0.705773\n",
      "[337]\ttrain-kappa:-0.706065\n",
      "[338]\ttrain-kappa:-0.706070\n",
      "[339]\ttrain-kappa:-0.706326\n",
      "[340]\ttrain-kappa:-0.706428\n",
      "[341]\ttrain-kappa:-0.706571\n",
      "[342]\ttrain-kappa:-0.706717\n",
      "[343]\ttrain-kappa:-0.706807\n",
      "[344]\ttrain-kappa:-0.706837\n",
      "[345]\ttrain-kappa:-0.706947\n",
      "[346]\ttrain-kappa:-0.707083\n",
      "[347]\ttrain-kappa:-0.707203\n",
      "[348]\ttrain-kappa:-0.707310\n",
      "[349]\ttrain-kappa:-0.707351\n",
      "[350]\ttrain-kappa:-0.707553\n",
      "[351]\ttrain-kappa:-0.707820\n",
      "[352]\ttrain-kappa:-0.707780\n",
      "[353]\ttrain-kappa:-0.707871\n",
      "[354]\ttrain-kappa:-0.707940\n",
      "[355]\ttrain-kappa:-0.708063\n",
      "[356]\ttrain-kappa:-0.708232\n",
      "[357]\ttrain-kappa:-0.708373\n",
      "[358]\ttrain-kappa:-0.708494\n",
      "[359]\ttrain-kappa:-0.708750\n",
      "[360]\ttrain-kappa:-0.708824\n",
      "[361]\ttrain-kappa:-0.708956\n",
      "[362]\ttrain-kappa:-0.709073\n",
      "[363]\ttrain-kappa:-0.709096\n",
      "[364]\ttrain-kappa:-0.709300\n",
      "[365]\ttrain-kappa:-0.709628\n",
      "[366]\ttrain-kappa:-0.709765\n",
      "[367]\ttrain-kappa:-0.709854\n",
      "[368]\ttrain-kappa:-0.709918\n",
      "[369]\ttrain-kappa:-0.709965\n",
      "[370]\ttrain-kappa:-0.710072\n",
      "[371]\ttrain-kappa:-0.710289\n",
      "[372]\ttrain-kappa:-0.710492\n",
      "[373]\ttrain-kappa:-0.710659\n",
      "[374]\ttrain-kappa:-0.710708\n",
      "[375]\ttrain-kappa:-0.710641\n",
      "[376]\ttrain-kappa:-0.710915\n",
      "[377]\ttrain-kappa:-0.710992\n",
      "[378]\ttrain-kappa:-0.711172\n",
      "[379]\ttrain-kappa:-0.711420\n",
      "[380]\ttrain-kappa:-0.711639\n",
      "[381]\ttrain-kappa:-0.711711\n",
      "[382]\ttrain-kappa:-0.711813\n",
      "[383]\ttrain-kappa:-0.711857\n",
      "[384]\ttrain-kappa:-0.711981\n",
      "[385]\ttrain-kappa:-0.712096\n",
      "[386]\ttrain-kappa:-0.712152\n",
      "[387]\ttrain-kappa:-0.712234\n",
      "[388]\ttrain-kappa:-0.712261\n",
      "[389]\ttrain-kappa:-0.712368\n",
      "[390]\ttrain-kappa:-0.712555\n",
      "[391]\ttrain-kappa:-0.712787\n",
      "[392]\ttrain-kappa:-0.713083\n",
      "[393]\ttrain-kappa:-0.713323\n",
      "[394]\ttrain-kappa:-0.713533\n",
      "[395]\ttrain-kappa:-0.713633\n",
      "[396]\ttrain-kappa:-0.713685\n",
      "[397]\ttrain-kappa:-0.713796\n",
      "[398]\ttrain-kappa:-0.713912\n",
      "[399]\ttrain-kappa:-0.713856\n",
      "[400]\ttrain-kappa:-0.714124\n",
      "[401]\ttrain-kappa:-0.714346\n",
      "[402]\ttrain-kappa:-0.714501\n",
      "[403]\ttrain-kappa:-0.714591\n",
      "[404]\ttrain-kappa:-0.714766\n",
      "[405]\ttrain-kappa:-0.714953\n",
      "[406]\ttrain-kappa:-0.715207\n",
      "[407]\ttrain-kappa:-0.715450\n",
      "[408]\ttrain-kappa:-0.715652\n",
      "[409]\ttrain-kappa:-0.715755\n",
      "[410]\ttrain-kappa:-0.715936\n",
      "[411]\ttrain-kappa:-0.715949\n",
      "[412]\ttrain-kappa:-0.715969\n",
      "[413]\ttrain-kappa:-0.716232\n",
      "[414]\ttrain-kappa:-0.716476\n",
      "[415]\ttrain-kappa:-0.716548\n",
      "[416]\ttrain-kappa:-0.716622\n",
      "[417]\ttrain-kappa:-0.716737\n",
      "[418]\ttrain-kappa:-0.716940\n",
      "[419]\ttrain-kappa:-0.717081\n",
      "[420]\ttrain-kappa:-0.717293\n",
      "[421]\ttrain-kappa:-0.717368\n",
      "[422]\ttrain-kappa:-0.717462\n",
      "[423]\ttrain-kappa:-0.717660\n",
      "[424]\ttrain-kappa:-0.717848\n",
      "[425]\ttrain-kappa:-0.717967\n",
      "[426]\ttrain-kappa:-0.718079\n",
      "[427]\ttrain-kappa:-0.718306\n",
      "[428]\ttrain-kappa:-0.718411\n",
      "[429]\ttrain-kappa:-0.718642\n",
      "[430]\ttrain-kappa:-0.718672\n",
      "[431]\ttrain-kappa:-0.718706\n",
      "[432]\ttrain-kappa:-0.719008\n",
      "[433]\ttrain-kappa:-0.719175\n",
      "[434]\ttrain-kappa:-0.719232\n",
      "[435]\ttrain-kappa:-0.719228\n",
      "[436]\ttrain-kappa:-0.719261\n",
      "[437]\ttrain-kappa:-0.719365\n",
      "[438]\ttrain-kappa:-0.719435\n",
      "[439]\ttrain-kappa:-0.719795\n",
      "[440]\ttrain-kappa:-0.719893\n",
      "[441]\ttrain-kappa:-0.720072\n",
      "[442]\ttrain-kappa:-0.720136\n",
      "[443]\ttrain-kappa:-0.720272\n",
      "[444]\ttrain-kappa:-0.720413\n",
      "[445]\ttrain-kappa:-0.720670\n",
      "[446]\ttrain-kappa:-0.720776\n",
      "[447]\ttrain-kappa:-0.720961\n",
      "[448]\ttrain-kappa:-0.721037\n",
      "[449]\ttrain-kappa:-0.721095\n",
      "[450]\ttrain-kappa:-0.721037\n",
      "[451]\ttrain-kappa:-0.721189\n",
      "[452]\ttrain-kappa:-0.721263\n",
      "[453]\ttrain-kappa:-0.721622\n",
      "[454]\ttrain-kappa:-0.721652\n",
      "[455]\ttrain-kappa:-0.721794\n",
      "[456]\ttrain-kappa:-0.721897\n",
      "[457]\ttrain-kappa:-0.721998\n",
      "[458]\ttrain-kappa:-0.722217\n",
      "[459]\ttrain-kappa:-0.722299\n",
      "[460]\ttrain-kappa:-0.722461\n",
      "[461]\ttrain-kappa:-0.722558\n",
      "[462]\ttrain-kappa:-0.722731\n",
      "[463]\ttrain-kappa:-0.722807\n",
      "[464]\ttrain-kappa:-0.722823\n",
      "[465]\ttrain-kappa:-0.722913\n",
      "[466]\ttrain-kappa:-0.723071\n",
      "[467]\ttrain-kappa:-0.723183\n",
      "[468]\ttrain-kappa:-0.723152\n",
      "[469]\ttrain-kappa:-0.723239\n",
      "[470]\ttrain-kappa:-0.723372\n",
      "[471]\ttrain-kappa:-0.723505\n",
      "[472]\ttrain-kappa:-0.723561\n",
      "[473]\ttrain-kappa:-0.723672\n",
      "[474]\ttrain-kappa:-0.723919\n",
      "[475]\ttrain-kappa:-0.724009\n",
      "[476]\ttrain-kappa:-0.724050\n",
      "[477]\ttrain-kappa:-0.724190\n",
      "[478]\ttrain-kappa:-0.724221\n",
      "[479]\ttrain-kappa:-0.724305\n",
      "[480]\ttrain-kappa:-0.724318\n",
      "[481]\ttrain-kappa:-0.724460\n",
      "[482]\ttrain-kappa:-0.724570\n",
      "[483]\ttrain-kappa:-0.724731\n",
      "[484]\ttrain-kappa:-0.724803\n",
      "[485]\ttrain-kappa:-0.724864\n",
      "[486]\ttrain-kappa:-0.725137\n",
      "[487]\ttrain-kappa:-0.725359\n",
      "[488]\ttrain-kappa:-0.725387\n",
      "[489]\ttrain-kappa:-0.725431\n",
      "[490]\ttrain-kappa:-0.725586\n",
      "[491]\ttrain-kappa:-0.725680\n",
      "[492]\ttrain-kappa:-0.725930\n",
      "[493]\ttrain-kappa:-0.725992\n",
      "[494]\ttrain-kappa:-0.726034\n",
      "[495]\ttrain-kappa:-0.726103\n",
      "[496]\ttrain-kappa:-0.726159\n",
      "[497]\ttrain-kappa:-0.726196\n",
      "[498]\ttrain-kappa:-0.726314\n",
      "[499]\ttrain-kappa:-0.726323\n",
      "[500]\ttrain-kappa:-0.726389\n",
      "[501]\ttrain-kappa:-0.726462\n",
      "[502]\ttrain-kappa:-0.726638\n",
      "[503]\ttrain-kappa:-0.726660\n",
      "[504]\ttrain-kappa:-0.726771\n",
      "[505]\ttrain-kappa:-0.726884\n",
      "[506]\ttrain-kappa:-0.727008\n",
      "[507]\ttrain-kappa:-0.727156\n",
      "[508]\ttrain-kappa:-0.727193\n",
      "[509]\ttrain-kappa:-0.727277\n",
      "[510]\ttrain-kappa:-0.727310\n",
      "[511]\ttrain-kappa:-0.727443\n",
      "[512]\ttrain-kappa:-0.727502\n",
      "[513]\ttrain-kappa:-0.727458\n",
      "[514]\ttrain-kappa:-0.727533\n",
      "[515]\ttrain-kappa:-0.727639\n",
      "[516]\ttrain-kappa:-0.727796\n",
      "[517]\ttrain-kappa:-0.727925\n",
      "[518]\ttrain-kappa:-0.728088\n",
      "[519]\ttrain-kappa:-0.728132\n",
      "[520]\ttrain-kappa:-0.728109\n",
      "[521]\ttrain-kappa:-0.728313\n",
      "[522]\ttrain-kappa:-0.728434\n",
      "[523]\ttrain-kappa:-0.728724\n",
      "[524]\ttrain-kappa:-0.728761\n",
      "[525]\ttrain-kappa:-0.728842\n",
      "[526]\ttrain-kappa:-0.729221\n",
      "[527]\ttrain-kappa:-0.729401\n",
      "[528]\ttrain-kappa:-0.729622\n",
      "[529]\ttrain-kappa:-0.729793\n",
      "[530]\ttrain-kappa:-0.729780\n",
      "[531]\ttrain-kappa:-0.730032\n",
      "[532]\ttrain-kappa:-0.730096\n",
      "[533]\ttrain-kappa:-0.730161\n",
      "[534]\ttrain-kappa:-0.730163\n",
      "[535]\ttrain-kappa:-0.730267\n",
      "[536]\ttrain-kappa:-0.730279\n",
      "[537]\ttrain-kappa:-0.730425\n",
      "[538]\ttrain-kappa:-0.730674\n",
      "[539]\ttrain-kappa:-0.730743\n",
      "[540]\ttrain-kappa:-0.730951\n",
      "[541]\ttrain-kappa:-0.730936\n",
      "[542]\ttrain-kappa:-0.731040\n",
      "[543]\ttrain-kappa:-0.731293\n",
      "[544]\ttrain-kappa:-0.731291\n",
      "[545]\ttrain-kappa:-0.731408\n",
      "[546]\ttrain-kappa:-0.731525\n",
      "[547]\ttrain-kappa:-0.731789\n",
      "[548]\ttrain-kappa:-0.731915\n",
      "[549]\ttrain-kappa:-0.732006\n",
      "[550]\ttrain-kappa:-0.732064\n",
      "[551]\ttrain-kappa:-0.732207\n",
      "[552]\ttrain-kappa:-0.732271\n",
      "[553]\ttrain-kappa:-0.732223\n",
      "[554]\ttrain-kappa:-0.732299\n",
      "[555]\ttrain-kappa:-0.732436\n",
      "[556]\ttrain-kappa:-0.732454\n",
      "[557]\ttrain-kappa:-0.732455\n",
      "[558]\ttrain-kappa:-0.732564\n",
      "[559]\ttrain-kappa:-0.732598\n",
      "[560]\ttrain-kappa:-0.732751\n",
      "[561]\ttrain-kappa:-0.732767\n",
      "[562]\ttrain-kappa:-0.732909\n",
      "[563]\ttrain-kappa:-0.733013\n",
      "[564]\ttrain-kappa:-0.733163\n",
      "[565]\ttrain-kappa:-0.733214\n",
      "[566]\ttrain-kappa:-0.733288\n",
      "[567]\ttrain-kappa:-0.733403\n",
      "[568]\ttrain-kappa:-0.733298\n",
      "[569]\ttrain-kappa:-0.733408\n",
      "[570]\ttrain-kappa:-0.733544\n",
      "[571]\ttrain-kappa:-0.733702\n",
      "[572]\ttrain-kappa:-0.733802\n",
      "[573]\ttrain-kappa:-0.733799\n",
      "[574]\ttrain-kappa:-0.734153\n",
      "[575]\ttrain-kappa:-0.734193\n",
      "[576]\ttrain-kappa:-0.734493\n",
      "[577]\ttrain-kappa:-0.734662\n",
      "[578]\ttrain-kappa:-0.734883\n",
      "[579]\ttrain-kappa:-0.735070\n",
      "[580]\ttrain-kappa:-0.735098\n",
      "[581]\ttrain-kappa:-0.735182\n",
      "[582]\ttrain-kappa:-0.735248\n",
      "[583]\ttrain-kappa:-0.735357\n",
      "[584]\ttrain-kappa:-0.735337\n",
      "[585]\ttrain-kappa:-0.735382\n",
      "[586]\ttrain-kappa:-0.735431\n",
      "[587]\ttrain-kappa:-0.735560\n",
      "[588]\ttrain-kappa:-0.735583\n",
      "[589]\ttrain-kappa:-0.735918\n",
      "[590]\ttrain-kappa:-0.735884\n",
      "[591]\ttrain-kappa:-0.736123\n",
      "[592]\ttrain-kappa:-0.736055\n",
      "[593]\ttrain-kappa:-0.736023\n",
      "[594]\ttrain-kappa:-0.736210\n",
      "[595]\ttrain-kappa:-0.736296\n",
      "[596]\ttrain-kappa:-0.736390\n",
      "[597]\ttrain-kappa:-0.736482\n",
      "[598]\ttrain-kappa:-0.736734\n",
      "[599]\ttrain-kappa:-0.736938\n",
      "[600]\ttrain-kappa:-0.736998\n",
      "[601]\ttrain-kappa:-0.737176\n",
      "[602]\ttrain-kappa:-0.737218\n",
      "[603]\ttrain-kappa:-0.737279\n",
      "[604]\ttrain-kappa:-0.737291\n",
      "[605]\ttrain-kappa:-0.737314\n",
      "[606]\ttrain-kappa:-0.737465\n",
      "[607]\ttrain-kappa:-0.737689\n",
      "[608]\ttrain-kappa:-0.737788\n",
      "[609]\ttrain-kappa:-0.737816\n",
      "[610]\ttrain-kappa:-0.737905\n",
      "[611]\ttrain-kappa:-0.737961\n",
      "[612]\ttrain-kappa:-0.738117\n",
      "[613]\ttrain-kappa:-0.738244\n",
      "[614]\ttrain-kappa:-0.738240\n",
      "[615]\ttrain-kappa:-0.738249\n",
      "[616]\ttrain-kappa:-0.738327\n",
      "[617]\ttrain-kappa:-0.738262\n",
      "[618]\ttrain-kappa:-0.738354\n",
      "[619]\ttrain-kappa:-0.738632\n",
      "[620]\ttrain-kappa:-0.738650\n",
      "[621]\ttrain-kappa:-0.738726\n",
      "[622]\ttrain-kappa:-0.738878\n",
      "[623]\ttrain-kappa:-0.738933\n",
      "[624]\ttrain-kappa:-0.739044\n",
      "[625]\ttrain-kappa:-0.739151\n",
      "[626]\ttrain-kappa:-0.739246\n",
      "[627]\ttrain-kappa:-0.739281\n",
      "[628]\ttrain-kappa:-0.739367\n",
      "[629]\ttrain-kappa:-0.739506\n",
      "[630]\ttrain-kappa:-0.739591\n",
      "[631]\ttrain-kappa:-0.739708\n",
      "[632]\ttrain-kappa:-0.739857\n",
      "[633]\ttrain-kappa:-0.739930\n",
      "[634]\ttrain-kappa:-0.740034\n",
      "[635]\ttrain-kappa:-0.740111\n",
      "[636]\ttrain-kappa:-0.740260\n",
      "[637]\ttrain-kappa:-0.740509\n",
      "[638]\ttrain-kappa:-0.740576\n",
      "[639]\ttrain-kappa:-0.740833\n",
      "[640]\ttrain-kappa:-0.740891\n",
      "[641]\ttrain-kappa:-0.741023\n",
      "[642]\ttrain-kappa:-0.741123\n",
      "[643]\ttrain-kappa:-0.741100\n",
      "[644]\ttrain-kappa:-0.741166\n",
      "[645]\ttrain-kappa:-0.741337\n",
      "[646]\ttrain-kappa:-0.741409\n",
      "[647]\ttrain-kappa:-0.741389\n",
      "[648]\ttrain-kappa:-0.741383\n",
      "[649]\ttrain-kappa:-0.741340\n",
      "[650]\ttrain-kappa:-0.741484\n",
      "[651]\ttrain-kappa:-0.741645\n",
      "[652]\ttrain-kappa:-0.741792\n",
      "[653]\ttrain-kappa:-0.742039\n",
      "[654]\ttrain-kappa:-0.742039\n",
      "[655]\ttrain-kappa:-0.742092\n",
      "[656]\ttrain-kappa:-0.742140\n",
      "[657]\ttrain-kappa:-0.742260\n",
      "[658]\ttrain-kappa:-0.742380\n",
      "[659]\ttrain-kappa:-0.742414\n",
      "[660]\ttrain-kappa:-0.742557\n",
      "[661]\ttrain-kappa:-0.742798\n",
      "[662]\ttrain-kappa:-0.742818\n",
      "[663]\ttrain-kappa:-0.742915\n",
      "[664]\ttrain-kappa:-0.742927\n",
      "[665]\ttrain-kappa:-0.743181\n",
      "[666]\ttrain-kappa:-0.743290\n",
      "[667]\ttrain-kappa:-0.743224\n",
      "[668]\ttrain-kappa:-0.743301\n",
      "[669]\ttrain-kappa:-0.743220\n",
      "[670]\ttrain-kappa:-0.743262\n",
      "[671]\ttrain-kappa:-0.743316\n",
      "[672]\ttrain-kappa:-0.743376\n",
      "[673]\ttrain-kappa:-0.743590\n",
      "[674]\ttrain-kappa:-0.743679\n",
      "[675]\ttrain-kappa:-0.743755\n",
      "[676]\ttrain-kappa:-0.743941\n",
      "[677]\ttrain-kappa:-0.743959\n",
      "[678]\ttrain-kappa:-0.744048\n",
      "[679]\ttrain-kappa:-0.744110\n",
      "[680]\ttrain-kappa:-0.744112\n",
      "[681]\ttrain-kappa:-0.744184\n",
      "[682]\ttrain-kappa:-0.744200\n",
      "[683]\ttrain-kappa:-0.744271\n",
      "[684]\ttrain-kappa:-0.744511\n",
      "[685]\ttrain-kappa:-0.744611\n",
      "[686]\ttrain-kappa:-0.744761\n",
      "[687]\ttrain-kappa:-0.744754\n",
      "[688]\ttrain-kappa:-0.744845\n",
      "[689]\ttrain-kappa:-0.744879\n",
      "[690]\ttrain-kappa:-0.745076\n",
      "[691]\ttrain-kappa:-0.745086\n",
      "[692]\ttrain-kappa:-0.745176\n",
      "[693]\ttrain-kappa:-0.745164\n",
      "[694]\ttrain-kappa:-0.745189\n",
      "[695]\ttrain-kappa:-0.745530\n",
      "[696]\ttrain-kappa:-0.745571\n",
      "[697]\ttrain-kappa:-0.745673\n",
      "[698]\ttrain-kappa:-0.745754\n",
      "[699]\ttrain-kappa:-0.745904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.750802\n",
      "         Iterations: 140\n",
      "         Function evaluations: 283\n",
      "Train QWK: 0.7508021494\n",
      "\n",
      "0.65347253962"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-kappa:-0.000000\n",
      "[1]\ttrain-kappa:-0.000000\n",
      "[2]\ttrain-kappa:-0.000000\n",
      "[3]\ttrain-kappa:-0.000000\n",
      "[4]\ttrain-kappa:-0.000000\n",
      "[5]\ttrain-kappa:-0.000000\n",
      "[6]\ttrain-kappa:-0.000000\n",
      "[7]\ttrain-kappa:-0.000000\n",
      "[8]\ttrain-kappa:-0.000000\n",
      "[9]\ttrain-kappa:-0.027221\n",
      "[10]\ttrain-kappa:-0.044445\n",
      "[11]\ttrain-kappa:-0.054444\n",
      "[12]\ttrain-kappa:-0.057209\n",
      "[13]\ttrain-kappa:-0.055807\n",
      "[14]\ttrain-kappa:-0.053280\n",
      "[15]\ttrain-kappa:-0.049959\n",
      "[16]\ttrain-kappa:-0.046134\n",
      "[17]\ttrain-kappa:-0.042143\n",
      "[18]\ttrain-kappa:-0.038419\n",
      "[19]\ttrain-kappa:-0.034509\n",
      "[20]\ttrain-kappa:-0.031719\n",
      "[21]\ttrain-kappa:-0.037911\n",
      "[22]\ttrain-kappa:-0.064180\n",
      "[23]\ttrain-kappa:-0.073492\n",
      "[24]\ttrain-kappa:-0.080168\n",
      "[25]\ttrain-kappa:-0.087199\n",
      "[26]\ttrain-kappa:-0.092708\n",
      "[27]\ttrain-kappa:-0.096091\n",
      "[28]\ttrain-kappa:-0.098282\n",
      "[29]\ttrain-kappa:-0.099380\n",
      "[30]\ttrain-kappa:-0.100208\n",
      "[31]\ttrain-kappa:-0.106030\n",
      "[32]\ttrain-kappa:-0.126998\n",
      "[33]\ttrain-kappa:-0.149147\n",
      "[34]\ttrain-kappa:-0.160082\n",
      "[35]\ttrain-kappa:-0.166515\n",
      "[36]\ttrain-kappa:-0.172198\n",
      "[37]\ttrain-kappa:-0.178094\n",
      "[38]\ttrain-kappa:-0.183910\n",
      "[39]\ttrain-kappa:-0.189783\n",
      "[40]\ttrain-kappa:-0.195345\n",
      "[41]\ttrain-kappa:-0.213096\n",
      "[42]\ttrain-kappa:-0.239661\n",
      "[43]\ttrain-kappa:-0.261801\n",
      "[44]\ttrain-kappa:-0.276658\n",
      "[45]\ttrain-kappa:-0.286333\n",
      "[46]\ttrain-kappa:-0.292684\n",
      "[47]\ttrain-kappa:-0.299943\n",
      "[48]\ttrain-kappa:-0.306206\n",
      "[49]\ttrain-kappa:-0.312398\n",
      "[50]\ttrain-kappa:-0.318599\n",
      "[51]\ttrain-kappa:-0.323136\n",
      "[52]\ttrain-kappa:-0.346540\n",
      "[53]\ttrain-kappa:-0.368002\n",
      "[54]\ttrain-kappa:-0.385692\n",
      "[55]\ttrain-kappa:-0.402200\n",
      "[56]\ttrain-kappa:-0.415507\n",
      "[57]\ttrain-kappa:-0.426174\n",
      "[58]\ttrain-kappa:-0.434135\n",
      "[59]\ttrain-kappa:-0.441443\n",
      "[60]\ttrain-kappa:-0.447920\n",
      "[61]\ttrain-kappa:-0.453897\n",
      "[62]\ttrain-kappa:-0.459172\n",
      "[63]\ttrain-kappa:-0.464467\n",
      "[64]\ttrain-kappa:-0.469268\n",
      "[65]\ttrain-kappa:-0.474711\n",
      "[66]\ttrain-kappa:-0.479533\n",
      "[67]\ttrain-kappa:-0.491127\n",
      "[68]\ttrain-kappa:-0.503957\n",
      "[69]\ttrain-kappa:-0.511514\n",
      "[70]\ttrain-kappa:-0.520560\n",
      "[71]\ttrain-kappa:-0.530877\n",
      "[72]\ttrain-kappa:-0.538932\n",
      "[73]\ttrain-kappa:-0.546334\n",
      "[74]\ttrain-kappa:-0.553591\n",
      "[75]\ttrain-kappa:-0.559426\n",
      "[76]\ttrain-kappa:-0.565075\n",
      "[77]\ttrain-kappa:-0.570504\n",
      "[78]\ttrain-kappa:-0.575084\n",
      "[79]\ttrain-kappa:-0.578226\n",
      "[80]\ttrain-kappa:-0.581886\n",
      "[81]\ttrain-kappa:-0.584950\n",
      "[82]\ttrain-kappa:-0.587572\n",
      "[83]\ttrain-kappa:-0.590324\n",
      "[84]\ttrain-kappa:-0.592796\n",
      "[85]\ttrain-kappa:-0.595337\n",
      "[86]\ttrain-kappa:-0.598353\n",
      "[87]\ttrain-kappa:-0.603760\n",
      "[88]\ttrain-kappa:-0.607794\n",
      "[89]\ttrain-kappa:-0.611396\n",
      "[90]\ttrain-kappa:-0.613975\n",
      "[91]\ttrain-kappa:-0.616771\n",
      "[92]\ttrain-kappa:-0.619023\n",
      "[93]\ttrain-kappa:-0.621680\n",
      "[94]\ttrain-kappa:-0.625273\n",
      "[95]\ttrain-kappa:-0.627390\n",
      "[96]\ttrain-kappa:-0.629934\n",
      "[97]\ttrain-kappa:-0.631832\n",
      "[98]\ttrain-kappa:-0.634108\n",
      "[99]\ttrain-kappa:-0.635903\n",
      "[100]\ttrain-kappa:-0.637473\n",
      "[101]\ttrain-kappa:-0.639686\n",
      "[102]\ttrain-kappa:-0.641004\n",
      "[103]\ttrain-kappa:-0.642376\n",
      "[104]\ttrain-kappa:-0.643880\n",
      "[105]\ttrain-kappa:-0.645447\n",
      "[106]\ttrain-kappa:-0.647125\n",
      "[107]\ttrain-kappa:-0.648221\n",
      "[108]\ttrain-kappa:-0.649363\n",
      "[109]\ttrain-kappa:-0.650512\n",
      "[110]\ttrain-kappa:-0.651590\n",
      "[111]\ttrain-kappa:-0.652772\n",
      "[112]\ttrain-kappa:-0.653565\n",
      "[113]\ttrain-kappa:-0.654359\n",
      "[114]\ttrain-kappa:-0.655025\n",
      "[115]\ttrain-kappa:-0.655172\n",
      "[116]\ttrain-kappa:-0.655528\n",
      "[117]\ttrain-kappa:-0.656390\n",
      "[118]\ttrain-kappa:-0.656425\n",
      "[119]\ttrain-kappa:-0.657086\n",
      "[120]\ttrain-kappa:-0.657651\n",
      "[121]\ttrain-kappa:-0.658280\n",
      "[122]\ttrain-kappa:-0.659087\n",
      "[123]\ttrain-kappa:-0.659653\n",
      "[124]\ttrain-kappa:-0.660219\n",
      "[125]\ttrain-kappa:-0.660909\n",
      "[126]\ttrain-kappa:-0.661535\n",
      "[127]\ttrain-kappa:-0.662048\n",
      "[128]\ttrain-kappa:-0.662872\n",
      "[129]\ttrain-kappa:-0.662947\n",
      "[130]\ttrain-kappa:-0.663406\n",
      "[131]\ttrain-kappa:-0.663799\n",
      "[132]\ttrain-kappa:-0.664173\n",
      "[133]\ttrain-kappa:-0.664351\n",
      "[134]\ttrain-kappa:-0.664743\n",
      "[135]\ttrain-kappa:-0.664757\n",
      "[136]\ttrain-kappa:-0.665360\n",
      "[137]\ttrain-kappa:-0.665624\n",
      "[138]\ttrain-kappa:-0.665827\n",
      "[139]\ttrain-kappa:-0.666283\n",
      "[140]\ttrain-kappa:-0.666706\n",
      "[141]\ttrain-kappa:-0.666885\n",
      "[142]\ttrain-kappa:-0.667179\n",
      "[143]\ttrain-kappa:-0.667501\n",
      "[144]\ttrain-kappa:-0.667884\n",
      "[145]\ttrain-kappa:-0.668083\n",
      "[146]\ttrain-kappa:-0.668359\n",
      "[147]\ttrain-kappa:-0.668620\n",
      "[148]\ttrain-kappa:-0.668693\n",
      "[149]\ttrain-kappa:-0.668780\n",
      "[150]\ttrain-kappa:-0.669220\n",
      "[151]\ttrain-kappa:-0.669478\n",
      "[152]\ttrain-kappa:-0.669650\n",
      "[153]\ttrain-kappa:-0.669722\n",
      "[154]\ttrain-kappa:-0.670028\n",
      "[155]\ttrain-kappa:-0.670074\n",
      "[156]\ttrain-kappa:-0.670387\n",
      "[157]\ttrain-kappa:-0.670859\n",
      "[158]\ttrain-kappa:-0.671107\n",
      "[159]\ttrain-kappa:-0.671659\n",
      "[160]\ttrain-kappa:-0.671891\n",
      "[161]\ttrain-kappa:-0.672077\n",
      "[162]\ttrain-kappa:-0.672288\n",
      "[163]\ttrain-kappa:-0.672307\n",
      "[164]\ttrain-kappa:-0.672681\n",
      "[165]\ttrain-kappa:-0.673190\n",
      "[166]\ttrain-kappa:-0.673377\n",
      "[167]\ttrain-kappa:-0.673500\n",
      "[168]\ttrain-kappa:-0.673703\n",
      "[169]\ttrain-kappa:-0.673778\n",
      "[170]\ttrain-kappa:-0.674134\n",
      "[171]\ttrain-kappa:-0.674497\n",
      "[172]\ttrain-kappa:-0.674932\n",
      "[173]\ttrain-kappa:-0.675068\n",
      "[174]\ttrain-kappa:-0.675328\n",
      "[175]\ttrain-kappa:-0.675625\n",
      "[176]\ttrain-kappa:-0.675840\n",
      "[177]\ttrain-kappa:-0.676102\n",
      "[178]\ttrain-kappa:-0.676351\n",
      "[179]\ttrain-kappa:-0.676718\n",
      "[180]\ttrain-kappa:-0.677015\n",
      "[181]\ttrain-kappa:-0.677200\n",
      "[182]\ttrain-kappa:-0.677409\n",
      "[183]\ttrain-kappa:-0.678087\n",
      "[184]\ttrain-kappa:-0.678381\n",
      "[185]\ttrain-kappa:-0.678497\n",
      "[186]\ttrain-kappa:-0.678703\n",
      "[187]\ttrain-kappa:-0.678929\n",
      "[188]\ttrain-kappa:-0.679144\n",
      "[189]\ttrain-kappa:-0.679116\n",
      "[190]\ttrain-kappa:-0.679571\n",
      "[191]\ttrain-kappa:-0.679645\n",
      "[192]\ttrain-kappa:-0.679857\n",
      "[193]\ttrain-kappa:-0.680335\n",
      "[194]\ttrain-kappa:-0.680470\n",
      "[195]\ttrain-kappa:-0.680735\n",
      "[196]\ttrain-kappa:-0.680916\n",
      "[197]\ttrain-kappa:-0.681385\n",
      "[198]\ttrain-kappa:-0.681776\n",
      "[199]\ttrain-kappa:-0.682030\n",
      "[200]\ttrain-kappa:-0.682297\n",
      "[201]\ttrain-kappa:-0.682296\n",
      "[202]\ttrain-kappa:-0.682507\n",
      "[203]\ttrain-kappa:-0.682702\n",
      "[204]\ttrain-kappa:-0.683107\n",
      "[205]\ttrain-kappa:-0.683142\n",
      "[206]\ttrain-kappa:-0.683339\n",
      "[207]\ttrain-kappa:-0.683586\n",
      "[208]\ttrain-kappa:-0.683954\n",
      "[209]\ttrain-kappa:-0.684203\n",
      "[210]\ttrain-kappa:-0.684264\n",
      "[211]\ttrain-kappa:-0.684352\n",
      "[212]\ttrain-kappa:-0.684750\n",
      "[213]\ttrain-kappa:-0.684936\n",
      "[214]\ttrain-kappa:-0.684985\n",
      "[215]\ttrain-kappa:-0.685290\n",
      "[216]\ttrain-kappa:-0.685333\n",
      "[217]\ttrain-kappa:-0.685504\n",
      "[218]\ttrain-kappa:-0.685651\n",
      "[219]\ttrain-kappa:-0.685827\n",
      "[220]\ttrain-kappa:-0.685983\n",
      "[221]\ttrain-kappa:-0.686339\n",
      "[222]\ttrain-kappa:-0.686429\n",
      "[223]\ttrain-kappa:-0.686547\n",
      "[224]\ttrain-kappa:-0.686858\n",
      "[225]\ttrain-kappa:-0.687088\n",
      "[226]\ttrain-kappa:-0.687292\n",
      "[227]\ttrain-kappa:-0.687671\n",
      "[228]\ttrain-kappa:-0.687713\n",
      "[229]\ttrain-kappa:-0.687958\n",
      "[230]\ttrain-kappa:-0.688021\n",
      "[231]\ttrain-kappa:-0.688235\n",
      "[232]\ttrain-kappa:-0.688307\n",
      "[233]\ttrain-kappa:-0.688325\n",
      "[234]\ttrain-kappa:-0.688412\n",
      "[235]\ttrain-kappa:-0.688610\n",
      "[236]\ttrain-kappa:-0.688770\n",
      "[237]\ttrain-kappa:-0.688938\n",
      "[238]\ttrain-kappa:-0.689016\n",
      "[239]\ttrain-kappa:-0.689219\n",
      "[240]\ttrain-kappa:-0.689513\n",
      "[241]\ttrain-kappa:-0.689725\n",
      "[242]\ttrain-kappa:-0.689928\n",
      "[243]\ttrain-kappa:-0.690182\n",
      "[244]\ttrain-kappa:-0.690149\n",
      "[245]\ttrain-kappa:-0.690261\n",
      "[246]\ttrain-kappa:-0.690492\n",
      "[247]\ttrain-kappa:-0.690510\n",
      "[248]\ttrain-kappa:-0.690877\n",
      "[249]\ttrain-kappa:-0.690962\n",
      "[250]\ttrain-kappa:-0.691271\n",
      "[251]\ttrain-kappa:-0.691452\n",
      "[252]\ttrain-kappa:-0.691700\n",
      "[253]\ttrain-kappa:-0.691993\n",
      "[254]\ttrain-kappa:-0.692231\n",
      "[255]\ttrain-kappa:-0.692415\n",
      "[256]\ttrain-kappa:-0.692644\n",
      "[257]\ttrain-kappa:-0.692775\n",
      "[258]\ttrain-kappa:-0.693032\n",
      "[259]\ttrain-kappa:-0.692931\n",
      "[260]\ttrain-kappa:-0.693323\n",
      "[261]\ttrain-kappa:-0.693466\n",
      "[262]\ttrain-kappa:-0.693744\n",
      "[263]\ttrain-kappa:-0.693794\n",
      "[264]\ttrain-kappa:-0.693842\n",
      "[265]\ttrain-kappa:-0.694095\n",
      "[266]\ttrain-kappa:-0.694384\n",
      "[267]\ttrain-kappa:-0.694523\n",
      "[268]\ttrain-kappa:-0.694775\n",
      "[269]\ttrain-kappa:-0.694957\n",
      "[270]\ttrain-kappa:-0.695170\n",
      "[271]\ttrain-kappa:-0.695279\n",
      "[272]\ttrain-kappa:-0.695639\n",
      "[273]\ttrain-kappa:-0.695831\n",
      "[274]\ttrain-kappa:-0.696088\n",
      "[275]\ttrain-kappa:-0.696184\n",
      "[276]\ttrain-kappa:-0.696276\n",
      "[277]\ttrain-kappa:-0.696427\n",
      "[278]\ttrain-kappa:-0.696471\n",
      "[279]\ttrain-kappa:-0.696483\n",
      "[280]\ttrain-kappa:-0.696494\n",
      "[281]\ttrain-kappa:-0.696919\n",
      "[282]\ttrain-kappa:-0.697085\n",
      "[283]\ttrain-kappa:-0.697222\n",
      "[284]\ttrain-kappa:-0.697511\n",
      "[285]\ttrain-kappa:-0.697553\n",
      "[286]\ttrain-kappa:-0.697692\n",
      "[287]\ttrain-kappa:-0.697915\n",
      "[288]\ttrain-kappa:-0.698077\n",
      "[289]\ttrain-kappa:-0.698290\n",
      "[290]\ttrain-kappa:-0.698390\n",
      "[291]\ttrain-kappa:-0.698607\n",
      "[292]\ttrain-kappa:-0.698768\n",
      "[293]\ttrain-kappa:-0.699053\n",
      "[294]\ttrain-kappa:-0.699214\n",
      "[295]\ttrain-kappa:-0.699256\n",
      "[296]\ttrain-kappa:-0.699489\n",
      "[297]\ttrain-kappa:-0.699747\n",
      "[298]\ttrain-kappa:-0.699826\n",
      "[299]\ttrain-kappa:-0.699916\n",
      "[300]\ttrain-kappa:-0.700211\n",
      "[301]\ttrain-kappa:-0.700271\n",
      "[302]\ttrain-kappa:-0.700312\n",
      "[303]\ttrain-kappa:-0.700453\n",
      "[304]\ttrain-kappa:-0.700643\n",
      "[305]\ttrain-kappa:-0.701020\n",
      "[306]\ttrain-kappa:-0.701167\n",
      "[307]\ttrain-kappa:-0.701317\n",
      "[308]\ttrain-kappa:-0.701627\n",
      "[309]\ttrain-kappa:-0.701802\n",
      "[310]\ttrain-kappa:-0.701888\n",
      "[311]\ttrain-kappa:-0.702454\n",
      "[312]\ttrain-kappa:-0.702592\n",
      "[313]\ttrain-kappa:-0.702792\n",
      "[314]\ttrain-kappa:-0.702699\n",
      "[315]\ttrain-kappa:-0.702874\n",
      "[316]\ttrain-kappa:-0.703118\n",
      "[317]\ttrain-kappa:-0.703239\n",
      "[318]\ttrain-kappa:-0.703564\n",
      "[319]\ttrain-kappa:-0.703745\n",
      "[320]\ttrain-kappa:-0.703854\n",
      "[321]\ttrain-kappa:-0.704090\n",
      "[322]\ttrain-kappa:-0.704318\n",
      "[323]\ttrain-kappa:-0.704321\n",
      "[324]\ttrain-kappa:-0.704485\n",
      "[325]\ttrain-kappa:-0.704615\n",
      "[326]\ttrain-kappa:-0.704874\n",
      "[327]\ttrain-kappa:-0.704971\n",
      "[328]\ttrain-kappa:-0.705109\n",
      "[329]\ttrain-kappa:-0.705228\n",
      "[330]\ttrain-kappa:-0.705322\n",
      "[331]\ttrain-kappa:-0.705552\n",
      "[332]\ttrain-kappa:-0.705753\n",
      "[333]\ttrain-kappa:-0.705918\n",
      "[334]\ttrain-kappa:-0.706083\n",
      "[335]\ttrain-kappa:-0.706286\n",
      "[336]\ttrain-kappa:-0.706507\n",
      "[337]\ttrain-kappa:-0.706556\n",
      "[338]\ttrain-kappa:-0.706781\n",
      "[339]\ttrain-kappa:-0.706772\n",
      "[340]\ttrain-kappa:-0.706837\n",
      "[341]\ttrain-kappa:-0.706884\n",
      "[342]\ttrain-kappa:-0.706911\n",
      "[343]\ttrain-kappa:-0.707058\n",
      "[344]\ttrain-kappa:-0.707193\n",
      "[345]\ttrain-kappa:-0.707453\n",
      "[346]\ttrain-kappa:-0.707741\n",
      "[347]\ttrain-kappa:-0.707784\n",
      "[348]\ttrain-kappa:-0.707859\n",
      "[349]\ttrain-kappa:-0.708020\n",
      "[350]\ttrain-kappa:-0.708152\n",
      "[351]\ttrain-kappa:-0.708336\n",
      "[352]\ttrain-kappa:-0.708422\n",
      "[353]\ttrain-kappa:-0.708660\n",
      "[354]\ttrain-kappa:-0.708894\n",
      "[355]\ttrain-kappa:-0.709220\n",
      "[356]\ttrain-kappa:-0.709477\n",
      "[357]\ttrain-kappa:-0.709791\n",
      "[358]\ttrain-kappa:-0.709802\n",
      "[359]\ttrain-kappa:-0.709822\n",
      "[360]\ttrain-kappa:-0.710089\n",
      "[361]\ttrain-kappa:-0.710193\n",
      "[362]\ttrain-kappa:-0.710337\n",
      "[363]\ttrain-kappa:-0.710441\n",
      "[364]\ttrain-kappa:-0.710663\n",
      "[365]\ttrain-kappa:-0.710653\n",
      "[366]\ttrain-kappa:-0.710737\n",
      "[367]\ttrain-kappa:-0.710873\n",
      "[368]\ttrain-kappa:-0.711054\n",
      "[369]\ttrain-kappa:-0.711112\n",
      "[370]\ttrain-kappa:-0.711292\n",
      "[371]\ttrain-kappa:-0.711502\n",
      "[372]\ttrain-kappa:-0.711624\n",
      "[373]\ttrain-kappa:-0.711882\n",
      "[374]\ttrain-kappa:-0.712131\n",
      "[375]\ttrain-kappa:-0.712197\n",
      "[376]\ttrain-kappa:-0.712296\n",
      "[377]\ttrain-kappa:-0.712470\n",
      "[378]\ttrain-kappa:-0.712504\n",
      "[379]\ttrain-kappa:-0.712712\n",
      "[380]\ttrain-kappa:-0.712796\n",
      "[381]\ttrain-kappa:-0.713015\n",
      "[382]\ttrain-kappa:-0.713129\n",
      "[383]\ttrain-kappa:-0.713266\n",
      "[384]\ttrain-kappa:-0.713474\n",
      "[385]\ttrain-kappa:-0.713535\n",
      "[386]\ttrain-kappa:-0.713766\n",
      "[387]\ttrain-kappa:-0.713828\n",
      "[388]\ttrain-kappa:-0.714036\n",
      "[389]\ttrain-kappa:-0.714089\n",
      "[390]\ttrain-kappa:-0.714085\n",
      "[391]\ttrain-kappa:-0.714188\n",
      "[392]\ttrain-kappa:-0.714280\n",
      "[393]\ttrain-kappa:-0.714372\n",
      "[394]\ttrain-kappa:-0.714470\n",
      "[395]\ttrain-kappa:-0.714429\n",
      "[396]\ttrain-kappa:-0.714471\n",
      "[397]\ttrain-kappa:-0.714620\n",
      "[398]\ttrain-kappa:-0.714662\n",
      "[399]\ttrain-kappa:-0.714775\n",
      "[400]\ttrain-kappa:-0.715079\n",
      "[401]\ttrain-kappa:-0.715339\n",
      "[402]\ttrain-kappa:-0.715439\n",
      "[403]\ttrain-kappa:-0.715482\n",
      "[404]\ttrain-kappa:-0.715703\n",
      "[405]\ttrain-kappa:-0.715803\n",
      "[406]\ttrain-kappa:-0.715971\n",
      "[407]\ttrain-kappa:-0.716031\n",
      "[408]\ttrain-kappa:-0.716105\n",
      "[409]\ttrain-kappa:-0.716235\n",
      "[410]\ttrain-kappa:-0.716579\n",
      "[411]\ttrain-kappa:-0.716715\n",
      "[412]\ttrain-kappa:-0.716856\n",
      "[413]\ttrain-kappa:-0.716822\n",
      "[414]\ttrain-kappa:-0.717043\n",
      "[415]\ttrain-kappa:-0.717181\n",
      "[416]\ttrain-kappa:-0.717261\n",
      "[417]\ttrain-kappa:-0.717381\n",
      "[418]\ttrain-kappa:-0.717556\n",
      "[419]\ttrain-kappa:-0.717731\n",
      "[420]\ttrain-kappa:-0.717826\n",
      "[421]\ttrain-kappa:-0.717873\n",
      "[422]\ttrain-kappa:-0.718030\n",
      "[423]\ttrain-kappa:-0.718246\n",
      "[424]\ttrain-kappa:-0.718183\n",
      "[425]\ttrain-kappa:-0.718435\n",
      "[426]\ttrain-kappa:-0.718436\n",
      "[427]\ttrain-kappa:-0.718624\n",
      "[428]\ttrain-kappa:-0.718686\n",
      "[429]\ttrain-kappa:-0.718902\n",
      "[430]\ttrain-kappa:-0.719059\n",
      "[431]\ttrain-kappa:-0.719184\n",
      "[432]\ttrain-kappa:-0.719230\n",
      "[433]\ttrain-kappa:-0.719337\n",
      "[434]\ttrain-kappa:-0.719424\n",
      "[435]\ttrain-kappa:-0.719421\n",
      "[436]\ttrain-kappa:-0.719546\n",
      "[437]\ttrain-kappa:-0.719608\n",
      "[438]\ttrain-kappa:-0.719639\n",
      "[439]\ttrain-kappa:-0.719645\n",
      "[440]\ttrain-kappa:-0.719792\n",
      "[441]\ttrain-kappa:-0.719938\n",
      "[442]\ttrain-kappa:-0.720054\n",
      "[443]\ttrain-kappa:-0.720364\n",
      "[444]\ttrain-kappa:-0.720641\n",
      "[445]\ttrain-kappa:-0.720618\n",
      "[446]\ttrain-kappa:-0.720673\n",
      "[447]\ttrain-kappa:-0.720971\n",
      "[448]\ttrain-kappa:-0.721088\n",
      "[449]\ttrain-kappa:-0.721186\n",
      "[450]\ttrain-kappa:-0.721266\n",
      "[451]\ttrain-kappa:-0.721456\n",
      "[452]\ttrain-kappa:-0.721680\n",
      "[453]\ttrain-kappa:-0.721707\n",
      "[454]\ttrain-kappa:-0.721678\n",
      "[455]\ttrain-kappa:-0.721846\n",
      "[456]\ttrain-kappa:-0.722046\n",
      "[457]\ttrain-kappa:-0.722237\n",
      "[458]\ttrain-kappa:-0.722223\n",
      "[459]\ttrain-kappa:-0.722374\n",
      "[460]\ttrain-kappa:-0.722389\n",
      "[461]\ttrain-kappa:-0.722441\n",
      "[462]\ttrain-kappa:-0.722528\n",
      "[463]\ttrain-kappa:-0.722688\n",
      "[464]\ttrain-kappa:-0.722767\n",
      "[465]\ttrain-kappa:-0.722775\n",
      "[466]\ttrain-kappa:-0.722972\n",
      "[467]\ttrain-kappa:-0.723219\n",
      "[468]\ttrain-kappa:-0.723442\n",
      "[469]\ttrain-kappa:-0.723424\n",
      "[470]\ttrain-kappa:-0.723614\n",
      "[471]\ttrain-kappa:-0.723805\n",
      "[472]\ttrain-kappa:-0.724188\n",
      "[473]\ttrain-kappa:-0.724154\n",
      "[474]\ttrain-kappa:-0.724215\n",
      "[475]\ttrain-kappa:-0.724226\n",
      "[476]\ttrain-kappa:-0.724410\n",
      "[477]\ttrain-kappa:-0.724546\n",
      "[478]\ttrain-kappa:-0.724669\n",
      "[479]\ttrain-kappa:-0.724708\n",
      "[480]\ttrain-kappa:-0.724709\n",
      "[481]\ttrain-kappa:-0.724896\n",
      "[482]\ttrain-kappa:-0.724944\n",
      "[483]\ttrain-kappa:-0.725062\n",
      "[484]\ttrain-kappa:-0.725142\n",
      "[485]\ttrain-kappa:-0.725381\n",
      "[486]\ttrain-kappa:-0.725430\n",
      "[487]\ttrain-kappa:-0.725667\n",
      "[488]\ttrain-kappa:-0.725693\n",
      "[489]\ttrain-kappa:-0.725731\n",
      "[490]\ttrain-kappa:-0.725689\n",
      "[491]\ttrain-kappa:-0.725811\n",
      "[492]\ttrain-kappa:-0.726122\n",
      "[493]\ttrain-kappa:-0.726139\n",
      "[494]\ttrain-kappa:-0.726243\n",
      "[495]\ttrain-kappa:-0.726407\n",
      "[496]\ttrain-kappa:-0.726486\n",
      "[497]\ttrain-kappa:-0.726460\n",
      "[498]\ttrain-kappa:-0.726718\n",
      "[499]\ttrain-kappa:-0.726722\n",
      "[500]\ttrain-kappa:-0.726803\n",
      "[501]\ttrain-kappa:-0.726947\n",
      "[502]\ttrain-kappa:-0.727100\n",
      "[503]\ttrain-kappa:-0.727279\n",
      "[504]\ttrain-kappa:-0.727394\n",
      "[505]\ttrain-kappa:-0.727466\n",
      "[506]\ttrain-kappa:-0.727644\n",
      "[507]\ttrain-kappa:-0.727675\n",
      "[508]\ttrain-kappa:-0.727778\n",
      "[509]\ttrain-kappa:-0.727824\n",
      "[510]\ttrain-kappa:-0.727895\n",
      "[511]\ttrain-kappa:-0.727928\n",
      "[512]\ttrain-kappa:-0.727988\n",
      "[513]\ttrain-kappa:-0.728267\n",
      "[514]\ttrain-kappa:-0.728355\n",
      "[515]\ttrain-kappa:-0.728544\n",
      "[516]\ttrain-kappa:-0.728518\n",
      "[517]\ttrain-kappa:-0.728603\n",
      "[518]\ttrain-kappa:-0.728674\n",
      "[519]\ttrain-kappa:-0.728757\n",
      "[520]\ttrain-kappa:-0.728993\n",
      "[521]\ttrain-kappa:-0.728970\n",
      "[522]\ttrain-kappa:-0.729117\n",
      "[523]\ttrain-kappa:-0.729339\n",
      "[524]\ttrain-kappa:-0.729409\n",
      "[525]\ttrain-kappa:-0.729499\n",
      "[526]\ttrain-kappa:-0.729664\n",
      "[527]\ttrain-kappa:-0.729704\n",
      "[528]\ttrain-kappa:-0.729777\n",
      "[529]\ttrain-kappa:-0.729935\n",
      "[530]\ttrain-kappa:-0.730047\n",
      "[531]\ttrain-kappa:-0.730230\n",
      "[532]\ttrain-kappa:-0.730198\n",
      "[533]\ttrain-kappa:-0.730312\n",
      "[534]\ttrain-kappa:-0.730458\n",
      "[535]\ttrain-kappa:-0.730504\n",
      "[536]\ttrain-kappa:-0.730563\n",
      "[537]\ttrain-kappa:-0.730617\n",
      "[538]\ttrain-kappa:-0.730718\n",
      "[539]\ttrain-kappa:-0.730866\n",
      "[540]\ttrain-kappa:-0.730983\n",
      "[541]\ttrain-kappa:-0.731231\n",
      "[542]\ttrain-kappa:-0.731160\n",
      "[543]\ttrain-kappa:-0.731173\n",
      "[544]\ttrain-kappa:-0.731194\n",
      "[545]\ttrain-kappa:-0.731326\n",
      "[546]\ttrain-kappa:-0.731486\n",
      "[547]\ttrain-kappa:-0.731554\n",
      "[548]\ttrain-kappa:-0.731536\n",
      "[549]\ttrain-kappa:-0.731602\n",
      "[550]\ttrain-kappa:-0.731751\n",
      "[551]\ttrain-kappa:-0.731779\n",
      "[552]\ttrain-kappa:-0.731799\n",
      "[553]\ttrain-kappa:-0.731931\n",
      "[554]\ttrain-kappa:-0.732061\n",
      "[555]\ttrain-kappa:-0.732115\n",
      "[556]\ttrain-kappa:-0.732170\n",
      "[557]\ttrain-kappa:-0.732247\n",
      "[558]\ttrain-kappa:-0.732327\n",
      "[559]\ttrain-kappa:-0.732556\n",
      "[560]\ttrain-kappa:-0.732691\n",
      "[561]\ttrain-kappa:-0.732738\n",
      "[562]\ttrain-kappa:-0.732927\n",
      "[563]\ttrain-kappa:-0.733078\n",
      "[564]\ttrain-kappa:-0.733174\n",
      "[565]\ttrain-kappa:-0.733135\n",
      "[566]\ttrain-kappa:-0.733251\n",
      "[567]\ttrain-kappa:-0.733510\n",
      "[568]\ttrain-kappa:-0.733585\n",
      "[569]\ttrain-kappa:-0.733620\n",
      "[570]\ttrain-kappa:-0.733594\n",
      "[571]\ttrain-kappa:-0.733755\n",
      "[572]\ttrain-kappa:-0.733736\n",
      "[573]\ttrain-kappa:-0.733970\n",
      "[574]\ttrain-kappa:-0.734015\n",
      "[575]\ttrain-kappa:-0.734094\n",
      "[576]\ttrain-kappa:-0.734190\n",
      "[577]\ttrain-kappa:-0.734225\n",
      "[578]\ttrain-kappa:-0.734232\n",
      "[579]\ttrain-kappa:-0.734292\n",
      "[580]\ttrain-kappa:-0.734278\n",
      "[581]\ttrain-kappa:-0.734378\n",
      "[582]\ttrain-kappa:-0.734425\n",
      "[583]\ttrain-kappa:-0.734608\n",
      "[584]\ttrain-kappa:-0.734679\n",
      "[585]\ttrain-kappa:-0.734738\n",
      "[586]\ttrain-kappa:-0.734955\n",
      "[587]\ttrain-kappa:-0.735013\n",
      "[588]\ttrain-kappa:-0.735087\n",
      "[589]\ttrain-kappa:-0.735168\n",
      "[590]\ttrain-kappa:-0.735225\n",
      "[591]\ttrain-kappa:-0.735266\n",
      "[592]\ttrain-kappa:-0.735381\n",
      "[593]\ttrain-kappa:-0.735437\n",
      "[594]\ttrain-kappa:-0.735546\n",
      "[595]\ttrain-kappa:-0.735605\n",
      "[596]\ttrain-kappa:-0.735714\n",
      "[597]\ttrain-kappa:-0.735667\n",
      "[598]\ttrain-kappa:-0.735749\n",
      "[599]\ttrain-kappa:-0.735927\n",
      "[600]\ttrain-kappa:-0.735951\n",
      "[601]\ttrain-kappa:-0.736006\n",
      "[602]\ttrain-kappa:-0.736173\n",
      "[603]\ttrain-kappa:-0.736268\n",
      "[604]\ttrain-kappa:-0.736385\n",
      "[605]\ttrain-kappa:-0.736408\n",
      "[606]\ttrain-kappa:-0.736522\n",
      "[607]\ttrain-kappa:-0.736577\n",
      "[608]\ttrain-kappa:-0.736639\n",
      "[609]\ttrain-kappa:-0.736761\n",
      "[610]\ttrain-kappa:-0.736878\n",
      "[611]\ttrain-kappa:-0.736891\n",
      "[612]\ttrain-kappa:-0.737046\n",
      "[613]\ttrain-kappa:-0.737159\n",
      "[614]\ttrain-kappa:-0.737211\n",
      "[615]\ttrain-kappa:-0.737268\n",
      "[616]\ttrain-kappa:-0.737408\n",
      "[617]\ttrain-kappa:-0.737540\n",
      "[618]\ttrain-kappa:-0.737628\n",
      "[619]\ttrain-kappa:-0.737769\n",
      "[620]\ttrain-kappa:-0.737949\n",
      "[621]\ttrain-kappa:-0.738080\n",
      "[622]\ttrain-kappa:-0.738321\n",
      "[623]\ttrain-kappa:-0.738435\n",
      "[624]\ttrain-kappa:-0.738435\n",
      "[625]\ttrain-kappa:-0.738550\n",
      "[626]\ttrain-kappa:-0.738605\n",
      "[627]\ttrain-kappa:-0.738617\n",
      "[628]\ttrain-kappa:-0.738693\n",
      "[629]\ttrain-kappa:-0.738886\n",
      "[630]\ttrain-kappa:-0.738905\n",
      "[631]\ttrain-kappa:-0.738991\n",
      "[632]\ttrain-kappa:-0.738982\n",
      "[633]\ttrain-kappa:-0.739091\n",
      "[634]\ttrain-kappa:-0.739364\n",
      "[635]\ttrain-kappa:-0.739522\n",
      "[636]\ttrain-kappa:-0.739734\n",
      "[637]\ttrain-kappa:-0.739930\n",
      "[638]\ttrain-kappa:-0.740025\n",
      "[639]\ttrain-kappa:-0.740128\n",
      "[640]\ttrain-kappa:-0.740195\n",
      "[641]\ttrain-kappa:-0.740357\n",
      "[642]\ttrain-kappa:-0.740279\n",
      "[643]\ttrain-kappa:-0.740376\n",
      "[644]\ttrain-kappa:-0.740285\n",
      "[645]\ttrain-kappa:-0.740289\n",
      "[646]\ttrain-kappa:-0.740424\n",
      "[647]\ttrain-kappa:-0.740537\n",
      "[648]\ttrain-kappa:-0.740564\n",
      "[649]\ttrain-kappa:-0.740737\n",
      "[650]\ttrain-kappa:-0.740910\n",
      "[651]\ttrain-kappa:-0.741057\n",
      "[652]\ttrain-kappa:-0.741059\n",
      "[653]\ttrain-kappa:-0.741155\n",
      "[654]\ttrain-kappa:-0.741226\n",
      "[655]\ttrain-kappa:-0.741278\n",
      "[656]\ttrain-kappa:-0.741288\n",
      "[657]\ttrain-kappa:-0.741292\n",
      "[658]\ttrain-kappa:-0.741387\n",
      "[659]\ttrain-kappa:-0.741599\n",
      "[660]\ttrain-kappa:-0.741649\n",
      "[661]\ttrain-kappa:-0.741757\n",
      "[662]\ttrain-kappa:-0.741935\n",
      "[663]\ttrain-kappa:-0.742066\n",
      "[664]\ttrain-kappa:-0.742041\n",
      "[665]\ttrain-kappa:-0.742138\n",
      "[666]\ttrain-kappa:-0.742278\n",
      "[667]\ttrain-kappa:-0.742353\n",
      "[668]\ttrain-kappa:-0.742436\n",
      "[669]\ttrain-kappa:-0.742445\n",
      "[670]\ttrain-kappa:-0.742792\n",
      "[671]\ttrain-kappa:-0.742758\n",
      "[672]\ttrain-kappa:-0.742906\n",
      "[673]\ttrain-kappa:-0.743073\n",
      "[674]\ttrain-kappa:-0.743037\n",
      "[675]\ttrain-kappa:-0.743108\n",
      "[676]\ttrain-kappa:-0.743239\n",
      "[677]\ttrain-kappa:-0.743406\n",
      "[678]\ttrain-kappa:-0.743547\n",
      "[679]\ttrain-kappa:-0.743669\n",
      "[680]\ttrain-kappa:-0.743759\n",
      "[681]\ttrain-kappa:-0.743799\n",
      "[682]\ttrain-kappa:-0.743973\n",
      "[683]\ttrain-kappa:-0.744017\n",
      "[684]\ttrain-kappa:-0.744197\n",
      "[685]\ttrain-kappa:-0.744267\n",
      "[686]\ttrain-kappa:-0.744304\n",
      "[687]\ttrain-kappa:-0.744298\n",
      "[688]\ttrain-kappa:-0.744584\n",
      "[689]\ttrain-kappa:-0.744665\n",
      "[690]\ttrain-kappa:-0.744842\n",
      "[691]\ttrain-kappa:-0.744875\n",
      "[692]\ttrain-kappa:-0.744920\n",
      "[693]\ttrain-kappa:-0.744941\n",
      "[694]\ttrain-kappa:-0.744984\n",
      "[695]\ttrain-kappa:-0.745000\n",
      "[696]\ttrain-kappa:-0.745034\n",
      "[697]\ttrain-kappa:-0.745055\n",
      "[698]\ttrain-kappa:-0.745087\n",
      "[699]\ttrain-kappa:-0.745143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.753574\n",
      "         Iterations: 151\n",
      "         Function evaluations: 293\n",
      "Train QWK: 0.753574258641\n",
      "\n",
      "0.655978714789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-kappa:-0.000000\n",
      "[1]\ttrain-kappa:-0.000000\n",
      "[2]\ttrain-kappa:-0.000000\n",
      "[3]\ttrain-kappa:-0.000000\n",
      "[4]\ttrain-kappa:-0.000000\n",
      "[5]\ttrain-kappa:-0.000000\n",
      "[6]\ttrain-kappa:-0.000000\n",
      "[7]\ttrain-kappa:-0.000000\n",
      "[8]\ttrain-kappa:-0.000000\n",
      "[9]\ttrain-kappa:-0.024116\n",
      "[10]\ttrain-kappa:-0.044916\n",
      "[11]\ttrain-kappa:-0.054085\n",
      "[12]\ttrain-kappa:-0.056635\n",
      "[13]\ttrain-kappa:-0.055230\n",
      "[14]\ttrain-kappa:-0.051927\n",
      "[15]\ttrain-kappa:-0.048464\n",
      "[16]\ttrain-kappa:-0.044694\n",
      "[17]\ttrain-kappa:-0.040952\n",
      "[18]\ttrain-kappa:-0.037246\n",
      "[19]\ttrain-kappa:-0.033488\n",
      "[20]\ttrain-kappa:-0.030613\n",
      "[21]\ttrain-kappa:-0.041266\n",
      "[22]\ttrain-kappa:-0.061067\n",
      "[23]\ttrain-kappa:-0.072003\n",
      "[24]\ttrain-kappa:-0.079174\n",
      "[25]\ttrain-kappa:-0.084486\n",
      "[26]\ttrain-kappa:-0.089894\n",
      "[27]\ttrain-kappa:-0.094119\n",
      "[28]\ttrain-kappa:-0.097003\n",
      "[29]\ttrain-kappa:-0.097975\n",
      "[30]\ttrain-kappa:-0.098706\n",
      "[31]\ttrain-kappa:-0.105630\n",
      "[32]\ttrain-kappa:-0.125041\n",
      "[33]\ttrain-kappa:-0.146317\n",
      "[34]\ttrain-kappa:-0.158076\n",
      "[35]\ttrain-kappa:-0.165851\n",
      "[36]\ttrain-kappa:-0.171192\n",
      "[37]\ttrain-kappa:-0.175893\n",
      "[38]\ttrain-kappa:-0.180751\n",
      "[39]\ttrain-kappa:-0.186173\n",
      "[40]\ttrain-kappa:-0.191442\n",
      "[41]\ttrain-kappa:-0.216956\n",
      "[42]\ttrain-kappa:-0.235993\n",
      "[43]\ttrain-kappa:-0.258474\n",
      "[44]\ttrain-kappa:-0.273237\n",
      "[45]\ttrain-kappa:-0.284255\n",
      "[46]\ttrain-kappa:-0.292073\n",
      "[47]\ttrain-kappa:-0.298271\n",
      "[48]\ttrain-kappa:-0.304745\n",
      "[49]\ttrain-kappa:-0.310170\n",
      "[50]\ttrain-kappa:-0.315024\n",
      "[51]\ttrain-kappa:-0.319552\n",
      "[52]\ttrain-kappa:-0.348364\n",
      "[53]\ttrain-kappa:-0.364580\n",
      "[54]\ttrain-kappa:-0.381884\n",
      "[55]\ttrain-kappa:-0.397437\n",
      "[56]\ttrain-kappa:-0.411549\n",
      "[57]\ttrain-kappa:-0.423638\n",
      "[58]\ttrain-kappa:-0.432353\n",
      "[59]\ttrain-kappa:-0.439614\n",
      "[60]\ttrain-kappa:-0.445983\n",
      "[61]\ttrain-kappa:-0.452053\n",
      "[62]\ttrain-kappa:-0.457966\n",
      "[63]\ttrain-kappa:-0.463018\n",
      "[64]\ttrain-kappa:-0.467221\n",
      "[65]\ttrain-kappa:-0.472719\n",
      "[66]\ttrain-kappa:-0.476513\n",
      "[67]\ttrain-kappa:-0.488963\n",
      "[68]\ttrain-kappa:-0.501075\n",
      "[69]\ttrain-kappa:-0.510708\n",
      "[70]\ttrain-kappa:-0.519350\n",
      "[71]\ttrain-kappa:-0.526422\n",
      "[72]\ttrain-kappa:-0.534268\n",
      "[73]\ttrain-kappa:-0.543444\n",
      "[74]\ttrain-kappa:-0.550533\n",
      "[75]\ttrain-kappa:-0.556263\n",
      "[76]\ttrain-kappa:-0.562794\n",
      "[77]\ttrain-kappa:-0.567822\n",
      "[78]\ttrain-kappa:-0.572265\n",
      "[79]\ttrain-kappa:-0.576605\n",
      "[80]\ttrain-kappa:-0.580278\n",
      "[81]\ttrain-kappa:-0.583139\n",
      "[82]\ttrain-kappa:-0.585961\n",
      "[83]\ttrain-kappa:-0.588488\n",
      "[84]\ttrain-kappa:-0.591153\n",
      "[85]\ttrain-kappa:-0.593209\n",
      "[86]\ttrain-kappa:-0.598814\n",
      "[87]\ttrain-kappa:-0.602208\n",
      "[88]\ttrain-kappa:-0.607817\n",
      "[89]\ttrain-kappa:-0.610612\n",
      "[90]\ttrain-kappa:-0.612934\n",
      "[91]\ttrain-kappa:-0.616740\n",
      "[92]\ttrain-kappa:-0.619440\n",
      "[93]\ttrain-kappa:-0.621458\n",
      "[94]\ttrain-kappa:-0.624082\n",
      "[95]\ttrain-kappa:-0.625973\n",
      "[96]\ttrain-kappa:-0.627580\n",
      "[97]\ttrain-kappa:-0.629054\n",
      "[98]\ttrain-kappa:-0.631632\n",
      "[99]\ttrain-kappa:-0.634162\n",
      "[100]\ttrain-kappa:-0.636445\n",
      "[101]\ttrain-kappa:-0.638209\n",
      "[102]\ttrain-kappa:-0.639674\n",
      "[103]\ttrain-kappa:-0.641663\n",
      "[104]\ttrain-kappa:-0.642742\n",
      "[105]\ttrain-kappa:-0.643561\n",
      "[106]\ttrain-kappa:-0.645443\n",
      "[107]\ttrain-kappa:-0.646327\n",
      "[108]\ttrain-kappa:-0.647627\n",
      "[109]\ttrain-kappa:-0.648508\n",
      "[110]\ttrain-kappa:-0.649937\n",
      "[111]\ttrain-kappa:-0.650571\n",
      "[112]\ttrain-kappa:-0.651714\n",
      "[113]\ttrain-kappa:-0.652371\n",
      "[114]\ttrain-kappa:-0.652849\n",
      "[115]\ttrain-kappa:-0.653916\n",
      "[116]\ttrain-kappa:-0.654628\n",
      "[117]\ttrain-kappa:-0.655251\n",
      "[118]\ttrain-kappa:-0.655490\n",
      "[119]\ttrain-kappa:-0.656068\n",
      "[120]\ttrain-kappa:-0.656600\n",
      "[121]\ttrain-kappa:-0.657311\n",
      "[122]\ttrain-kappa:-0.657889\n",
      "[123]\ttrain-kappa:-0.658127\n",
      "[124]\ttrain-kappa:-0.658900\n",
      "[125]\ttrain-kappa:-0.659328\n",
      "[126]\ttrain-kappa:-0.659680\n",
      "[127]\ttrain-kappa:-0.660172\n",
      "[128]\ttrain-kappa:-0.660690\n",
      "[129]\ttrain-kappa:-0.661046\n",
      "[130]\ttrain-kappa:-0.661424\n",
      "[131]\ttrain-kappa:-0.661998\n",
      "[132]\ttrain-kappa:-0.662411\n",
      "[133]\ttrain-kappa:-0.662677\n",
      "[134]\ttrain-kappa:-0.662634\n",
      "[135]\ttrain-kappa:-0.662992\n",
      "[136]\ttrain-kappa:-0.663225\n",
      "[137]\ttrain-kappa:-0.663523\n",
      "[138]\ttrain-kappa:-0.664042\n",
      "[139]\ttrain-kappa:-0.664274\n",
      "[140]\ttrain-kappa:-0.664683\n",
      "[141]\ttrain-kappa:-0.664837\n",
      "[142]\ttrain-kappa:-0.665294\n",
      "[143]\ttrain-kappa:-0.665563\n",
      "[144]\ttrain-kappa:-0.665989\n",
      "[145]\ttrain-kappa:-0.666181\n",
      "[146]\ttrain-kappa:-0.666558\n",
      "[147]\ttrain-kappa:-0.667059\n",
      "[148]\ttrain-kappa:-0.667226\n",
      "[149]\ttrain-kappa:-0.667267\n",
      "[150]\ttrain-kappa:-0.667711\n",
      "[151]\ttrain-kappa:-0.667978\n",
      "[152]\ttrain-kappa:-0.668290\n",
      "[153]\ttrain-kappa:-0.668814\n",
      "[154]\ttrain-kappa:-0.668961\n",
      "[155]\ttrain-kappa:-0.669159\n",
      "[156]\ttrain-kappa:-0.669210\n",
      "[157]\ttrain-kappa:-0.669196\n",
      "[158]\ttrain-kappa:-0.669342\n",
      "[159]\ttrain-kappa:-0.669838\n",
      "[160]\ttrain-kappa:-0.669896\n",
      "[161]\ttrain-kappa:-0.670129\n",
      "[162]\ttrain-kappa:-0.670354\n",
      "[163]\ttrain-kappa:-0.670237\n",
      "[164]\ttrain-kappa:-0.670537\n",
      "[165]\ttrain-kappa:-0.671070\n",
      "[166]\ttrain-kappa:-0.671381\n",
      "[167]\ttrain-kappa:-0.671435\n",
      "[168]\ttrain-kappa:-0.671533\n",
      "[169]\ttrain-kappa:-0.671935\n",
      "[170]\ttrain-kappa:-0.672191\n",
      "[171]\ttrain-kappa:-0.672423\n",
      "[172]\ttrain-kappa:-0.672650\n",
      "[173]\ttrain-kappa:-0.672691\n",
      "[174]\ttrain-kappa:-0.673035\n",
      "[175]\ttrain-kappa:-0.673218\n",
      "[176]\ttrain-kappa:-0.673350\n",
      "[177]\ttrain-kappa:-0.673608\n",
      "[178]\ttrain-kappa:-0.674150\n",
      "[179]\ttrain-kappa:-0.674277\n",
      "[180]\ttrain-kappa:-0.674547\n",
      "[181]\ttrain-kappa:-0.675150\n",
      "[182]\ttrain-kappa:-0.675442\n",
      "[183]\ttrain-kappa:-0.675570\n",
      "[184]\ttrain-kappa:-0.675862\n",
      "[185]\ttrain-kappa:-0.676003\n",
      "[186]\ttrain-kappa:-0.676111\n",
      "[187]\ttrain-kappa:-0.676339\n",
      "[188]\ttrain-kappa:-0.676705\n",
      "[189]\ttrain-kappa:-0.676930\n",
      "[190]\ttrain-kappa:-0.677210\n",
      "[191]\ttrain-kappa:-0.677475\n",
      "[192]\ttrain-kappa:-0.677728\n",
      "[193]\ttrain-kappa:-0.677857\n",
      "[194]\ttrain-kappa:-0.677930\n",
      "[195]\ttrain-kappa:-0.678201\n",
      "[196]\ttrain-kappa:-0.678255\n",
      "[197]\ttrain-kappa:-0.678589\n",
      "[198]\ttrain-kappa:-0.679037\n",
      "[199]\ttrain-kappa:-0.679410\n",
      "[200]\ttrain-kappa:-0.679648\n",
      "[201]\ttrain-kappa:-0.679811\n",
      "[202]\ttrain-kappa:-0.680004\n",
      "[203]\ttrain-kappa:-0.680264\n",
      "[204]\ttrain-kappa:-0.680580\n",
      "[205]\ttrain-kappa:-0.680747\n",
      "[206]\ttrain-kappa:-0.681028\n",
      "[207]\ttrain-kappa:-0.680998\n",
      "[208]\ttrain-kappa:-0.681384\n",
      "[209]\ttrain-kappa:-0.681522\n",
      "[210]\ttrain-kappa:-0.681917\n",
      "[211]\ttrain-kappa:-0.682070\n",
      "[212]\ttrain-kappa:-0.682245\n",
      "[213]\ttrain-kappa:-0.682311\n",
      "[214]\ttrain-kappa:-0.682667\n",
      "[215]\ttrain-kappa:-0.682758\n",
      "[216]\ttrain-kappa:-0.682973\n",
      "[217]\ttrain-kappa:-0.683156\n",
      "[218]\ttrain-kappa:-0.683615\n",
      "[219]\ttrain-kappa:-0.683840\n",
      "[220]\ttrain-kappa:-0.684056\n",
      "[221]\ttrain-kappa:-0.684224\n",
      "[222]\ttrain-kappa:-0.684533\n",
      "[223]\ttrain-kappa:-0.684639\n",
      "[224]\ttrain-kappa:-0.684902\n",
      "[225]\ttrain-kappa:-0.685272\n",
      "[226]\ttrain-kappa:-0.685335\n",
      "[227]\ttrain-kappa:-0.685584\n",
      "[228]\ttrain-kappa:-0.685807\n",
      "[229]\ttrain-kappa:-0.685882\n",
      "[230]\ttrain-kappa:-0.686002\n",
      "[231]\ttrain-kappa:-0.686295\n",
      "[232]\ttrain-kappa:-0.686446\n",
      "[233]\ttrain-kappa:-0.686641\n",
      "[234]\ttrain-kappa:-0.687002\n",
      "[235]\ttrain-kappa:-0.687272\n",
      "[236]\ttrain-kappa:-0.687170\n",
      "[237]\ttrain-kappa:-0.687483\n",
      "[238]\ttrain-kappa:-0.687735\n",
      "[239]\ttrain-kappa:-0.687957\n",
      "[240]\ttrain-kappa:-0.688069\n",
      "[241]\ttrain-kappa:-0.688277\n",
      "[242]\ttrain-kappa:-0.688696\n",
      "[243]\ttrain-kappa:-0.688803\n",
      "[244]\ttrain-kappa:-0.689032\n",
      "[245]\ttrain-kappa:-0.689177\n",
      "[246]\ttrain-kappa:-0.689549\n",
      "[247]\ttrain-kappa:-0.689636\n",
      "[248]\ttrain-kappa:-0.689803\n",
      "[249]\ttrain-kappa:-0.690079\n",
      "[250]\ttrain-kappa:-0.690177\n",
      "[251]\ttrain-kappa:-0.690402\n",
      "[252]\ttrain-kappa:-0.690626\n",
      "[253]\ttrain-kappa:-0.690809\n",
      "[254]\ttrain-kappa:-0.690979\n",
      "[255]\ttrain-kappa:-0.691065\n",
      "[256]\ttrain-kappa:-0.691210\n",
      "[257]\ttrain-kappa:-0.691362\n",
      "[258]\ttrain-kappa:-0.691637\n",
      "[259]\ttrain-kappa:-0.691654\n",
      "[260]\ttrain-kappa:-0.691863\n",
      "[261]\ttrain-kappa:-0.692020\n",
      "[262]\ttrain-kappa:-0.692270\n",
      "[263]\ttrain-kappa:-0.692490\n",
      "[264]\ttrain-kappa:-0.692700\n",
      "[265]\ttrain-kappa:-0.693014\n",
      "[266]\ttrain-kappa:-0.693340\n",
      "[267]\ttrain-kappa:-0.693496\n",
      "[268]\ttrain-kappa:-0.693490\n",
      "[269]\ttrain-kappa:-0.693475\n",
      "[270]\ttrain-kappa:-0.693660\n",
      "[271]\ttrain-kappa:-0.693725\n",
      "[272]\ttrain-kappa:-0.693926\n",
      "[273]\ttrain-kappa:-0.694025\n",
      "[274]\ttrain-kappa:-0.693980\n",
      "[275]\ttrain-kappa:-0.694176\n",
      "[276]\ttrain-kappa:-0.694323\n",
      "[277]\ttrain-kappa:-0.694762\n",
      "[278]\ttrain-kappa:-0.694852\n",
      "[279]\ttrain-kappa:-0.694898\n",
      "[280]\ttrain-kappa:-0.695212\n",
      "[281]\ttrain-kappa:-0.695309\n",
      "[282]\ttrain-kappa:-0.695748\n",
      "[283]\ttrain-kappa:-0.695815\n",
      "[284]\ttrain-kappa:-0.696033\n",
      "[285]\ttrain-kappa:-0.696150\n",
      "[286]\ttrain-kappa:-0.696340\n",
      "[287]\ttrain-kappa:-0.696402\n",
      "[288]\ttrain-kappa:-0.696637\n",
      "[289]\ttrain-kappa:-0.696959\n",
      "[290]\ttrain-kappa:-0.697086\n",
      "[291]\ttrain-kappa:-0.697471\n",
      "[292]\ttrain-kappa:-0.697612\n",
      "[293]\ttrain-kappa:-0.698007\n",
      "[294]\ttrain-kappa:-0.698114\n",
      "[295]\ttrain-kappa:-0.698339\n",
      "[296]\ttrain-kappa:-0.698478\n",
      "[297]\ttrain-kappa:-0.698621\n",
      "[298]\ttrain-kappa:-0.698818\n",
      "[299]\ttrain-kappa:-0.698899\n",
      "[300]\ttrain-kappa:-0.699053\n",
      "[301]\ttrain-kappa:-0.699197\n",
      "[302]\ttrain-kappa:-0.699274\n",
      "[303]\ttrain-kappa:-0.699477\n",
      "[304]\ttrain-kappa:-0.699719\n",
      "[305]\ttrain-kappa:-0.699790\n",
      "[306]\ttrain-kappa:-0.700261\n",
      "[307]\ttrain-kappa:-0.700305\n",
      "[308]\ttrain-kappa:-0.700574\n",
      "[309]\ttrain-kappa:-0.700670\n",
      "[310]\ttrain-kappa:-0.700832\n",
      "[311]\ttrain-kappa:-0.700878\n",
      "[312]\ttrain-kappa:-0.701257\n",
      "[313]\ttrain-kappa:-0.701278\n",
      "[314]\ttrain-kappa:-0.701583\n",
      "[315]\ttrain-kappa:-0.701819\n",
      "[316]\ttrain-kappa:-0.701834\n",
      "[317]\ttrain-kappa:-0.702145\n",
      "[318]\ttrain-kappa:-0.702342\n",
      "[319]\ttrain-kappa:-0.702504\n",
      "[320]\ttrain-kappa:-0.702887\n",
      "[321]\ttrain-kappa:-0.702920\n",
      "[322]\ttrain-kappa:-0.702920\n",
      "[323]\ttrain-kappa:-0.703004\n",
      "[324]\ttrain-kappa:-0.703265\n",
      "[325]\ttrain-kappa:-0.703677\n",
      "[326]\ttrain-kappa:-0.704033\n",
      "[327]\ttrain-kappa:-0.704143\n",
      "[328]\ttrain-kappa:-0.704359\n",
      "[329]\ttrain-kappa:-0.704361\n",
      "[330]\ttrain-kappa:-0.704707\n",
      "[331]\ttrain-kappa:-0.704852\n",
      "[332]\ttrain-kappa:-0.704847\n",
      "[333]\ttrain-kappa:-0.704920\n",
      "[334]\ttrain-kappa:-0.705191\n",
      "[335]\ttrain-kappa:-0.705328\n",
      "[336]\ttrain-kappa:-0.705549\n",
      "[337]\ttrain-kappa:-0.705773\n",
      "[338]\ttrain-kappa:-0.706028\n",
      "[339]\ttrain-kappa:-0.706045\n",
      "[340]\ttrain-kappa:-0.706370\n",
      "[341]\ttrain-kappa:-0.706464\n",
      "[342]\ttrain-kappa:-0.706713\n",
      "[343]\ttrain-kappa:-0.706839\n",
      "[344]\ttrain-kappa:-0.706805\n",
      "[345]\ttrain-kappa:-0.706950\n",
      "[346]\ttrain-kappa:-0.707063\n",
      "[347]\ttrain-kappa:-0.707254\n",
      "[348]\ttrain-kappa:-0.707348\n",
      "[349]\ttrain-kappa:-0.707575\n",
      "[350]\ttrain-kappa:-0.707581\n",
      "[351]\ttrain-kappa:-0.707747\n",
      "[352]\ttrain-kappa:-0.707851\n",
      "[353]\ttrain-kappa:-0.707957\n",
      "[354]\ttrain-kappa:-0.708110\n",
      "[355]\ttrain-kappa:-0.708048\n",
      "[356]\ttrain-kappa:-0.708023\n",
      "[357]\ttrain-kappa:-0.708308\n",
      "[358]\ttrain-kappa:-0.708363\n",
      "[359]\ttrain-kappa:-0.708435\n",
      "[360]\ttrain-kappa:-0.708419\n",
      "[361]\ttrain-kappa:-0.708754\n",
      "[362]\ttrain-kappa:-0.708797\n",
      "[363]\ttrain-kappa:-0.708849\n",
      "[364]\ttrain-kappa:-0.708915\n",
      "[365]\ttrain-kappa:-0.709047\n",
      "[366]\ttrain-kappa:-0.709303\n",
      "[367]\ttrain-kappa:-0.709424\n",
      "[368]\ttrain-kappa:-0.709708\n",
      "[369]\ttrain-kappa:-0.709762\n",
      "[370]\ttrain-kappa:-0.709913\n",
      "[371]\ttrain-kappa:-0.710064\n",
      "[372]\ttrain-kappa:-0.710455\n",
      "[373]\ttrain-kappa:-0.710664\n",
      "[374]\ttrain-kappa:-0.710633\n",
      "[375]\ttrain-kappa:-0.710770\n",
      "[376]\ttrain-kappa:-0.710837\n",
      "[377]\ttrain-kappa:-0.710958\n",
      "[378]\ttrain-kappa:-0.711145\n",
      "[379]\ttrain-kappa:-0.711189\n",
      "[380]\ttrain-kappa:-0.711268\n",
      "[381]\ttrain-kappa:-0.711531\n",
      "[382]\ttrain-kappa:-0.711723\n",
      "[383]\ttrain-kappa:-0.711935\n",
      "[384]\ttrain-kappa:-0.711945\n",
      "[385]\ttrain-kappa:-0.712055\n",
      "[386]\ttrain-kappa:-0.712155\n",
      "[387]\ttrain-kappa:-0.712331\n",
      "[388]\ttrain-kappa:-0.712308\n",
      "[389]\ttrain-kappa:-0.712531\n",
      "[390]\ttrain-kappa:-0.712532\n",
      "[391]\ttrain-kappa:-0.712700\n",
      "[392]\ttrain-kappa:-0.713006\n",
      "[393]\ttrain-kappa:-0.713047\n",
      "[394]\ttrain-kappa:-0.713036\n",
      "[395]\ttrain-kappa:-0.713067\n",
      "[396]\ttrain-kappa:-0.713094\n",
      "[397]\ttrain-kappa:-0.713185\n",
      "[398]\ttrain-kappa:-0.713320\n",
      "[399]\ttrain-kappa:-0.713376\n",
      "[400]\ttrain-kappa:-0.713373\n",
      "[401]\ttrain-kappa:-0.713409\n",
      "[402]\ttrain-kappa:-0.713697\n",
      "[403]\ttrain-kappa:-0.713735\n",
      "[404]\ttrain-kappa:-0.713836\n",
      "[405]\ttrain-kappa:-0.714155\n",
      "[406]\ttrain-kappa:-0.714162\n",
      "[407]\ttrain-kappa:-0.714385\n",
      "[408]\ttrain-kappa:-0.714433\n",
      "[409]\ttrain-kappa:-0.714584\n",
      "[410]\ttrain-kappa:-0.714658\n",
      "[411]\ttrain-kappa:-0.714930\n",
      "[412]\ttrain-kappa:-0.715026\n",
      "[413]\ttrain-kappa:-0.715140\n",
      "[414]\ttrain-kappa:-0.715297\n",
      "[415]\ttrain-kappa:-0.715480\n",
      "[416]\ttrain-kappa:-0.715439\n",
      "[417]\ttrain-kappa:-0.715542\n",
      "[418]\ttrain-kappa:-0.715797\n",
      "[419]\ttrain-kappa:-0.715833\n",
      "[420]\ttrain-kappa:-0.715916\n",
      "[421]\ttrain-kappa:-0.716003\n",
      "[422]\ttrain-kappa:-0.716153\n",
      "[423]\ttrain-kappa:-0.716328\n",
      "[424]\ttrain-kappa:-0.716332\n",
      "[425]\ttrain-kappa:-0.716407\n",
      "[426]\ttrain-kappa:-0.716537\n",
      "[427]\ttrain-kappa:-0.716577\n",
      "[428]\ttrain-kappa:-0.716672\n",
      "[429]\ttrain-kappa:-0.716695\n",
      "[430]\ttrain-kappa:-0.716764\n",
      "[431]\ttrain-kappa:-0.716969\n",
      "[432]\ttrain-kappa:-0.717153\n",
      "[433]\ttrain-kappa:-0.717189\n",
      "[434]\ttrain-kappa:-0.717247\n",
      "[435]\ttrain-kappa:-0.717380\n",
      "[436]\ttrain-kappa:-0.717418\n",
      "[437]\ttrain-kappa:-0.717363\n",
      "[438]\ttrain-kappa:-0.717661\n",
      "[439]\ttrain-kappa:-0.717699\n",
      "[440]\ttrain-kappa:-0.717987\n",
      "[441]\ttrain-kappa:-0.717943\n",
      "[442]\ttrain-kappa:-0.718074\n",
      "[443]\ttrain-kappa:-0.718225\n",
      "[444]\ttrain-kappa:-0.718407\n",
      "[445]\ttrain-kappa:-0.718549\n",
      "[446]\ttrain-kappa:-0.718750\n",
      "[447]\ttrain-kappa:-0.718996\n",
      "[448]\ttrain-kappa:-0.718958\n",
      "[449]\ttrain-kappa:-0.719108\n",
      "[450]\ttrain-kappa:-0.719200\n",
      "[451]\ttrain-kappa:-0.719394\n",
      "[452]\ttrain-kappa:-0.719384\n",
      "[453]\ttrain-kappa:-0.719373\n",
      "[454]\ttrain-kappa:-0.719454\n",
      "[455]\ttrain-kappa:-0.719408\n",
      "[456]\ttrain-kappa:-0.719689\n",
      "[457]\ttrain-kappa:-0.719791\n",
      "[458]\ttrain-kappa:-0.720016\n",
      "[459]\ttrain-kappa:-0.720183\n",
      "[460]\ttrain-kappa:-0.720181\n",
      "[461]\ttrain-kappa:-0.720258\n",
      "[462]\ttrain-kappa:-0.720418\n",
      "[463]\ttrain-kappa:-0.720680\n",
      "[464]\ttrain-kappa:-0.720832\n",
      "[465]\ttrain-kappa:-0.720912\n",
      "[466]\ttrain-kappa:-0.721003\n",
      "[467]\ttrain-kappa:-0.721248\n",
      "[468]\ttrain-kappa:-0.721386\n",
      "[469]\ttrain-kappa:-0.721363\n",
      "[470]\ttrain-kappa:-0.721502\n",
      "[471]\ttrain-kappa:-0.721588\n",
      "[472]\ttrain-kappa:-0.721776\n",
      "[473]\ttrain-kappa:-0.721985\n",
      "[474]\ttrain-kappa:-0.722068\n",
      "[475]\ttrain-kappa:-0.722296\n",
      "[476]\ttrain-kappa:-0.722254\n",
      "[477]\ttrain-kappa:-0.722269\n",
      "[478]\ttrain-kappa:-0.722473\n",
      "[479]\ttrain-kappa:-0.722568\n",
      "[480]\ttrain-kappa:-0.722822\n",
      "[481]\ttrain-kappa:-0.722931\n",
      "[482]\ttrain-kappa:-0.723153\n",
      "[483]\ttrain-kappa:-0.723203\n",
      "[484]\ttrain-kappa:-0.723188\n",
      "[485]\ttrain-kappa:-0.723334\n",
      "[486]\ttrain-kappa:-0.723420\n",
      "[487]\ttrain-kappa:-0.723587\n",
      "[488]\ttrain-kappa:-0.723593\n",
      "[489]\ttrain-kappa:-0.723701\n",
      "[490]\ttrain-kappa:-0.723750\n",
      "[491]\ttrain-kappa:-0.723897\n",
      "[492]\ttrain-kappa:-0.724093\n",
      "[493]\ttrain-kappa:-0.724261\n",
      "[494]\ttrain-kappa:-0.724368\n",
      "[495]\ttrain-kappa:-0.724446\n",
      "[496]\ttrain-kappa:-0.724566\n",
      "[497]\ttrain-kappa:-0.724579\n",
      "[498]\ttrain-kappa:-0.724742\n",
      "[499]\ttrain-kappa:-0.724817\n",
      "[500]\ttrain-kappa:-0.724871\n",
      "[501]\ttrain-kappa:-0.725000\n",
      "[502]\ttrain-kappa:-0.725047\n",
      "[503]\ttrain-kappa:-0.725120\n",
      "[504]\ttrain-kappa:-0.725200\n",
      "[505]\ttrain-kappa:-0.725213\n",
      "[506]\ttrain-kappa:-0.725422\n",
      "[507]\ttrain-kappa:-0.725526\n",
      "[508]\ttrain-kappa:-0.725586\n",
      "[509]\ttrain-kappa:-0.725583\n",
      "[510]\ttrain-kappa:-0.725629\n",
      "[511]\ttrain-kappa:-0.725573\n",
      "[512]\ttrain-kappa:-0.725629\n",
      "[513]\ttrain-kappa:-0.725895\n",
      "[514]\ttrain-kappa:-0.725981\n",
      "[515]\ttrain-kappa:-0.725937\n",
      "[516]\ttrain-kappa:-0.726091\n",
      "[517]\ttrain-kappa:-0.726197\n",
      "[518]\ttrain-kappa:-0.726268\n",
      "[519]\ttrain-kappa:-0.726615\n",
      "[520]\ttrain-kappa:-0.726762\n",
      "[521]\ttrain-kappa:-0.726869\n",
      "[522]\ttrain-kappa:-0.727053\n",
      "[523]\ttrain-kappa:-0.727248\n",
      "[524]\ttrain-kappa:-0.727373\n",
      "[525]\ttrain-kappa:-0.727462\n",
      "[526]\ttrain-kappa:-0.727578\n",
      "[527]\ttrain-kappa:-0.727828\n",
      "[528]\ttrain-kappa:-0.727939\n",
      "[529]\ttrain-kappa:-0.727907\n",
      "[530]\ttrain-kappa:-0.728055\n",
      "[531]\ttrain-kappa:-0.728124\n",
      "[532]\ttrain-kappa:-0.728345\n",
      "[533]\ttrain-kappa:-0.728370\n",
      "[534]\ttrain-kappa:-0.728358\n",
      "[535]\ttrain-kappa:-0.728387\n",
      "[536]\ttrain-kappa:-0.728400\n",
      "[537]\ttrain-kappa:-0.728514\n",
      "[538]\ttrain-kappa:-0.728614\n",
      "[539]\ttrain-kappa:-0.728694\n",
      "[540]\ttrain-kappa:-0.728912\n",
      "[541]\ttrain-kappa:-0.728985\n",
      "[542]\ttrain-kappa:-0.729170\n",
      "[543]\ttrain-kappa:-0.729460\n",
      "[544]\ttrain-kappa:-0.729475\n",
      "[545]\ttrain-kappa:-0.729732\n",
      "[546]\ttrain-kappa:-0.729853\n",
      "[547]\ttrain-kappa:-0.729873\n",
      "[548]\ttrain-kappa:-0.729825\n",
      "[549]\ttrain-kappa:-0.730024\n",
      "[550]\ttrain-kappa:-0.730167\n",
      "[551]\ttrain-kappa:-0.730032\n",
      "[552]\ttrain-kappa:-0.730079\n",
      "[553]\ttrain-kappa:-0.730177\n",
      "[554]\ttrain-kappa:-0.730186\n",
      "[555]\ttrain-kappa:-0.730361\n",
      "[556]\ttrain-kappa:-0.730426\n",
      "[557]\ttrain-kappa:-0.730490\n",
      "[558]\ttrain-kappa:-0.730659\n",
      "[559]\ttrain-kappa:-0.730742\n",
      "[560]\ttrain-kappa:-0.731031\n",
      "[561]\ttrain-kappa:-0.731161\n",
      "[562]\ttrain-kappa:-0.731294\n",
      "[563]\ttrain-kappa:-0.731335\n",
      "[564]\ttrain-kappa:-0.731520\n",
      "[565]\ttrain-kappa:-0.731634\n",
      "[566]\ttrain-kappa:-0.731616\n",
      "[567]\ttrain-kappa:-0.731626\n",
      "[568]\ttrain-kappa:-0.731640\n",
      "[569]\ttrain-kappa:-0.731846\n",
      "[570]\ttrain-kappa:-0.731863\n",
      "[571]\ttrain-kappa:-0.731945\n",
      "[572]\ttrain-kappa:-0.732135\n",
      "[573]\ttrain-kappa:-0.732380\n",
      "[574]\ttrain-kappa:-0.732584\n",
      "[575]\ttrain-kappa:-0.732712\n",
      "[576]\ttrain-kappa:-0.732631\n",
      "[577]\ttrain-kappa:-0.732571\n",
      "[578]\ttrain-kappa:-0.732744\n",
      "[579]\ttrain-kappa:-0.732893\n",
      "[580]\ttrain-kappa:-0.733043\n",
      "[581]\ttrain-kappa:-0.733120\n",
      "[582]\ttrain-kappa:-0.733159\n",
      "[583]\ttrain-kappa:-0.733295\n",
      "[584]\ttrain-kappa:-0.733418\n",
      "[585]\ttrain-kappa:-0.733445\n",
      "[586]\ttrain-kappa:-0.733735\n",
      "[587]\ttrain-kappa:-0.733830\n",
      "[588]\ttrain-kappa:-0.733825\n",
      "[589]\ttrain-kappa:-0.733987\n",
      "[590]\ttrain-kappa:-0.733950\n",
      "[591]\ttrain-kappa:-0.734072\n",
      "[592]\ttrain-kappa:-0.734065\n",
      "[593]\ttrain-kappa:-0.734206\n",
      "[594]\ttrain-kappa:-0.734301\n",
      "[595]\ttrain-kappa:-0.734361\n",
      "[596]\ttrain-kappa:-0.734553\n",
      "[597]\ttrain-kappa:-0.734703\n",
      "[598]\ttrain-kappa:-0.734966\n",
      "[599]\ttrain-kappa:-0.735096\n",
      "[600]\ttrain-kappa:-0.735258\n",
      "[601]\ttrain-kappa:-0.735468\n",
      "[602]\ttrain-kappa:-0.735613\n",
      "[603]\ttrain-kappa:-0.735614\n",
      "[604]\ttrain-kappa:-0.735657\n",
      "[605]\ttrain-kappa:-0.735790\n",
      "[606]\ttrain-kappa:-0.735890\n",
      "[607]\ttrain-kappa:-0.735979\n",
      "[608]\ttrain-kappa:-0.736241\n",
      "[609]\ttrain-kappa:-0.736364\n",
      "[610]\ttrain-kappa:-0.736379\n",
      "[611]\ttrain-kappa:-0.736462\n",
      "[612]\ttrain-kappa:-0.736628\n",
      "[613]\ttrain-kappa:-0.736698\n",
      "[614]\ttrain-kappa:-0.736797\n",
      "[615]\ttrain-kappa:-0.736986\n",
      "[616]\ttrain-kappa:-0.737063\n",
      "[617]\ttrain-kappa:-0.737105\n",
      "[618]\ttrain-kappa:-0.737192\n",
      "[619]\ttrain-kappa:-0.737250\n",
      "[620]\ttrain-kappa:-0.737350\n",
      "[621]\ttrain-kappa:-0.737438\n",
      "[622]\ttrain-kappa:-0.737542\n",
      "[623]\ttrain-kappa:-0.737639\n",
      "[624]\ttrain-kappa:-0.737789\n",
      "[625]\ttrain-kappa:-0.737850\n",
      "[626]\ttrain-kappa:-0.737872\n",
      "[627]\ttrain-kappa:-0.737920\n",
      "[628]\ttrain-kappa:-0.738010\n",
      "[629]\ttrain-kappa:-0.738142\n",
      "[630]\ttrain-kappa:-0.738303\n",
      "[631]\ttrain-kappa:-0.738378\n",
      "[632]\ttrain-kappa:-0.738450\n",
      "[633]\ttrain-kappa:-0.738693\n",
      "[634]\ttrain-kappa:-0.738703\n",
      "[635]\ttrain-kappa:-0.738713\n",
      "[636]\ttrain-kappa:-0.738845\n",
      "[637]\ttrain-kappa:-0.738891\n",
      "[638]\ttrain-kappa:-0.739079\n",
      "[639]\ttrain-kappa:-0.739274\n",
      "[640]\ttrain-kappa:-0.739261\n",
      "[641]\ttrain-kappa:-0.739238\n",
      "[642]\ttrain-kappa:-0.739308\n",
      "[643]\ttrain-kappa:-0.739341\n",
      "[644]\ttrain-kappa:-0.739396\n",
      "[645]\ttrain-kappa:-0.739548\n",
      "[646]\ttrain-kappa:-0.739704\n",
      "[647]\ttrain-kappa:-0.739854\n",
      "[648]\ttrain-kappa:-0.739944\n",
      "[649]\ttrain-kappa:-0.739982\n",
      "[650]\ttrain-kappa:-0.740020\n",
      "[651]\ttrain-kappa:-0.740230\n",
      "[652]\ttrain-kappa:-0.740333\n",
      "[653]\ttrain-kappa:-0.740484\n",
      "[654]\ttrain-kappa:-0.740612\n",
      "[655]\ttrain-kappa:-0.740668\n",
      "[656]\ttrain-kappa:-0.740682\n",
      "[657]\ttrain-kappa:-0.740740\n",
      "[658]\ttrain-kappa:-0.740806\n",
      "[659]\ttrain-kappa:-0.740864\n",
      "[660]\ttrain-kappa:-0.740969\n",
      "[661]\ttrain-kappa:-0.741119\n",
      "[662]\ttrain-kappa:-0.741083\n",
      "[663]\ttrain-kappa:-0.741228\n",
      "[664]\ttrain-kappa:-0.741369\n",
      "[665]\ttrain-kappa:-0.741397\n",
      "[666]\ttrain-kappa:-0.741427\n",
      "[667]\ttrain-kappa:-0.741470\n",
      "[668]\ttrain-kappa:-0.741530\n",
      "[669]\ttrain-kappa:-0.741549\n",
      "[670]\ttrain-kappa:-0.741718\n",
      "[671]\ttrain-kappa:-0.741751\n",
      "[672]\ttrain-kappa:-0.742024\n",
      "[673]\ttrain-kappa:-0.742111\n",
      "[674]\ttrain-kappa:-0.742235\n",
      "[675]\ttrain-kappa:-0.742386\n",
      "[676]\ttrain-kappa:-0.742494\n",
      "[677]\ttrain-kappa:-0.742573\n",
      "[678]\ttrain-kappa:-0.742627\n",
      "[679]\ttrain-kappa:-0.742769\n",
      "[680]\ttrain-kappa:-0.742827\n",
      "[681]\ttrain-kappa:-0.742826\n",
      "[682]\ttrain-kappa:-0.743194\n",
      "[683]\ttrain-kappa:-0.743257\n",
      "[684]\ttrain-kappa:-0.743294\n",
      "[685]\ttrain-kappa:-0.743512\n",
      "[686]\ttrain-kappa:-0.743605\n",
      "[687]\ttrain-kappa:-0.743687\n",
      "[688]\ttrain-kappa:-0.743774\n",
      "[689]\ttrain-kappa:-0.743802\n",
      "[690]\ttrain-kappa:-0.743862\n",
      "[691]\ttrain-kappa:-0.743966\n",
      "[692]\ttrain-kappa:-0.743969\n",
      "[693]\ttrain-kappa:-0.744159\n",
      "[694]\ttrain-kappa:-0.744165\n",
      "[695]\ttrain-kappa:-0.744289\n",
      "[696]\ttrain-kappa:-0.744361\n",
      "[697]\ttrain-kappa:-0.744573\n",
      "[698]\ttrain-kappa:-0.744754\n",
      "[699]\ttrain-kappa:-0.745050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.751779\n",
      "         Iterations: 139\n",
      "         Function evaluations: 265\n",
      "Train QWK: 0.751779432643\n",
      "\n",
      "0.655458547729"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-kappa:-0.000000\n",
      "[1]\ttrain-kappa:-0.000000\n",
      "[2]\ttrain-kappa:-0.000000\n",
      "[3]\ttrain-kappa:-0.000000\n",
      "[4]\ttrain-kappa:-0.000000\n",
      "[5]\ttrain-kappa:-0.000000\n",
      "[6]\ttrain-kappa:-0.000000\n",
      "[7]\ttrain-kappa:-0.000000\n",
      "[8]\ttrain-kappa:-0.000000\n",
      "[9]\ttrain-kappa:-0.024398\n",
      "[10]\ttrain-kappa:-0.044896\n",
      "[11]\ttrain-kappa:-0.054728\n",
      "[12]\ttrain-kappa:-0.056639\n",
      "[13]\ttrain-kappa:-0.055506\n",
      "[14]\ttrain-kappa:-0.052512\n",
      "[15]\ttrain-kappa:-0.048715\n",
      "[16]\ttrain-kappa:-0.044858\n",
      "[17]\ttrain-kappa:-0.041095\n",
      "[18]\ttrain-kappa:-0.037529\n",
      "[19]\ttrain-kappa:-0.033855\n",
      "[20]\ttrain-kappa:-0.030977\n",
      "[21]\ttrain-kappa:-0.040372\n",
      "[22]\ttrain-kappa:-0.060609\n",
      "[23]\ttrain-kappa:-0.071529\n",
      "[24]\ttrain-kappa:-0.077859\n",
      "[25]\ttrain-kappa:-0.084445\n",
      "[26]\ttrain-kappa:-0.091077\n",
      "[27]\ttrain-kappa:-0.094818\n",
      "[28]\ttrain-kappa:-0.097335\n",
      "[29]\ttrain-kappa:-0.098253\n",
      "[30]\ttrain-kappa:-0.099384\n",
      "[31]\ttrain-kappa:-0.099289\n",
      "[32]\ttrain-kappa:-0.123201\n",
      "[33]\ttrain-kappa:-0.145475\n",
      "[34]\ttrain-kappa:-0.157321\n",
      "[35]\ttrain-kappa:-0.165394\n",
      "[36]\ttrain-kappa:-0.171281\n",
      "[37]\ttrain-kappa:-0.176454\n",
      "[38]\ttrain-kappa:-0.181778\n",
      "[39]\ttrain-kappa:-0.187686\n",
      "[40]\ttrain-kappa:-0.193588\n",
      "[41]\ttrain-kappa:-0.216355\n",
      "[42]\ttrain-kappa:-0.232785\n",
      "[43]\ttrain-kappa:-0.257692\n",
      "[44]\ttrain-kappa:-0.271996\n",
      "[45]\ttrain-kappa:-0.281814\n",
      "[46]\ttrain-kappa:-0.290189\n",
      "[47]\ttrain-kappa:-0.297449\n",
      "[48]\ttrain-kappa:-0.303598\n",
      "[49]\ttrain-kappa:-0.308850\n",
      "[50]\ttrain-kappa:-0.314548\n",
      "[51]\ttrain-kappa:-0.319150\n",
      "[52]\ttrain-kappa:-0.337021\n",
      "[53]\ttrain-kappa:-0.359267\n",
      "[54]\ttrain-kappa:-0.378941\n",
      "[55]\ttrain-kappa:-0.398357\n",
      "[56]\ttrain-kappa:-0.410596\n",
      "[57]\ttrain-kappa:-0.422098\n",
      "[58]\ttrain-kappa:-0.431428\n",
      "[59]\ttrain-kappa:-0.438537\n",
      "[60]\ttrain-kappa:-0.445468\n",
      "[61]\ttrain-kappa:-0.450987\n",
      "[62]\ttrain-kappa:-0.456572\n",
      "[63]\ttrain-kappa:-0.461288\n",
      "[64]\ttrain-kappa:-0.466120\n",
      "[65]\ttrain-kappa:-0.471008\n",
      "[66]\ttrain-kappa:-0.475701\n",
      "[67]\ttrain-kappa:-0.486823\n",
      "[68]\ttrain-kappa:-0.498153\n",
      "[69]\ttrain-kappa:-0.508089\n",
      "[70]\ttrain-kappa:-0.515489\n",
      "[71]\ttrain-kappa:-0.524563\n",
      "[72]\ttrain-kappa:-0.534235\n",
      "[73]\ttrain-kappa:-0.542427\n",
      "[74]\ttrain-kappa:-0.550047\n",
      "[75]\ttrain-kappa:-0.556222\n",
      "[76]\ttrain-kappa:-0.561329\n",
      "[77]\ttrain-kappa:-0.566173\n",
      "[78]\ttrain-kappa:-0.570374\n",
      "[79]\ttrain-kappa:-0.573986\n",
      "[80]\ttrain-kappa:-0.577585\n",
      "[81]\ttrain-kappa:-0.580646\n",
      "[82]\ttrain-kappa:-0.583806\n",
      "[83]\ttrain-kappa:-0.586881\n",
      "[84]\ttrain-kappa:-0.588781\n",
      "[85]\ttrain-kappa:-0.590906\n",
      "[86]\ttrain-kappa:-0.594777\n",
      "[87]\ttrain-kappa:-0.599610\n",
      "[88]\ttrain-kappa:-0.603656\n",
      "[89]\ttrain-kappa:-0.608717\n",
      "[90]\ttrain-kappa:-0.611620\n",
      "[91]\ttrain-kappa:-0.614772\n",
      "[92]\ttrain-kappa:-0.617460\n",
      "[93]\ttrain-kappa:-0.619703\n",
      "[94]\ttrain-kappa:-0.621736\n",
      "[95]\ttrain-kappa:-0.624067\n",
      "[96]\ttrain-kappa:-0.626294\n",
      "[97]\ttrain-kappa:-0.628897\n",
      "[98]\ttrain-kappa:-0.631196\n",
      "[99]\ttrain-kappa:-0.632762\n",
      "[100]\ttrain-kappa:-0.634529\n",
      "[101]\ttrain-kappa:-0.636110\n",
      "[102]\ttrain-kappa:-0.637127\n",
      "[103]\ttrain-kappa:-0.638939\n",
      "[104]\ttrain-kappa:-0.640119\n",
      "[105]\ttrain-kappa:-0.641045\n",
      "[106]\ttrain-kappa:-0.643139\n",
      "[107]\ttrain-kappa:-0.644089\n",
      "[108]\ttrain-kappa:-0.645219\n",
      "[109]\ttrain-kappa:-0.645854\n",
      "[110]\ttrain-kappa:-0.646830\n",
      "[111]\ttrain-kappa:-0.647776\n",
      "[112]\ttrain-kappa:-0.648789\n",
      "[113]\ttrain-kappa:-0.649398\n",
      "[114]\ttrain-kappa:-0.650350\n",
      "[115]\ttrain-kappa:-0.651098\n",
      "[116]\ttrain-kappa:-0.651916\n",
      "[117]\ttrain-kappa:-0.652636\n",
      "[118]\ttrain-kappa:-0.652772\n",
      "[119]\ttrain-kappa:-0.653184\n",
      "[120]\ttrain-kappa:-0.653915\n",
      "[121]\ttrain-kappa:-0.654209\n",
      "[122]\ttrain-kappa:-0.654996\n",
      "[123]\ttrain-kappa:-0.655459\n",
      "[124]\ttrain-kappa:-0.656035\n",
      "[125]\ttrain-kappa:-0.656890\n",
      "[126]\ttrain-kappa:-0.657543\n",
      "[127]\ttrain-kappa:-0.658139\n",
      "[128]\ttrain-kappa:-0.658757\n",
      "[129]\ttrain-kappa:-0.659177\n",
      "[130]\ttrain-kappa:-0.659691\n",
      "[131]\ttrain-kappa:-0.659999\n",
      "[132]\ttrain-kappa:-0.660283\n",
      "[133]\ttrain-kappa:-0.661116\n",
      "[134]\ttrain-kappa:-0.661159\n",
      "[135]\ttrain-kappa:-0.661567\n",
      "[136]\ttrain-kappa:-0.661746\n",
      "[137]\ttrain-kappa:-0.661990\n",
      "[138]\ttrain-kappa:-0.662482\n",
      "[139]\ttrain-kappa:-0.662686\n",
      "[140]\ttrain-kappa:-0.662939\n",
      "[141]\ttrain-kappa:-0.663426\n",
      "[142]\ttrain-kappa:-0.663804\n",
      "[143]\ttrain-kappa:-0.664378\n",
      "[144]\ttrain-kappa:-0.664833\n",
      "[145]\ttrain-kappa:-0.665256\n",
      "[146]\ttrain-kappa:-0.665499\n",
      "[147]\ttrain-kappa:-0.666077\n",
      "[148]\ttrain-kappa:-0.666241\n",
      "[149]\ttrain-kappa:-0.666393\n",
      "[150]\ttrain-kappa:-0.666800\n",
      "[151]\ttrain-kappa:-0.667019\n",
      "[152]\ttrain-kappa:-0.667120\n",
      "[153]\ttrain-kappa:-0.667754\n",
      "[154]\ttrain-kappa:-0.668163\n",
      "[155]\ttrain-kappa:-0.668370\n",
      "[156]\ttrain-kappa:-0.668542\n",
      "[157]\ttrain-kappa:-0.668855\n",
      "[158]\ttrain-kappa:-0.669193\n",
      "[159]\ttrain-kappa:-0.669566\n",
      "[160]\ttrain-kappa:-0.669853\n",
      "[161]\ttrain-kappa:-0.670101\n",
      "[162]\ttrain-kappa:-0.670365\n",
      "[163]\ttrain-kappa:-0.670379\n",
      "[164]\ttrain-kappa:-0.670771\n",
      "[165]\ttrain-kappa:-0.671067\n",
      "[166]\ttrain-kappa:-0.671153\n",
      "[167]\ttrain-kappa:-0.671202\n",
      "[168]\ttrain-kappa:-0.671393\n",
      "[169]\ttrain-kappa:-0.671497\n",
      "[170]\ttrain-kappa:-0.671750\n",
      "[171]\ttrain-kappa:-0.672234\n",
      "[172]\ttrain-kappa:-0.672602\n",
      "[173]\ttrain-kappa:-0.672615\n",
      "[174]\ttrain-kappa:-0.673099\n",
      "[175]\ttrain-kappa:-0.673248\n",
      "[176]\ttrain-kappa:-0.673345\n",
      "[177]\ttrain-kappa:-0.673918\n",
      "[178]\ttrain-kappa:-0.674246\n",
      "[179]\ttrain-kappa:-0.674674\n",
      "[180]\ttrain-kappa:-0.674723\n",
      "[181]\ttrain-kappa:-0.674826\n",
      "[182]\ttrain-kappa:-0.674961\n",
      "[183]\ttrain-kappa:-0.675138\n",
      "[184]\ttrain-kappa:-0.675515\n",
      "[185]\ttrain-kappa:-0.675693\n",
      "[186]\ttrain-kappa:-0.675841\n",
      "[187]\ttrain-kappa:-0.676011\n",
      "[188]\ttrain-kappa:-0.676197\n",
      "[189]\ttrain-kappa:-0.676483\n",
      "[190]\ttrain-kappa:-0.676712\n",
      "[191]\ttrain-kappa:-0.676926\n",
      "[192]\ttrain-kappa:-0.677370\n",
      "[193]\ttrain-kappa:-0.677524\n",
      "[194]\ttrain-kappa:-0.677728\n",
      "[195]\ttrain-kappa:-0.677857\n",
      "[196]\ttrain-kappa:-0.678011\n",
      "[197]\ttrain-kappa:-0.678624\n",
      "[198]\ttrain-kappa:-0.678665\n",
      "[199]\ttrain-kappa:-0.678693\n",
      "[200]\ttrain-kappa:-0.679009\n",
      "[201]\ttrain-kappa:-0.678976\n",
      "[202]\ttrain-kappa:-0.679372\n",
      "[203]\ttrain-kappa:-0.679743\n",
      "[204]\ttrain-kappa:-0.679962\n",
      "[205]\ttrain-kappa:-0.680370\n",
      "[206]\ttrain-kappa:-0.680613\n",
      "[207]\ttrain-kappa:-0.680855\n",
      "[208]\ttrain-kappa:-0.680924\n",
      "[209]\ttrain-kappa:-0.681189\n",
      "[210]\ttrain-kappa:-0.681491\n",
      "[211]\ttrain-kappa:-0.681858\n",
      "[212]\ttrain-kappa:-0.682008\n",
      "[213]\ttrain-kappa:-0.682148\n",
      "[214]\ttrain-kappa:-0.682402\n",
      "[215]\ttrain-kappa:-0.682911\n",
      "[216]\ttrain-kappa:-0.683105\n",
      "[217]\ttrain-kappa:-0.683431\n",
      "[218]\ttrain-kappa:-0.683746\n",
      "[219]\ttrain-kappa:-0.683933\n",
      "[220]\ttrain-kappa:-0.684110\n",
      "[221]\ttrain-kappa:-0.684321\n",
      "[222]\ttrain-kappa:-0.684308\n",
      "[223]\ttrain-kappa:-0.684605\n",
      "[224]\ttrain-kappa:-0.684712\n",
      "[225]\ttrain-kappa:-0.684808\n",
      "[226]\ttrain-kappa:-0.684958\n",
      "[227]\ttrain-kappa:-0.685032\n",
      "[228]\ttrain-kappa:-0.685410\n",
      "[229]\ttrain-kappa:-0.685463\n",
      "[230]\ttrain-kappa:-0.685832\n",
      "[231]\ttrain-kappa:-0.685905\n",
      "[232]\ttrain-kappa:-0.686175\n",
      "[233]\ttrain-kappa:-0.686428\n",
      "[234]\ttrain-kappa:-0.686456\n",
      "[235]\ttrain-kappa:-0.686686\n",
      "[236]\ttrain-kappa:-0.686746\n",
      "[237]\ttrain-kappa:-0.686857\n",
      "[238]\ttrain-kappa:-0.687072\n",
      "[239]\ttrain-kappa:-0.687156\n",
      "[240]\ttrain-kappa:-0.687601\n",
      "[241]\ttrain-kappa:-0.687586\n",
      "[242]\ttrain-kappa:-0.687895\n",
      "[243]\ttrain-kappa:-0.688075\n",
      "[244]\ttrain-kappa:-0.688112\n",
      "[245]\ttrain-kappa:-0.688451\n",
      "[246]\ttrain-kappa:-0.688570\n",
      "[247]\ttrain-kappa:-0.688827\n",
      "[248]\ttrain-kappa:-0.689225\n",
      "[249]\ttrain-kappa:-0.689495\n",
      "[250]\ttrain-kappa:-0.689798\n",
      "[251]\ttrain-kappa:-0.689745\n",
      "[252]\ttrain-kappa:-0.689827\n",
      "[253]\ttrain-kappa:-0.690453\n",
      "[254]\ttrain-kappa:-0.690463\n",
      "[255]\ttrain-kappa:-0.690602\n",
      "[256]\ttrain-kappa:-0.690721\n",
      "[257]\ttrain-kappa:-0.690919\n",
      "[258]\ttrain-kappa:-0.691212\n",
      "[259]\ttrain-kappa:-0.691280\n",
      "[260]\ttrain-kappa:-0.691421\n",
      "[261]\ttrain-kappa:-0.691557\n",
      "[262]\ttrain-kappa:-0.691736\n",
      "[263]\ttrain-kappa:-0.691782\n",
      "[264]\ttrain-kappa:-0.692121\n",
      "[265]\ttrain-kappa:-0.692348\n",
      "[266]\ttrain-kappa:-0.692511\n",
      "[267]\ttrain-kappa:-0.692709\n",
      "[268]\ttrain-kappa:-0.692880\n",
      "[269]\ttrain-kappa:-0.693232\n",
      "[270]\ttrain-kappa:-0.693693\n",
      "[271]\ttrain-kappa:-0.693966\n",
      "[272]\ttrain-kappa:-0.694029\n",
      "[273]\ttrain-kappa:-0.694214\n",
      "[274]\ttrain-kappa:-0.694365\n",
      "[275]\ttrain-kappa:-0.694527\n",
      "[276]\ttrain-kappa:-0.694571\n",
      "[277]\ttrain-kappa:-0.695152\n",
      "[278]\ttrain-kappa:-0.695503\n",
      "[279]\ttrain-kappa:-0.695811\n",
      "[280]\ttrain-kappa:-0.696018\n",
      "[281]\ttrain-kappa:-0.696194\n",
      "[282]\ttrain-kappa:-0.696252\n",
      "[283]\ttrain-kappa:-0.696221\n",
      "[284]\ttrain-kappa:-0.696239\n",
      "[285]\ttrain-kappa:-0.696381\n",
      "[286]\ttrain-kappa:-0.696653\n",
      "[287]\ttrain-kappa:-0.696880\n",
      "[288]\ttrain-kappa:-0.697119\n",
      "[289]\ttrain-kappa:-0.697283\n",
      "[290]\ttrain-kappa:-0.697421\n",
      "[291]\ttrain-kappa:-0.697871\n",
      "[292]\ttrain-kappa:-0.698005\n",
      "[293]\ttrain-kappa:-0.698110\n",
      "[294]\ttrain-kappa:-0.698543\n",
      "[295]\ttrain-kappa:-0.698673\n",
      "[296]\ttrain-kappa:-0.698908\n",
      "[297]\ttrain-kappa:-0.699028\n",
      "[298]\ttrain-kappa:-0.699203\n",
      "[299]\ttrain-kappa:-0.699185\n",
      "[300]\ttrain-kappa:-0.699377\n",
      "[301]\ttrain-kappa:-0.699391\n",
      "[302]\ttrain-kappa:-0.699508\n",
      "[303]\ttrain-kappa:-0.699913\n",
      "[304]\ttrain-kappa:-0.700094\n",
      "[305]\ttrain-kappa:-0.700268\n",
      "[306]\ttrain-kappa:-0.700354\n",
      "[307]\ttrain-kappa:-0.700582\n",
      "[308]\ttrain-kappa:-0.700749\n",
      "[309]\ttrain-kappa:-0.700987\n",
      "[310]\ttrain-kappa:-0.701102\n",
      "[311]\ttrain-kappa:-0.701482\n",
      "[312]\ttrain-kappa:-0.701524\n",
      "[313]\ttrain-kappa:-0.701892\n",
      "[314]\ttrain-kappa:-0.701986\n",
      "[315]\ttrain-kappa:-0.702042\n",
      "[316]\ttrain-kappa:-0.702131\n",
      "[317]\ttrain-kappa:-0.702316\n",
      "[318]\ttrain-kappa:-0.702440\n",
      "[319]\ttrain-kappa:-0.702566\n",
      "[320]\ttrain-kappa:-0.702865\n",
      "[321]\ttrain-kappa:-0.702952\n",
      "[322]\ttrain-kappa:-0.703081\n",
      "[323]\ttrain-kappa:-0.703192\n",
      "[324]\ttrain-kappa:-0.703253\n",
      "[325]\ttrain-kappa:-0.703525\n",
      "[326]\ttrain-kappa:-0.703598\n",
      "[327]\ttrain-kappa:-0.703873\n",
      "[328]\ttrain-kappa:-0.703924\n",
      "[329]\ttrain-kappa:-0.704181\n",
      "[330]\ttrain-kappa:-0.704149\n",
      "[331]\ttrain-kappa:-0.704326\n",
      "[332]\ttrain-kappa:-0.704354\n",
      "[333]\ttrain-kappa:-0.704530\n",
      "[334]\ttrain-kappa:-0.704663\n",
      "[335]\ttrain-kappa:-0.704662\n",
      "[336]\ttrain-kappa:-0.704860\n",
      "[337]\ttrain-kappa:-0.705146\n",
      "[338]\ttrain-kappa:-0.705256\n",
      "[339]\ttrain-kappa:-0.705440\n",
      "[340]\ttrain-kappa:-0.705453\n",
      "[341]\ttrain-kappa:-0.705555\n",
      "[342]\ttrain-kappa:-0.705831\n",
      "[343]\ttrain-kappa:-0.705938\n",
      "[344]\ttrain-kappa:-0.706151\n",
      "[345]\ttrain-kappa:-0.706160\n",
      "[346]\ttrain-kappa:-0.706316\n",
      "[347]\ttrain-kappa:-0.706464\n",
      "[348]\ttrain-kappa:-0.706679\n",
      "[349]\ttrain-kappa:-0.706804\n",
      "[350]\ttrain-kappa:-0.706954\n",
      "[351]\ttrain-kappa:-0.707032\n",
      "[352]\ttrain-kappa:-0.707130\n",
      "[353]\ttrain-kappa:-0.707272\n",
      "[354]\ttrain-kappa:-0.707421\n",
      "[355]\ttrain-kappa:-0.707607\n",
      "[356]\ttrain-kappa:-0.707642\n",
      "[357]\ttrain-kappa:-0.707836\n",
      "[358]\ttrain-kappa:-0.707878\n",
      "[359]\ttrain-kappa:-0.708024\n",
      "[360]\ttrain-kappa:-0.708125\n",
      "[361]\ttrain-kappa:-0.708279\n",
      "[362]\ttrain-kappa:-0.708428\n",
      "[363]\ttrain-kappa:-0.708506\n",
      "[364]\ttrain-kappa:-0.708506\n",
      "[365]\ttrain-kappa:-0.708691\n",
      "[366]\ttrain-kappa:-0.708750\n",
      "[367]\ttrain-kappa:-0.709023\n",
      "[368]\ttrain-kappa:-0.709418\n",
      "[369]\ttrain-kappa:-0.709411\n",
      "[370]\ttrain-kappa:-0.709493\n",
      "[371]\ttrain-kappa:-0.709730\n",
      "[372]\ttrain-kappa:-0.709829\n",
      "[373]\ttrain-kappa:-0.709952\n",
      "[374]\ttrain-kappa:-0.709988\n",
      "[375]\ttrain-kappa:-0.710141\n",
      "[376]\ttrain-kappa:-0.710257\n",
      "[377]\ttrain-kappa:-0.710256\n",
      "[378]\ttrain-kappa:-0.710314\n",
      "[379]\ttrain-kappa:-0.710410\n",
      "[380]\ttrain-kappa:-0.710439\n",
      "[381]\ttrain-kappa:-0.710500\n",
      "[382]\ttrain-kappa:-0.710601\n",
      "[383]\ttrain-kappa:-0.710726\n",
      "[384]\ttrain-kappa:-0.710945\n",
      "[385]\ttrain-kappa:-0.711064\n",
      "[386]\ttrain-kappa:-0.711286\n",
      "[387]\ttrain-kappa:-0.711485\n",
      "[388]\ttrain-kappa:-0.711555\n",
      "[389]\ttrain-kappa:-0.711719\n",
      "[390]\ttrain-kappa:-0.711704\n",
      "[391]\ttrain-kappa:-0.711803\n",
      "[392]\ttrain-kappa:-0.711981\n",
      "[393]\ttrain-kappa:-0.711996\n",
      "[394]\ttrain-kappa:-0.712184\n",
      "[395]\ttrain-kappa:-0.712159\n",
      "[396]\ttrain-kappa:-0.712202\n",
      "[397]\ttrain-kappa:-0.712359\n",
      "[398]\ttrain-kappa:-0.712482\n",
      "[399]\ttrain-kappa:-0.712592\n",
      "[400]\ttrain-kappa:-0.712611\n",
      "[401]\ttrain-kappa:-0.712587\n",
      "[402]\ttrain-kappa:-0.712868\n",
      "[403]\ttrain-kappa:-0.712874\n",
      "[404]\ttrain-kappa:-0.713057\n",
      "[405]\ttrain-kappa:-0.713267\n",
      "[406]\ttrain-kappa:-0.713431\n",
      "[407]\ttrain-kappa:-0.713792\n",
      "[408]\ttrain-kappa:-0.713976\n",
      "[409]\ttrain-kappa:-0.714074\n",
      "[410]\ttrain-kappa:-0.714262\n",
      "[411]\ttrain-kappa:-0.714238\n",
      "[412]\ttrain-kappa:-0.714282\n",
      "[413]\ttrain-kappa:-0.714507\n",
      "[414]\ttrain-kappa:-0.714722\n",
      "[415]\ttrain-kappa:-0.714841\n",
      "[416]\ttrain-kappa:-0.714920\n",
      "[417]\ttrain-kappa:-0.715005\n",
      "[418]\ttrain-kappa:-0.715038\n",
      "[419]\ttrain-kappa:-0.715178\n",
      "[420]\ttrain-kappa:-0.715167\n",
      "[421]\ttrain-kappa:-0.715239\n",
      "[422]\ttrain-kappa:-0.715337\n",
      "[423]\ttrain-kappa:-0.715354\n",
      "[424]\ttrain-kappa:-0.715507\n",
      "[425]\ttrain-kappa:-0.715745\n",
      "[426]\ttrain-kappa:-0.715860\n",
      "[427]\ttrain-kappa:-0.716018\n",
      "[428]\ttrain-kappa:-0.716040\n",
      "[429]\ttrain-kappa:-0.716139\n",
      "[430]\ttrain-kappa:-0.716407\n",
      "[431]\ttrain-kappa:-0.716519\n",
      "[432]\ttrain-kappa:-0.716617\n",
      "[433]\ttrain-kappa:-0.716660\n",
      "[434]\ttrain-kappa:-0.716778\n",
      "[435]\ttrain-kappa:-0.716945\n",
      "[436]\ttrain-kappa:-0.716948\n",
      "[437]\ttrain-kappa:-0.717121\n",
      "[438]\ttrain-kappa:-0.717294\n",
      "[439]\ttrain-kappa:-0.717451\n",
      "[440]\ttrain-kappa:-0.717363\n",
      "[441]\ttrain-kappa:-0.717564\n",
      "[442]\ttrain-kappa:-0.717706\n",
      "[443]\ttrain-kappa:-0.717930\n",
      "[444]\ttrain-kappa:-0.718108\n",
      "[445]\ttrain-kappa:-0.718157\n",
      "[446]\ttrain-kappa:-0.718274\n",
      "[447]\ttrain-kappa:-0.718550\n",
      "[448]\ttrain-kappa:-0.718630\n",
      "[449]\ttrain-kappa:-0.718785\n",
      "[450]\ttrain-kappa:-0.718897\n",
      "[451]\ttrain-kappa:-0.719292\n",
      "[452]\ttrain-kappa:-0.719512\n",
      "[453]\ttrain-kappa:-0.719651\n",
      "[454]\ttrain-kappa:-0.719710\n",
      "[455]\ttrain-kappa:-0.719740\n",
      "[456]\ttrain-kappa:-0.719767\n",
      "[457]\ttrain-kappa:-0.719927\n",
      "[458]\ttrain-kappa:-0.720036\n",
      "[459]\ttrain-kappa:-0.720080\n",
      "[460]\ttrain-kappa:-0.720148\n",
      "[461]\ttrain-kappa:-0.720221\n",
      "[462]\ttrain-kappa:-0.720445\n",
      "[463]\ttrain-kappa:-0.720492\n",
      "[464]\ttrain-kappa:-0.720711\n",
      "[465]\ttrain-kappa:-0.720775\n",
      "[466]\ttrain-kappa:-0.720786\n",
      "[467]\ttrain-kappa:-0.720964\n",
      "[468]\ttrain-kappa:-0.721105\n",
      "[469]\ttrain-kappa:-0.721156\n",
      "[470]\ttrain-kappa:-0.721235\n",
      "[471]\ttrain-kappa:-0.721424\n",
      "[472]\ttrain-kappa:-0.721537\n",
      "[473]\ttrain-kappa:-0.721551\n",
      "[474]\ttrain-kappa:-0.721807\n",
      "[475]\ttrain-kappa:-0.721944\n",
      "[476]\ttrain-kappa:-0.721991\n",
      "[477]\ttrain-kappa:-0.722046\n",
      "[478]\ttrain-kappa:-0.722293\n",
      "[479]\ttrain-kappa:-0.722401\n",
      "[480]\ttrain-kappa:-0.722425\n",
      "[481]\ttrain-kappa:-0.722846\n",
      "[482]\ttrain-kappa:-0.722954\n",
      "[483]\ttrain-kappa:-0.723192\n",
      "[484]\ttrain-kappa:-0.723283\n",
      "[485]\ttrain-kappa:-0.723421\n",
      "[486]\ttrain-kappa:-0.723609\n",
      "[487]\ttrain-kappa:-0.723618\n",
      "[488]\ttrain-kappa:-0.723725\n",
      "[489]\ttrain-kappa:-0.723840\n",
      "[490]\ttrain-kappa:-0.723863\n",
      "[491]\ttrain-kappa:-0.723954\n",
      "[492]\ttrain-kappa:-0.723994\n",
      "[493]\ttrain-kappa:-0.724184\n",
      "[494]\ttrain-kappa:-0.724345\n",
      "[495]\ttrain-kappa:-0.724619\n",
      "[496]\ttrain-kappa:-0.724734\n",
      "[497]\ttrain-kappa:-0.724780\n",
      "[498]\ttrain-kappa:-0.724977\n",
      "[499]\ttrain-kappa:-0.725003\n",
      "[500]\ttrain-kappa:-0.725197\n",
      "[501]\ttrain-kappa:-0.725289\n",
      "[502]\ttrain-kappa:-0.725658\n",
      "[503]\ttrain-kappa:-0.725740\n",
      "[504]\ttrain-kappa:-0.725780\n",
      "[505]\ttrain-kappa:-0.725804\n",
      "[506]\ttrain-kappa:-0.726030\n",
      "[507]\ttrain-kappa:-0.726156\n",
      "[508]\ttrain-kappa:-0.726178\n",
      "[509]\ttrain-kappa:-0.726280\n",
      "[510]\ttrain-kappa:-0.726269\n",
      "[511]\ttrain-kappa:-0.726418\n",
      "[512]\ttrain-kappa:-0.726543\n",
      "[513]\ttrain-kappa:-0.726635\n",
      "[514]\ttrain-kappa:-0.726809\n",
      "[515]\ttrain-kappa:-0.726872\n",
      "[516]\ttrain-kappa:-0.726926\n",
      "[517]\ttrain-kappa:-0.727022\n",
      "[518]\ttrain-kappa:-0.727141\n",
      "[519]\ttrain-kappa:-0.727232\n",
      "[520]\ttrain-kappa:-0.727433\n",
      "[521]\ttrain-kappa:-0.727408\n",
      "[522]\ttrain-kappa:-0.727482\n",
      "[523]\ttrain-kappa:-0.727586\n",
      "[524]\ttrain-kappa:-0.727528\n",
      "[525]\ttrain-kappa:-0.727592\n",
      "[526]\ttrain-kappa:-0.727606\n",
      "[527]\ttrain-kappa:-0.727728\n",
      "[528]\ttrain-kappa:-0.727845\n",
      "[529]\ttrain-kappa:-0.727948\n",
      "[530]\ttrain-kappa:-0.727915\n",
      "[531]\ttrain-kappa:-0.728067\n",
      "[532]\ttrain-kappa:-0.728129\n",
      "[533]\ttrain-kappa:-0.728174\n",
      "[534]\ttrain-kappa:-0.728271\n",
      "[535]\ttrain-kappa:-0.728352\n",
      "[536]\ttrain-kappa:-0.728422\n",
      "[537]\ttrain-kappa:-0.728595\n",
      "[538]\ttrain-kappa:-0.728667\n",
      "[539]\ttrain-kappa:-0.728832\n",
      "[540]\ttrain-kappa:-0.729033\n",
      "[541]\ttrain-kappa:-0.729100\n",
      "[542]\ttrain-kappa:-0.729161\n",
      "[543]\ttrain-kappa:-0.729427\n",
      "[544]\ttrain-kappa:-0.729628\n",
      "[545]\ttrain-kappa:-0.729662\n",
      "[546]\ttrain-kappa:-0.729740\n",
      "[547]\ttrain-kappa:-0.729935\n",
      "[548]\ttrain-kappa:-0.730114\n",
      "[549]\ttrain-kappa:-0.730198\n",
      "[550]\ttrain-kappa:-0.730410\n",
      "[551]\ttrain-kappa:-0.730532\n",
      "[552]\ttrain-kappa:-0.730754\n",
      "[553]\ttrain-kappa:-0.730775\n",
      "[554]\ttrain-kappa:-0.730742\n",
      "[555]\ttrain-kappa:-0.730873\n",
      "[556]\ttrain-kappa:-0.730915\n",
      "[557]\ttrain-kappa:-0.731100\n",
      "[558]\ttrain-kappa:-0.731122\n",
      "[559]\ttrain-kappa:-0.731349\n",
      "[560]\ttrain-kappa:-0.731423\n",
      "[561]\ttrain-kappa:-0.731709\n",
      "[562]\ttrain-kappa:-0.731633\n",
      "[563]\ttrain-kappa:-0.731690\n",
      "[564]\ttrain-kappa:-0.731745\n",
      "[565]\ttrain-kappa:-0.731735\n",
      "[566]\ttrain-kappa:-0.731813\n",
      "[567]\ttrain-kappa:-0.731957\n",
      "[568]\ttrain-kappa:-0.731982\n",
      "[569]\ttrain-kappa:-0.731995\n",
      "[570]\ttrain-kappa:-0.732019\n",
      "[571]\ttrain-kappa:-0.732318\n",
      "[572]\ttrain-kappa:-0.732413\n",
      "[573]\ttrain-kappa:-0.732548\n",
      "[574]\ttrain-kappa:-0.732787\n",
      "[575]\ttrain-kappa:-0.732823\n",
      "[576]\ttrain-kappa:-0.732905\n",
      "[577]\ttrain-kappa:-0.733164\n",
      "[578]\ttrain-kappa:-0.733294\n",
      "[579]\ttrain-kappa:-0.733290\n",
      "[580]\ttrain-kappa:-0.733508\n",
      "[581]\ttrain-kappa:-0.733391\n",
      "[582]\ttrain-kappa:-0.733388\n",
      "[583]\ttrain-kappa:-0.733489\n",
      "[584]\ttrain-kappa:-0.733581\n",
      "[585]\ttrain-kappa:-0.733713\n",
      "[586]\ttrain-kappa:-0.733806\n",
      "[587]\ttrain-kappa:-0.733828\n",
      "[588]\ttrain-kappa:-0.733805\n",
      "[589]\ttrain-kappa:-0.734017\n",
      "[590]\ttrain-kappa:-0.734024\n",
      "[591]\ttrain-kappa:-0.734035\n",
      "[592]\ttrain-kappa:-0.734159\n",
      "[593]\ttrain-kappa:-0.734199\n",
      "[594]\ttrain-kappa:-0.734286\n",
      "[595]\ttrain-kappa:-0.734279\n",
      "[596]\ttrain-kappa:-0.734435\n",
      "[597]\ttrain-kappa:-0.734631\n",
      "[598]\ttrain-kappa:-0.734684\n",
      "[599]\ttrain-kappa:-0.734753\n",
      "[600]\ttrain-kappa:-0.734864\n",
      "[601]\ttrain-kappa:-0.735004\n",
      "[602]\ttrain-kappa:-0.735268\n",
      "[603]\ttrain-kappa:-0.735402\n",
      "[604]\ttrain-kappa:-0.735238\n",
      "[605]\ttrain-kappa:-0.735384\n",
      "[606]\ttrain-kappa:-0.735501\n",
      "[607]\ttrain-kappa:-0.735524\n",
      "[608]\ttrain-kappa:-0.735549\n",
      "[609]\ttrain-kappa:-0.735593\n",
      "[610]\ttrain-kappa:-0.735629\n",
      "[611]\ttrain-kappa:-0.735644\n",
      "[612]\ttrain-kappa:-0.735994\n",
      "[613]\ttrain-kappa:-0.736165\n",
      "[614]\ttrain-kappa:-0.736151\n",
      "[615]\ttrain-kappa:-0.736283\n",
      "[616]\ttrain-kappa:-0.736327\n",
      "[617]\ttrain-kappa:-0.736411\n",
      "[618]\ttrain-kappa:-0.736433\n",
      "[619]\ttrain-kappa:-0.736486\n",
      "[620]\ttrain-kappa:-0.736601\n",
      "[621]\ttrain-kappa:-0.736647\n",
      "[622]\ttrain-kappa:-0.736726\n",
      "[623]\ttrain-kappa:-0.736862\n",
      "[624]\ttrain-kappa:-0.736997\n",
      "[625]\ttrain-kappa:-0.737036\n",
      "[626]\ttrain-kappa:-0.737022\n",
      "[627]\ttrain-kappa:-0.737137\n",
      "[628]\ttrain-kappa:-0.737211\n",
      "[629]\ttrain-kappa:-0.737404\n",
      "[630]\ttrain-kappa:-0.737385\n",
      "[631]\ttrain-kappa:-0.737445\n",
      "[632]\ttrain-kappa:-0.737600\n",
      "[633]\ttrain-kappa:-0.737542\n",
      "[634]\ttrain-kappa:-0.737661\n",
      "[635]\ttrain-kappa:-0.737754\n",
      "[636]\ttrain-kappa:-0.737836\n",
      "[637]\ttrain-kappa:-0.737951\n",
      "[638]\ttrain-kappa:-0.737986\n",
      "[639]\ttrain-kappa:-0.738040\n",
      "[640]\ttrain-kappa:-0.738051\n",
      "[641]\ttrain-kappa:-0.738099\n",
      "[642]\ttrain-kappa:-0.738103\n",
      "[643]\ttrain-kappa:-0.738182\n",
      "[644]\ttrain-kappa:-0.738544\n",
      "[645]\ttrain-kappa:-0.738591\n",
      "[646]\ttrain-kappa:-0.738766\n",
      "[647]\ttrain-kappa:-0.739068\n",
      "[648]\ttrain-kappa:-0.739116\n",
      "[649]\ttrain-kappa:-0.739144\n",
      "[650]\ttrain-kappa:-0.739248\n",
      "[651]\ttrain-kappa:-0.739365\n",
      "[652]\ttrain-kappa:-0.739470\n",
      "[653]\ttrain-kappa:-0.739538\n",
      "[654]\ttrain-kappa:-0.739654\n",
      "[655]\ttrain-kappa:-0.739728\n",
      "[656]\ttrain-kappa:-0.739817\n",
      "[657]\ttrain-kappa:-0.739999\n",
      "[658]\ttrain-kappa:-0.740058\n",
      "[659]\ttrain-kappa:-0.740087\n",
      "[660]\ttrain-kappa:-0.740163\n",
      "[661]\ttrain-kappa:-0.740256\n",
      "[662]\ttrain-kappa:-0.740273\n",
      "[663]\ttrain-kappa:-0.740487\n",
      "[664]\ttrain-kappa:-0.740548\n",
      "[665]\ttrain-kappa:-0.740737\n",
      "[666]\ttrain-kappa:-0.740759\n",
      "[667]\ttrain-kappa:-0.740745\n",
      "[668]\ttrain-kappa:-0.740923\n",
      "[669]\ttrain-kappa:-0.741068\n",
      "[670]\ttrain-kappa:-0.741218\n",
      "[671]\ttrain-kappa:-0.741194\n",
      "[672]\ttrain-kappa:-0.741272\n",
      "[673]\ttrain-kappa:-0.741385\n",
      "[674]\ttrain-kappa:-0.741523\n",
      "[675]\ttrain-kappa:-0.741780\n",
      "[676]\ttrain-kappa:-0.741779\n",
      "[677]\ttrain-kappa:-0.742008\n",
      "[678]\ttrain-kappa:-0.742126\n",
      "[679]\ttrain-kappa:-0.742145\n",
      "[680]\ttrain-kappa:-0.742342\n",
      "[681]\ttrain-kappa:-0.742498\n",
      "[682]\ttrain-kappa:-0.742540\n",
      "[683]\ttrain-kappa:-0.742522\n",
      "[684]\ttrain-kappa:-0.742811\n",
      "[685]\ttrain-kappa:-0.742772\n",
      "[686]\ttrain-kappa:-0.742820\n",
      "[687]\ttrain-kappa:-0.742838\n",
      "[688]\ttrain-kappa:-0.742983\n",
      "[689]\ttrain-kappa:-0.743056\n",
      "[690]\ttrain-kappa:-0.743144\n",
      "[691]\ttrain-kappa:-0.743283\n",
      "[692]\ttrain-kappa:-0.743272\n",
      "[693]\ttrain-kappa:-0.743259\n",
      "[694]\ttrain-kappa:-0.743326\n",
      "[695]\ttrain-kappa:-0.743424\n",
      "[696]\ttrain-kappa:-0.743592\n",
      "[697]\ttrain-kappa:-0.743512\n",
      "[698]\ttrain-kappa:-0.743559\n",
      "[699]\ttrain-kappa:-0.743673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.747403\n",
      "         Iterations: 115\n",
      "         Function evaluations: 239\n",
      "Train QWK: 0.747403149753\n",
      "\n",
      "0.660406480846\n",
      "Overall Test Qwk: 0.65633649509\n"
     ]
    }
   ],
   "source": [
    "from keras import objectives\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.20, random_state=1892)\n",
    "X_train = train\n",
    "y_train = labels\n",
    "\n",
    "folds = 4\n",
    "kf = KFold(len(X_train), folds)\n",
    "num = 1\n",
    "combinedModels = list()\n",
    "qwks = list()\n",
    "\n",
    "cpa = np.ndarray((5, 7))\n",
    "# cpa[0,:] = [1.94094589,  2.77827061, 4.47922564,  5.59375133,  5.98938611,  6.86875996, 7.45790894]\n",
    "cpa[0,:] = [2.27420049,  3.32644001,  3.22110338,  5.37172015,  6.01216561,  6.68318259, 7.4242825]\n",
    "cpa[1,:] = [1.93043217,  3.06875824,  4.8242938,   5.40717815,  6.17241788,  6.97729218, 7.46620894]\n",
    "cpa[2,:] = [1.23315182,  3.39688586,  4.6380404,   5.37977014,  6.16224374,  6.73081194, 7.48322552]\n",
    "cpa[3,:] = [2.33061803,  3.5055729,   4.52173829,  5.24184554,  6.1711789,   6.96746293, 7.7211743]\n",
    "cpa[4,:] = [2.26716625,  3.59018596,  4.3780072,   5.21537676,  5.95488354,  6.72023874, 7.5133742]\n",
    "\n",
    "# nnModel = NN(inputShape = train.shape[1], cutPointArray=cpa, layers = [250, 75, 25], dropout = [0.5, 0.5, 0.5], activation='relu', patience=5, loss=custom_loss, optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 50)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    \n",
    "    combinedModel = CombinedModel([], [XGBoostModel(700, 7, 0.025, 0.50, 25)])\n",
    "#     combinedModel = CombinedModel(['Boost'], [])\n",
    "    \n",
    "    trainFile = 'combinedTrainPredictions%s.csv' % str(num)\n",
    "    validateFile = 'combinedValidatePredictions%s.csv' % str(num)\n",
    "\n",
    "    xTrain = X_train.iloc[train_index].values\n",
    "    yTrain = y_train.iloc[train_index]      \n",
    "    xValidate = X_train.iloc[test_index].values\n",
    "    yValidate = y_train.iloc[test_index]\n",
    "\n",
    "    combinedModel.fit(xTrain, yTrain, trainFile, num-1)\n",
    "    predictions = combinedModel.predict(xValidate, validateFile, num-1)\n",
    "    qwk = quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, yValidate)\n",
    "    qwks.append(qwk)\n",
    "    print qwk\n",
    "    \n",
    "    combinedModels.append(combinedModel)\n",
    "    num += 1\n",
    "    \n",
    "meanQwk = quadratic_weighted_kappa.mean_quadratic_weighted_kappa(qwks)\n",
    "print \"Overall Test Qwk: %s\" % meanQwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# validatePredictions = np.zeros(len(X_test))\n",
    "cutPoints = np.ndarray((7, 7))\n",
    "coef = 0\n",
    "inter = 0\n",
    "for i in range(1, folds + 1):\n",
    "    model = combinedModels[i-1]\n",
    "#     print model.cutPoints\n",
    "    coef += model.stackingModel.coef_ / folds\n",
    "    inter += model.stackingModel.intercept_ / folds\n",
    "    cutPoints[i-1] = model.cutPoints\n",
    "    \n",
    "print coef\n",
    "print inter\n",
    "    \n",
    "# print np.mean(cutPoints, axis=0)\n",
    "#     validatePredictions += model.predict(X_test, 'combinedTestPredictions%s.csv' % str(i), i)\n",
    "    \n",
    "# validatePredictions /= folds\n",
    "# validatePredictions = np.round(validatePredictions).astype(int)\n",
    "\n",
    "# print quadratic_weighted_kappa.quadratic_weighted_kappa(validatePredictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CombinedModel:\n",
    "    \n",
    "    def __init__(self, modelsFromFile, modelsToCalculate):\n",
    "        self.modelsToCalculate = modelsToCalculate \n",
    "        self.modelsFromFile = modelsFromFile\n",
    "        self.stackingModel = LinearRegression()\n",
    "        self.boost = [0.652626727963, 0.652281118598, 0.651128156224, 0.651189716136, 0.662062433355]\n",
    "        self.keras = [0.621456525096, 0.640965818589, 0.64586691993, 0.66517675706, 0.679622912852]\n",
    "        \n",
    "    def fit(self, X, Y, fileName, num):     \n",
    "        \n",
    "        stackingData = np.ndarray((X.shape[0], len(self.modelsFromFile) + len(self.modelsToCalculate)))    \n",
    "        df = pd.read_csv(fileName) if os.path.isfile(fileName) else pd.DataFrame()\n",
    "        \n",
    "        for i in range(len(self.modelsToCalculate)):\n",
    "            model = self.modelsToCalculate[i]\n",
    "            model.fit(X, Y, num)\n",
    "            predictions = model.predict(X)\n",
    "            stackingData[:,i] = predictions\n",
    "#             df[model] = predictions\n",
    "            \n",
    "        if len(self.modelsFromFile) > 0:\n",
    "            colsToChange = range(len(self.modelsToCalculate), len(self.modelsToCalculate) + len(self.modelsFromFile))\n",
    "            stackingData[:,colsToChange] = df[self.modelsFromFile].values\n",
    "            \n",
    "        self.stackingModel.fit(stackingData, Y)\n",
    "        predictions = self.stackingModel.predict(stackingData)\n",
    "#         predictions = np.mean(stackingData, axis=1)\n",
    "#         predictions = np.add(self.boost[num] * stackingTrainData[:,0], self.keras[num]*stackingTrainData[:,1]) / (self.boost[num] + self.keras[num])\n",
    "        \n",
    "        initialCutPoints = np.array([1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5])\n",
    "        cpo = CutPointOptimizer(predictions, Y)\n",
    "        self.cutPoints = optimize.fmin(cpo.qwk, initialCutPoints)\n",
    "        \n",
    "        predictions = np.searchsorted(self.cutPoints, predictions) + 1   \n",
    "\n",
    "        trainQwk = quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, Y)\n",
    "        print \"Train QWK: %s\\n\" % trainQwk\n",
    "                           \n",
    "    def predict(self, X, fileName, num):\n",
    "        stackingData = np.ndarray((X.shape[0], len(self.modelsFromFile) + len(self.modelsToCalculate)))\n",
    "        df = pd.read_csv(fileName) if os.path.isfile(fileName) else pd.DataFrame()\n",
    "        \n",
    "        for i in range(len(self.modelsToCalculate)):\n",
    "            model = self.modelsToCalculate[i]\n",
    "            predictions = model.predict(X)\n",
    "            stackingData[:,i] = predictions\n",
    "            \n",
    "        if len(self.modelsFromFile) > 0:\n",
    "            colsToChange = range(len(self.modelsToCalculate), len(self.modelsToCalculate) + len(self.modelsFromFile))\n",
    "            stackingData[:,colsToChange] = df[self.modelsFromFile].values\n",
    "            \n",
    "        predictions = self.stackingModel.predict(stackingData)\n",
    "#         predictions = np.mean(stackingData, axis=1)\n",
    "        return np.searchsorted(self.cutPoints, predictions) + 1      \n",
    "#         return np.searchsorted(np.median(cpa, axis=0), predictions) + 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'folds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b54b60729fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestPredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombinedModels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtestPredictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'combinedTestPredictions%s.csv'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'folds' is not defined"
     ]
    }
   ],
   "source": [
    "testPredictions = np.zeros(len(test))\n",
    "for i in range(1, folds + 1):\n",
    "    model = combinedModels[i-1]\n",
    "    testPredictions += model.predict(test, 'combinedTestPredictions%s.csv' % str(i), i)\n",
    "\n",
    "    \n",
    "testPredictions /= folds\n",
    "predDf = pd.DataFrame()\n",
    "predDf['Id'] = dfTest['Id']\n",
    "predDf['Response'] = np.round(testPredictions).astype(int)\n",
    "print predDf['Response'].values\n",
    "predDf.to_csv(path_or_buf='XgBoost.csv', columns=['Id', 'Response'], index=False, header=['Id', 'Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(len(train), 8)\n",
    "num = 1\n",
    "qwks = list()\n",
    "\n",
    "name = 'Boost'\n",
    "# model = NN(inputShape = train.shape[1], cutPointArray=cpa, layers = [250, 75, 15], dropout = [0.5, 0.5, 0.5], activation='relu', patience=5, loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 50)\n",
    "\n",
    "model = XGBoostModel(800, 7, 0.025, 0.50, 25)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    trainFile = 'combinedTrainPredictions%s.csv' % str(num)\n",
    "    validateFile = 'combinedValidatePredictions%s.csv' % str(num)\n",
    "    testFile = 'combinedTestPredictions%s.csv' % str(num)\n",
    "    \n",
    "    trainDF = pd.read_csv(trainFile) if os.path.isfile(trainFile) else pd.DataFrame()  \n",
    "    validateDF = pd.read_csv(validateFile) if os.path.isfile(validateFile) else pd.DataFrame()  \n",
    "    testDF = pd.read_csv(testFile) if os.path.isfile(testFile) else pd.DataFrame()  \n",
    "\n",
    "    xTrain = train.iloc[train_index].values\n",
    "    yTrain = labels.iloc[train_index]      \n",
    "    xValidate = train.iloc[test_index].values\n",
    "    yValidate = labels.iloc[test_index]\n",
    "    \n",
    "    model.fit(xTrain, yTrain, num-1)\n",
    "    testPredictions = model.predict(test.values)\n",
    "    trainPredictions = model.predict(xTrain)\n",
    "    validatePredictions = model.predict(xValidate)\n",
    "    \n",
    "    trainDF[name] = trainPredictions\n",
    "    validateDF[name] = validatePredictions\n",
    "    testDF[name] = testPredictions\n",
    "        \n",
    "    trainDF.to_csv(path_or_buf=trainFile, index=False)\n",
    "    validateDF.to_csv(path_or_buf=validateFile, index=False)\n",
    "    testDF.to_csv(path_or_buf=testFile, index=False)\n",
    "            \n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keywords = ['Medical_Keyword_' + str(i) for i in range(1, 49)]\n",
    "# print dfTrain[keywords].sum(axis=1)\n",
    "\n",
    "print dfTrain[]['Response'].mean()\n",
    "print dfTrain['Response'].mean()\n",
    "#     Medical_History_10\n",
    "# # uniqueValues = dfTest['InsuredInfo_7'].unique()\n",
    "# # for i in range(len(uniqueValues)):\n",
    "# #     arr = dfTrain['InsuredInfo_7'].apply(lambda x: x == uniqueValues[i])\n",
    "    \n",
    "\n",
    "# for column in ['Product_Info_4', 'Ins_Age', 'Ht', 'Wt', 'BMI', 'Employment_Info_1', 'Employment_Info_4', 'Employment_Info_6', 'Insurance_History_5', 'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5']:\n",
    "   \n",
    "#     if dfTrain[column].isnull().sum():\n",
    "#         print column\n",
    "#         print pearsonr(dfTrain[dfTrain[column].notnull()][column], dfTrain[dfTrain[column].notnull()].Response)\n",
    "#         print dfTrain[column].median()\n",
    "#         print len(dfTrain[dfTrain.Response == 8])\n",
    "#         print len(dfTrain)\n",
    "# #         print dfTrain[column]\n",
    "# #         plt.plot(dfTrain[dfTrain[column].notnull()][column], dfTrain[dfTrain[column].notnull()].Response)\n",
    "#         break\n",
    "# # plt.show()\n",
    "# # print dfTest['InsuredInfo_4'].unique()\n",
    "# # print dfTrain['InsuredInfo_6'].apply(lambda x: x == 1)\n",
    "# print dfTest['Medical_History_32'].isnull().sum()\n",
    "            \n",
    "# print dfTrain['Medical_History_32'].median()\n",
    "# print dfTrain[dfTrain['Family_Hist_2'].notnull()]['Family_Hist_2'].max()\n",
    "# # print dfTrain['Medical_History_32'].null().sum()\n",
    "# # print dfTest[dfTest['Medical_History_32'].notnull()]['Medical_History_32'].median()\n",
    "# # print len(pd.concat([dfTest[dfTest['Medical_History_32'].notnull()]['Medical_History_32'], dfTrain[dfTrain['Medical_History_32'].notnull()]['Medical_History_32']]))\n",
    "# # print pearsonr(dfTrain[dfTrain['Medical_History_32'].notnull()]['Medical_History_32'], dfTrain[dfTrain['Medical_History_32'].notnull()]['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neighborModel = GetBestModel(lambda: NearestNeighbors(), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nearestNeighbors = neighborModel.kneighbors(dfTrain[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = np.zeros(len(dfTrain))\n",
    "for i in range(len(dfTrain)):\n",
    "#     predictions[i] = dfTrain.iloc[nearestNeighbors[1][i]].Response.mean()\n",
    "    responses = dfTrain.iloc[nearestNeighbors[1][i]].Response\n",
    "    weights = nearestNeighbors[0][i][4] - nearestNeighbors[0][i]\n",
    "    predictions[i] = np.sum(responses * weights) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = np.rint(predictions)\n",
    "print quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, dfTrain.Response)\n",
    "print np.min(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "lowerBound = np.mean(dfTrain['BMI']) - np.std(dfTrain['BMI'])\n",
    "upperBound = np.mean(dfTrain['BMI']) + np.std(dfTrain['BMI'])\n",
    "mean = np.mean(dfTrain['BMI'])\n",
    "stats.normaltest(dfTrain['BMI'])\n",
    "\n",
    "bmiScore = dfTrain['BMI'].apply(lambda x: max(0, lowerBound - x) if x < mean else max(0, x - upperBound))\n",
    "\n",
    "model = LinearRegression(normalize=True)\n",
    "features = ['Ins_Age', 'Wt', 'BMI']\n",
    "X = np.ndarray((59381, 9))\n",
    "X[:,0] = dfTrain['Ins_Age'].values\n",
    "X[:,1] = dfTrain['BMI'].values\n",
    "X[:,2] = dfTrain['Wt'].values\n",
    "X[:,3] = dfTrain['Ins_Age'] * dfTrain['Wt']\n",
    "X[:,4] = dfTrain['Ins_Age'] * dfTrain['BMI']\n",
    "X[:,5] = dfTrain['Ins_Age'] * dfTrain['Ins_Age']\n",
    "X[:,6] = dfTrain['Wt'] * dfTrain['Wt']\n",
    "X[:,7] = dfTrain['Wt'] * dfTrain['BMI']\n",
    "X[:,8] = dfTrain['Wt'] * dfTrain['Ins_Age']\n",
    "\n",
    "model.fit(X, dfTrain['Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import quadratic_weighted_kappa\n",
    "y1, y2 = T.vectors('y1', 'y2')\n",
    "# s = 1 / (1 + T.exp(-x))\n",
    "s = quadratic_weighted_kappa.quadratic_weighted_kappa(y1, y2)\n",
    "# logistic = theano.function([y1, y2], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2 layer 200/75 cut points\n",
    "cpa = np.ndarray((5, 7))\n",
    "cpa[0,:] = [2.27420049,  3.32644001,  3.22110338,  5.37172015,  6.01216561,  6.68318259, 7.4242825]\n",
    "cpa[1,:] = [1.93043217,  3.06875824,  4.8242938,   5.40717815,  6.17241788,  6.97729218, 7.46620894]\n",
    "cpa[2,:] = [1.23315182,  3.39688586,  4.6380404,   5.37977014,  6.16224374,  6.73081194, 7.48322552]\n",
    "cpa[3,:] = [2.33061803,  3.5055729,   4.52173829,  5.24184554,  6.1711789,   6.96746293, 7.7211743]\n",
    "cpa[4,:] = [2.26716625,  3.59018596,  4.3780072,   5.21537676,  5.95488354,  6.72023874, 7.5133742]\n",
    "\n",
    "print np.mean(cpa, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "epsilon = 1.0e-9\n",
    "def custom_objective(y_true, y_pred):\n",
    "    y_pred\n",
    "    '''Just another crossentropy'''\n",
    "    y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "    y_pred /= y_pred.sum(axis=-1, keepdims=True)\n",
    "    cce = T.nnet.categorical_crossentropy(y_pred, y_true)\n",
    "    return cce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "results, updates = theano.scan(lambda v: np.searchsorted(cutPoints, v), sequences=y_pred)\n",
    "compute_elementwise = theano.function(inputs=[X, W, b_sym], outputs=[results])\n",
    "\n",
    "result, updates = theano.scan(fn=lambda prior_result, A: np.searchsorted(cutPoints, A)\n",
    "                              outputs_info=T.ones_like(A),\n",
    "                              non_sequences=A,\n",
    "                              n_steps=7)\n",
    "\n",
    "# We only care about A**k, but scan has provided us with A**1 through A**k.\n",
    "# Discard the values that we don't care about. Scan is smart enough to\n",
    "# notice this and not waste memory saving them.\n",
    "prediction = 1\n",
    "for i in range(len(result)):\n",
    "    if num < result[i]:\n",
    "        break\n",
    "    num += 1\n",
    "\n",
    "# compiled function that returns A**k\n",
    "\n",
    "power = theano.function(inputs=[A,k], outputs=prediction, updates=updates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
