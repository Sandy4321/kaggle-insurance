{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import quadratic_weighted_kappa\n",
    "from sklearn.cross_validation import KFold\n",
    "import feature_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    #I made a small wrapper for the Keras model to make it more scikit-learn like\n",
    "    #I think they have something like this built in already, oh well\n",
    "    #See http://keras.io/ for parameter options\n",
    "    def __init__(self, inputShape, layers, dropout = [], activation = 'relu', init = 'uniform', loss = 'rmse', optimizer = 'adadelta', nb_epochs = 50, batch_size = 32, verbose = 1):\n",
    "\n",
    "        model = Sequential()\n",
    "        for i in range(len(layers)):\n",
    "            if i == 0:\n",
    "                print (\"Input shape: \" + str(inputShape))\n",
    "                print (\"Adding Layer \" + str(i) + \": \" + str(layers[i]))\n",
    "                model.add(Dense(layers[i], input_dim = inputShape, init = init))\n",
    "            else:\n",
    "                print (\"Adding Layer \" + str(i) + \": \" + str(layers[i]))\n",
    "                model.add(Dense(layers[i], init = init))\n",
    "            print (\"Adding \" + activation + \" layer\")\n",
    "            model.add(Activation(activation))\n",
    "            model.add(BatchNormalization())\n",
    "            if len(dropout) > i:\n",
    "                print (\"Adding \" + str(dropout[i]) + \" dropout\")\n",
    "                model.add(Dropout(dropout[i]))\n",
    "        model.add(Dense(1, init = init)) #End in a single output node for regression style output\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "        \n",
    "        self.model = model\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y): \n",
    "        self.model.fit(X, y, nb_epoch=self.nb_epochs, batch_size=self.batch_size, verbose = self.verbose)\n",
    "        \n",
    "    def predict(self, X, batch_size = 128, verbose = 1):\n",
    "        return self.model.predict(X, batch_size = batch_size, verbose = verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class pdStandardScaler:\n",
    "    #Applies the sklearn StandardScaler to pandas dataframes\n",
    "    def __init__(self):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        self.StandardScaler = StandardScaler()\n",
    "    def fit(self, df):\n",
    "        self.StandardScaler.fit(df)\n",
    "    def transform(self, df):\n",
    "        df = pd.DataFrame(self.StandardScaler.transform(df), columns=df.columns)\n",
    "        return df\n",
    "    def fit_transform(self, df):\n",
    "        df = pd.DataFrame(self.StandardScaler.fit_transform(df), columns=df.columns)\n",
    "        return df\n",
    "        \n",
    "def getDummiesInplace(columnList, train, test = None):\n",
    "    #Takes in a list of column names and one or two pandas dataframes\n",
    "    #One-hot encodes all indicated columns inplace\n",
    "    columns = []\n",
    "    \n",
    "    if test is not None:\n",
    "        df = pd.concat([train,test], axis= 0)\n",
    "    else:\n",
    "        df = train\n",
    "        \n",
    "    for columnName in df.columns:\n",
    "        index = df.columns.get_loc(columnName)\n",
    "        if columnName in columnList:\n",
    "            dummies = pd.get_dummies(df.ix[:,index], prefix = columnName, prefix_sep = \".\")\n",
    "            columns.append(dummies)\n",
    "        else:\n",
    "            columns.append(df.ix[:,index])\n",
    "    df = pd.concat(columns, axis = 1)\n",
    "    \n",
    "    if test is not None:\n",
    "        train = df[:train.shape[0]]\n",
    "        test = df[train.shape[0]:]\n",
    "        return train, test\n",
    "    else:\n",
    "        train = df\n",
    "        return train\n",
    "        \n",
    "def pdFillNAN(df, strategy = \"mean\"):\n",
    "    #Fills empty values with either the mean value of each feature, or an indicated number\n",
    "    if strategy == \"mean\":\n",
    "        return df.fillna(df.mean())\n",
    "    elif type(strategy) == int:\n",
    "        return df.fillna(strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_dataset(useDummies, fillNANStrategy, useNormalization, train, test):\n",
    "    data_dir = \"./\"\n",
    "    \n",
    "    labels = train[\"Response\"]\n",
    "    train.drop(labels = \"Id\", axis = 1, inplace = True)\n",
    "    train.drop(labels = \"Response\", axis = 1, inplace = True)\n",
    "    test.drop(labels = \"Id\", axis = 1, inplace = True)\n",
    "    \n",
    "    categoricalVariables = [\"Product_Info_1\", \"Product_Info_2\", \"Product_Info_3\", \"Product_Info_5\", \"Product_Info_6\", \"Product_Info_7\", \"Employment_Info_2\", \"Employment_Info_3\", \"Employment_Info_5\", \"InsuredInfo_1\", \"InsuredInfo_2\", \"InsuredInfo_3\", \"InsuredInfo_4\", \"InsuredInfo_5\", \"InsuredInfo_6\", \"InsuredInfo_7\", \"Insurance_History_1\", \"Insurance_History_2\", \"Insurance_History_3\", \"Insurance_History_4\", \"Insurance_History_7\", \"Insurance_History_8\", \"Insurance_History_9\", \"Family_Hist_1\", \"Medical_History_2\", \"Medical_History_3\", \"Medical_History_4\", \"Medical_History_5\", \"Medical_History_6\", \"Medical_History_7\", \"Medical_History_8\", \"Medical_History_9\", \"Medical_History_10\", \"Medical_History_11\", \"Medical_History_12\", \"Medical_History_13\", \"Medical_History_14\", \"Medical_History_16\", \"Medical_History_17\", \"Medical_History_18\", \"Medical_History_19\", \"Medical_History_20\", \"Medical_History_21\", \"Medical_History_22\", \"Medical_History_23\", \"Medical_History_25\", \"Medical_History_26\", \"Medical_History_27\", \"Medical_History_28\", \"Medical_History_29\", \"Medical_History_30\", \"Medical_History_31\", \"Medical_History_33\", \"Medical_History_34\", \"Medical_History_35\", \"Medical_History_36\", \"Medical_History_37\", \"Medical_History_38\", \"Medical_History_39\", \"Medical_History_40\", \"Medical_History_41\"]\n",
    "\n",
    "    if useDummies == True:\n",
    "        print (\"Generating dummies...\")\n",
    "        train, test = getDummiesInplace(categoricalVariables, train, test)\n",
    "    \n",
    "    if fillNANStrategy is not None:\n",
    "        print (\"Filling in missing values...\")\n",
    "        train = pdFillNAN(train, fillNANStrategy)\n",
    "        test = pdFillNAN(test, fillNANStrategy)\n",
    "\n",
    "    if useNormalization == True:\n",
    "        print (\"Scaling...\")\n",
    "        scaler = pdStandardScaler()\n",
    "        train = scaler.fit_transform(train)\n",
    "        test = scaler.transform(test)\n",
    "    \n",
    "    return train, test, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTrain = pd.read_csv('train.csv')\n",
    "dfTest = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical_History_2\n",
      "Medical_History_10\n"
     ]
    }
   ],
   "source": [
    "# features = feature_generator.GetFeatures(dfTrain, dfTest, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n",
      "Input shape: 1077\n",
      "Adding Layer 0: 100\n",
      "Adding sigmoid layer\n",
      "Adding 0.5 dropout\n",
      "Adding Layer 1: 50\n",
      "Adding sigmoid layer\n",
      "Adding 0.5 dropout\n",
      "Epoch 1/6\n",
      "39587/39587 [==============================] - 8s - loss: 1.7322     \n",
      "Epoch 2/6\n",
      "39587/39587 [==============================] - 8s - loss: 1.5447     \n",
      "Epoch 3/6\n",
      "39587/39587 [==============================] - 8s - loss: 1.4744     \n",
      "Epoch 4/6\n",
      "39587/39587 [==============================] - 8s - loss: 1.4245     \n",
      "Epoch 5/6\n",
      "39587/39587 [==============================] - 8s - loss: 1.3830     \n",
      "Epoch 6/6\n",
      "39587/39587 [==============================] - 9s - loss: 1.3548     \n",
      "19794/19794 [==============================] - 1s     \n",
      "39587/39587 [==============================] - 2s     \n",
      "0.587861528082\n",
      "0.616101674688\n",
      "Epoch 1/6\n",
      "39587/39587 [==============================] - 10s - loss: 1.3507    \n",
      "Epoch 2/6\n",
      "39587/39587 [==============================] - 9s - loss: 1.3286     \n",
      "Epoch 3/6\n",
      "39587/39587 [==============================] - 10s - loss: 1.3135    \n",
      "Epoch 4/6\n",
      "39587/39587 [==============================] - 8s - loss: 1.2989     \n",
      "Epoch 5/6\n",
      "39587/39587 [==============================] - 8s - loss: 1.2870     \n",
      "Epoch 6/6\n",
      "39587/39587 [==============================] - 9s - loss: 1.2824     \n",
      "19794/19794 [==============================] - 1s     \n",
      "39587/39587 [==============================] - 2s     \n",
      "0.587819393989\n",
      "0.618205808878\n",
      "Epoch 1/6\n",
      "39588/39588 [==============================] - 10s - loss: 1.3051    \n",
      "Epoch 2/6\n",
      "39588/39588 [==============================] - 10s - loss: 1.2898    \n",
      "Epoch 3/6\n",
      "39588/39588 [==============================] - 9s - loss: 1.2820     \n",
      "Epoch 4/6\n",
      "39588/39588 [==============================] - 11s - loss: 1.2691    \n",
      "Epoch 5/6\n",
      "39588/39588 [==============================] - 10s - loss: 1.2702    \n",
      "Epoch 6/6\n",
      "39588/39588 [==============================] - 10s - loss: 1.2657    \n",
      "19793/19793 [==============================] - 1s     \n",
      "39588/39588 [==============================] - 2s     \n",
      "0.622001120047\n",
      "0.63897517134\n",
      "19765/19765 [==============================] - 1s     \n"
     ]
    }
   ],
   "source": [
    "print (\"Creating dataset...\") \n",
    "modelName = 'Keras100/50Layers6Epochs'\n",
    "kf = KFold(len(dfTrain), 3)\n",
    "num = 1\n",
    "num_inputs = train.shape[1]\n",
    "\n",
    "clf = NN(inputShape = num_inputs, layers = [100, 50], dropout = [0.5, 0.5], activation='sigmoid', loss='mae', optimizer = 'adadelta', init = 'glorot_normal', nb_epochs = 6)\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "\n",
    "    predictionsDF = pd.read_csv('fold%s.csv' % str(num))    \n",
    "#     xTrain = dfTrain.iloc[train_index][features].values\n",
    "#     yTrain = dfTrain.iloc[train_index]['Response'].values \n",
    "    \n",
    "    xTrain = train.iloc[train_index].values\n",
    "    yTrain = labels.iloc[train_index].values \n",
    "    clf.fit(xTrain, yTrain)\n",
    "\n",
    "    xValidate = train.iloc[test_index].values\n",
    "    yValidate = labels.iloc[test_index]\n",
    "    predictions = np.clip(clf.predict(xValidate), 1, 8)\n",
    "#     predictions = np.clip(clf.predict(train.values), 1, 8)\n",
    "    trainPredictions = np.clip(clf.predict(xTrain), 1, 8)\n",
    "\n",
    "    print quadratic_weighted_kappa.quadratic_weighted_kappa(predictions, yValidate)\n",
    "    print quadratic_weighted_kappa.quadratic_weighted_kappa(trainPredictions, yTrain)\n",
    "    \n",
    "    predictionsDF[modelName] = predictions\n",
    "    predictionsDF.to_csv(path_or_buf='fold%s.csv' % str(num), index=False)\n",
    "    num += 1\n",
    "\n",
    "testDF = pd.read_csv('testPredictions.csv')            \n",
    "xTest = test.values\n",
    "testPredictions = np.clip(clf.predict(xTest), 1, 8)\n",
    "testDF[modelName] = testPredictions\n",
    "testDF.to_csv(path_or_buf='testPredictions.csv', index=False)\n",
    "    \n",
    "# print (\"Training model...\")\n",
    "# clf.fit(train, labels)\n",
    "\n",
    "# print (\"Making predictions...\")\n",
    "# pred = clf.predict(test)\n",
    "# predClipped = np.clip(np.round(pred), 1, 8).astype(int) #Make the submissions within the accepted range\n",
    "\n",
    "# submission = pd.read_csv('../input/sample_submission.csv')\n",
    "# submission[\"Response\"] = predClipped\n",
    "# submission.to_csv('NNSubmission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dummies...\n",
      "Filling in missing values...\n",
      "Scaling...\n"
     ]
    }
   ],
   "source": [
    "train, test, labels = make_dataset(True, \"mean\", True, dfTrain, dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19794    8\n",
      "19795    8\n",
      "19796    8\n",
      "19797    8\n",
      "19798    1\n",
      "19799    1\n",
      "19800    2\n",
      "19801    2\n",
      "19802    7\n",
      "19803    2\n",
      "19804    8\n",
      "19805    2\n",
      "19806    7\n",
      "19807    2\n",
      "19808    6\n",
      "19809    6\n",
      "19810    1\n",
      "19811    2\n",
      "19812    1\n",
      "19813    1\n",
      "19814    6\n",
      "19815    8\n",
      "19816    8\n",
      "19817    7\n",
      "19818    7\n",
      "19819    8\n",
      "19820    3\n",
      "19821    8\n",
      "19822    7\n",
      "19823    2\n",
      "        ..\n",
      "59351    5\n",
      "59352    6\n",
      "59353    6\n",
      "59354    6\n",
      "59355    6\n",
      "59356    6\n",
      "59357    6\n",
      "59358    6\n",
      "59359    6\n",
      "59360    5\n",
      "59361    2\n",
      "59362    7\n",
      "59363    6\n",
      "59364    1\n",
      "59365    5\n",
      "59366    8\n",
      "59367    6\n",
      "59368    8\n",
      "59369    2\n",
      "59370    4\n",
      "59371    6\n",
      "59372    2\n",
      "59373    8\n",
      "59374    7\n",
      "59375    8\n",
      "59376    4\n",
      "59377    7\n",
      "59378    8\n",
      "59379    8\n",
      "59380    7\n",
      "Name: Response, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print labels.iloc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.      ]\n",
      " [ 3.859828]\n",
      " [ 8.      ]\n",
      " ..., \n",
      " [ 8.      ]\n",
      " [ 8.      ]\n",
      " [ 8.      ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
